\documentclass[letter, 12pt]{article}

% Police, caractères spéciaux
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{mathrsfs}
\usepackage{braket}

% Environnements, figures
\usepackage{graphicx}
\usepackage{caption}  % Pour avoir du contrôle sur la taille des titres
\usepackage{subcaption}  % Sous-figures
\usepackage{booktabs}  % Plus beaux tableaux
\usepackage{multirow}
%\usepackage{appendix}  % Pour avoir du contrôle sur les annexes
%\usepackage{tikz}
%\usepackage{float}
%\usepackage{array}
\usepackage{hyperref}

% Bibliographie
\usepackage[
    backend=biber,
    style=numeric, 
    citestyle=numeric,  %authoryear, 
    natbib=true,
    url=false, 
    doi=true,
    eprint=false, 
    isbn=false, 
    sorting=none, 
    maxnames=2
]{biblatex}
\usepackage{csquotes}
\bibliography{matrix_inverses}
% Enlever les "notes" exportées par Zotero (qui donnent le "publisher" par exemple)
\AtEveryBibitem{%
  \clearfield{note}
  \clearlist{language}%
}

% Enlever le "In:" inutile
\renewbibmacro{in:}{}

% Mise en page
\usepackage[left = 2.5cm, right = 2.5cm, top = 2.5cm, bottom = 2.5cm]{geometry}
\oddsidemargin=0mm
\evensidemargin=0mm
\parindent=0mm
\parskip=12pt plus 1pt minus 1pt
%\usepackage{setspace}  % Si on veut changer l'interligne
%\widowpenalty10000  % Pour éviter les lignes orphelines
%\clubpenalty10000


% Titres et sous-titres
\usepackage[pagestyles]{titlesec}
\titlespacing{\section}{0mm}{0pt}{0pt}
\captionsetup[figure]{font=small}  % Pour que le titre des figures soit plus petit

% Titres de sections  -> Changer après la table des matières seulement
\titleformat{\section}
{\large\normalfont\bfseries}
{\thesection}{0.1em}{. }
% Solution for numberless sections from https://tex.stackexchange.com/questions/102116/titlesec-and-section-in-titleformat
\titleformat{name=\section,numberless}[block]{\large\normalfont\bfseries}{}{0em}{}[]% starred version


% Titres de sous-sections
\titleformat{\subsection}
{\normalfont\bfseries}
{\thesubsection}{0.2em}{\vspace{-12pt}}

\usepackage{tocloft}
\renewcommand{\cftsecafterpnum}{\vspace{10pt}}

% Nouvelles commandes et raccourcis
\def\tb{\textbf} 
\def\mb{\mathbf}
\newcommand{\prob}[1]{\mathbb{P}\left[ #1 \right]}
\newcommand{\card}[1]{\mathrm{card}\left( #1 \right)}
\newcommand{\esper}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\variance}[1]{\mathrm{Var}\left[ #1 \right]}
\def\beq{ \begin{equation} }		% \beq remplace maintenant \begin{equation}	
\def\eeq{ \end{equation} } 			% \eeq remplace maintenant \end{equation}
\def\ovl{\overline}
\def\avgnu{\braket{\nu}}
\def\eg{\textit{e.g.}, }
\def\ie{\textit{i.e.}, }
\newcommand{\dul}[1]{\underline{\underline{#1}}}



\begin{document}

\title{Manifold learning \textit{vs} predictive filtering: solution}
\author{François Bourassa and Gautam Reddy}
\date{\today}

\maketitle

\tableofcontents

\section{Calculation setup}
As laid out in Gautam's note, we want to minimize the loss function

\beq
	\mathcal{L} = \Big\langle{\Big\| \vec{x} - \sum_{l=1}^{t-1} v_l \vec{b}_{t-l} - W (\vec{x} + \vec{b}_t) \Big\|^2} \Big\rangle
	\label{eq:loss_def}
\eeq
 as a function of the scalar coefficients $v_l$ (predictive filtering) and of the matrix $W$ (manifold learning). 
 
 The background is a linear combination of pre-defined odor vectors, $\hat{y}_{\rho}$, weighted by stochastic concentrations,  $\nu_{\rho, t'}$, so $\vec{b}_{t'} = \sum_{\rho=1}^K \nu_{\rho, t'} \hat{y}_{\rho}$. The concentrations are assumed i.i.d. and stationary with mean zero (for simplicity), variance $\braket{\nu_{\rho, t'} \nu_{\lambda, t'}} = \sigma^2 \delta_{\rho \lambda}$, and autocorrelation function $\braket{\nu_{\rho, t'} \nu_{\lambda, t' \pm s}} = C(s) \delta_{\rho \lambda}$, with $C(0) = \sigma^2$. The new odor $\vec{x}$ comes from some distribution with zero mean and finite covariance matrix $\braket{\vec{x}\vec{x}^T}$. 
 
 
 \subsection{Remarks on the notation}
 One difficulty in this calculation is the presence of three different indices over which sums are applied, denoting olfactory dimensions, time, and background odors. Moreover, there can be matrix products in each of the first two kinds of indices, \eg $(W \vec{x})_i = \sum_{j} W_{ij} x_j$. To make notation more concise, we rewrite certain sums as dot products. To clarify which indices these products are on, we use upper arrows $\vec{\,\,\,}$ on vectors in olfactory dimensions, and underlines $\underline{\,\,\,}$ ($\dul{\,\,\,}$) for \underline{vectors} (\dul{matrices}) in time dimensions. We write out explicit sums with Greek indices on background odor indices, $\sum_{\rho=1}^K$. 
 
 \subsection{Writing out the loss function terms}
With these assumptions on the background and new odor statistics, we can expand the square and write out the different terms in the loss function. First, using the statistical independence of $\vec{x}$ and $\vec{b}_{t'}$ and their zero means, the terms to evaluate are
 \begin{align*}
 	\mathcal{L} =&  \braket{{\vec{b}_t}^T \vec{b}_t} + \sum_{l, m} v_l v_m \braket{{\vec{b}_{t-l}}^T \vec{b}_{t-m}} +  \braket{ {\vec{b}_t}^T W^T W \vec{b}_t} + \braket{{\vec{x}}^T W^T W \vec{x}} \nonumber \\
		& -2 \sum_{l=1}^{t-1} v_l \braket{{\vec{b}_t}^T \vec{b}_{t-l}} - 2 \braket{{\vec{b}_t}^T W \vec{b}_t} + 2 \sum_{l=1}^{t-1} v_l \braket{{\vec{b}_{t-k}}^T W \vec{b}_t}  \nonumber \,\, .
 \end{align*}
 
We compute these terms more explicitly by using the background properties defined above. The loss function is thus
\begin{align}
\mathcal{L} =& K \sigma^2 + K \sum_{l, m=1}^{t-1} v_l v_m C(l-m) + \sigma^2 \sum_{\rho=1}^K \hat{y}_{\rho}^T W^T W \hat{y}_{\rho} + \braket{\vec{x}^T W^T W \vec{x}} \nonumber \\
 &-2 K \sum_{l=1}^{t-1} v_l C(l) - 2 \sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} + 2 \left(\sum_{l=1}^{t-1} v_l C(l) \right) \left(\sum_{\rho=1}^K \hat{y}_{\rho}^T W \hat{y}_{\rho} \right) 
 \,\, .
\label{eq:loss_function}
\end{align}
We did not need to assume that background odors were orthogonal to get this answer; the statistical independence of their concentrations $\nu_{\rho, t}$ removed cross-odor terms. Most terms involve only $W$ or $v$ and only the last term couples the two strategies together. 

% More details on this evaluation, decided to hide for conciseness. 
% \begin{align*}
%	 \braket{{\vec{b}_t}^T \vec{b}_t} &= \sum_{\rho, \lambda} \braket{\nu_{\rho, t} \nu_{\lambda, t}} \hat{y}_{\rho}^T \hat{y}_{\lambda} =  \sigma^2 \sum_{\rho} \hat{y}_{\rho}^T \hat{y}_{\rho}  =  K \sigma^2   \\
%	 \sum_{l, m} v_l v_m \braket{{\vec{b}_{t-l}}^T \vec{b}_{t-m}} &= 	\sum_{\rho, \lambda=1}^K \braket{\nu_{\rho, t-l} \nu_{\lambda, t-m} }\hat{y}_{\rho}^T \hat{y}_{\lambda} = K \sum_{l, m=1}^{t-1} v_l v_m C(l-m)	\\
%	 \braket{{\vec{b}_t}^T W^T W \vec{b}_t} &= \sum_{\rho, \lambda=1}^K \hat{y}_{\rho}^T W^T W \hat{y}_{\lambda} \braket{ \nu_{\rho, t} \nu_{\lambda, t}} = \sigma^2 \sum_{\rho=1}^K \hat{y}_{\rho}^T W^T W \hat{y}_{\rho}  	\\
%%	 \braket{{\vec{x}}^T W^T W \vec{x}}  &= 		\\.  % remains as is, no simplification for now. 
%	\sum_{l=1}^{t-1} v_l \braket{{\vec{b}_t}^T \vec{b}_{t-l}} &= 	\sum_l \sum_{\rho, \lambda} \braket{\nu_{\rho, t} \nu_{\rho, t-l}} \hat{y}_{\rho}^T \hat{y}_{\lambda} = \sum_l v_l C(l) \sum_{\rho} \hat{y}_{\rho}^T \hat{y}_{\rho} = K \sum_{l=1}^{t-1} v_l C(l)	\\
%	\braket{{\vec{b}_t}^T W \vec{b}_t} &=	 \sum_{\rho, \lambda} \braket{\nu_{\rho, t} \nu_{\rho, t}} \hat{y}_{\rho}^T W \hat{y}_{\lambda}^T = \sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho}	\\
%	 \sum_{l=1}^{t-1} v_l \braket{{\vec{b}_{t-k}}^T W \vec{b}_t} &= \sum_l v_l	C(l) \sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} 	\\
% \end{align*}



\section{Solving for the optimal $W$ and $v$}

\subsection{Loss function derivatives and resulting optimum equations}
We can now take the derivative of this loss function with respect to the parameters $v_j$ and $W_{ij}$. After working out the derivatives of the different terms, the result is
\begin{align*}
\frac{\partial \mathcal{L}}{\partial v_j} &= 2 K \sum_{l=1}^{t-1} v_l C(l-j) - 2 K C(j) + 2 \left(\sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} \right) C(j)	\\
\frac{\partial \mathcal{L}}{\partial W_{ij}} &= 2 \sigma^2 \sum_{l=1}^{N_R} W_{il} \hat{y}_{\rho, l} \hat{y}_{\rho, j} + 2 \sum_{l=1}^{N_R} W_{il} \braket{x_l x_j} - 2 \left(\sigma^2 - \sum_{l=1}^{t-1} v_l C(l) \right) \hat{y}_{\rho, i} \hat{y}_{\rho, j}
\end{align*}

To shorten the notation of terms involving the autocorrelation function $C(s)$, we introduce the vectors $\underline{c} = (C(1), \ldots, C(t-1))$ and $\underline{v} = (v_1, \ldots, v_{t-1})$, and the matrix $\dul{C}_{ij} = C(i-j)$. We note that $\dul{C}$ is a Toeplitz matrix, symmetric since $C(i-j) = C(j-i)$. These properties help to express its inverse explicitly in some cases~\cite{dow_explicit_2003}. 

Setting the derivatives to zero to find the optimum parameters, we thus have a set of vector and matrix equations for $\underline{v}$ and $W$, respectively:
\begin{align}
0 &= K \dul{C} \underline{v} - \left(K - \sum_{\rho=1}^K \hat{y}_{\rho}^T W \hat{y}_{\rho}\right) \underline{c}
	 \label{eq:v_equation} \\
0 &= W \left( \sigma^2 \sum_{\rho} \hat{y}_{\rho} \hat{y}_{\rho}^T + \braket{\vec{x} \vec{x}^T} \right) - (\sigma^2 - \underline{v}^T \underline{c}) \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T
	\label{eq:w_equation}
\end{align}

\subsection{Solving for W in terms of $\underline{v}$}
The easiest solution path is to first solve for $W$ in terms of $\underline{v}^T \underline{c}$, then solve for $\underline{u}${\protect\footnote{
The reverse solution path is harder because $\underline{v}$ depends on $\hat{y}_{\rho}^T W \hat{y}_{\rho}$, which is hard to solve for in the other equation, because $W$ is involved in a product with $\braket{\vec{x} \vec{x}^T}$. 
}}. We define the $N_R \times N_R$ symmetric matrix
\beq
M = \sigma^2 \sum_{\rho} \hat{y}_{\rho} \hat{y}_{\rho}^T + \braket{\vec{x} \vec{x}^T}
\label{eq:matrix_m}
\eeq
which admits a spectral decomposition $M = U \Sigma U^T$ and a Moore-Penrose pseudo-inverse $M^+ = U \Sigma^+ U^T$, which is the actual inverse $M^{-1}$ when $M$ is invertible (\ie no zero eigenvalue in $\Sigma$). Equation~\ref{eq:w_equation} thus takes the form $WM = (\sigma^2 - \underline{v}^T\underline{c}) \sum_{\rho} \hat{y}_{\rho} \hat{y}_{\rho}^T$, which can be inverted for $W_M = W U \Sigma \Sigma^+ U^T$, the component of $W$ in the subspace spanned by $M$'s eigenvectors of non-zero eigenvalues:
\begin{equation*}
W_M  = W U (\Sigma \Sigma^+) U^T = (\sigma^2 - \underline{v}^T \underline{c}) \sum_{\rho} \hat{y}_{\rho}^T (U \Sigma^+ U^T) 
\end{equation*}
The $W$ component in the null space of $M$, if any, is not constrained by this optimization problem, so we set it to zero, and take $W = W_M$. Hence, 
\beq
	W = (\sigma^2 - \underline{v}^T \underline{c}) \sum_{\rho} \hat{y}_{\rho}^T \hat{y}_{\rho} M^+
	\label{eq:w_in_terms_v}
\eeq

\subsection{Solving for $\underline{v}$}
We can now insert the solution for $W$ in equation~\ref{eq:v_equation} for $\underline{v}$. We first evaluate the term
\begin{align*}
	\sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} &= \sum_{\rho} \hat{y}_{\rho}^T \left[(\sigma^2 - \underline{v}^T\underline{c}) \sum_{\lambda} \hat{y}_{\lambda} \hat{y}_{\lambda}^T M^+ \right] \hat{y}_{\rho} \\
	&= (\sigma^2 - \underline{v}^T \underline{c}) K m_y
\end{align*}
where we have defined
\beq
	m_y = \frac{1}{K} \sum_{\rho, \lambda=1}^K (\hat{y}_{\rho}^T \hat{y}_{\lambda}) \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho} \,\, .
	\label{eq:m_y_def}
\eeq
We have not assumed orthogonality of the background odor directions $\hat{y}_{\rho}$, but if that were the case, $m_y$ would simplify to $\frac1K \sum_{\rho} \hat{y}_{\rho} M^+ \hat{y}_{\rho}$. 

Inserting in eq.~\eqref{eq:v_equation}, dividing by $K$, and isolating $\underline{v}$ by assuming that the autocorrelation matrix $\dul{C}$ is invertible, we have
\begin{equation*}
	\underline{v} = \dul{C}^{-1} \underline{c} - \sigma^2 m_y \dul{C}^{-1} \underline{c} + m_y (\underline{v}^T \underline{c}) \dul{C}^{-1} \underline{c}
\end{equation*}
This is still an implicit expression because $\underline{v}^T \underline{c}$ appears on the right; taking the dot product of this expression with $\underline{c}$, we can isolate $\underline{v}^T \underline{c}$, 
\begin{equation*}
	\underline{v}^T \underline{c} = \frac{1 - \sigma^2 m_y \underline{c}^T \dul{C}^{-1} \underline{c}}{1 - m_y \underline{c}^T \dul{C}^{-1} \underline{c}}
\end{equation*}
then reinsert in the implicit equation for $\underline{v}$ to arrive at an explicit solution, 
\beq
	\underline{v} = \frac{1 - \sigma^2 m_y }{1 - \gamma m_y} \dul{C}^{-1} \underline{c}
	\label{eq:v_solution}
\eeq
where we have defined
\beq
	\gamma = \underline{c}^T \dul{C}^{-1} \underline{c}
	\label{eq:gamma_def}
\eeq


\subsection{Replacing $\underline{v}$ in the W solution}
Having evaluated $\underline{v}^T \underline{c}$, we can put it back in the expression~\eqref{eq:w_in_terms_v} to obtain an explicit solution for $W$, resulting in
\beq
	W = \frac{\sigma^2 - \gamma}{1 - \gamma m_y} \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T M^+ \,\, .
	\label{eq:w_solution}
\eeq



\section{Evaluating the loss function at the optimum}
For simplicity, we first rewrite the loss function in eq.~\eqref{eq:loss_function} using the underlined vector notation for $\underline{c}, \underline{v}, \dul{C}$, giving
\begin{align}
	\mathcal{L}_{v, W} =& K \sigma^2 + K \underline{v}^T \dul{C} \, \underline{v} + \sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W^T W \hat{y}_{\rho} + \braket{\vec{x}^T W^T W \vec{x}} \nonumber \\
	& - 2 K \underline{v}^T \underline{c} - 2\sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} + 2 \underline{v}^T \underline{c} \sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} 
	\,\, .
	\label{eq:loss_function_compact}
\end{align}

\subsection{Details on simplifying terms in $\mathcal{L}$}
We give here a few details on how some terms simplify when we insert the solutions for $\underline{v}$ and $W$ in $\mathcal{L}$. First, 
\begin{equation*}
	\underline{v}^T \dul{C} \, \underline{v} = \left(\frac{1 - \sigma^2 m_y}{1 - \gamma m_y} \right)^2 \underline{c}^T \dul{C}^{-1} \dul{C} \dul{C}^{-1} \underline{c} = \left(\frac{1 - \sigma^2 m_y}{1 - \gamma m_y} \right)^2 \gamma \,\, .
\end{equation*}

The next two terms can be combined, using the fact that $W$ is symmetric (from its solution) and moving some scalars resulting from dot products around:
\begin{align*}
	\sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W^T W \hat{y}_{\rho} 
	&= \sigma^2 \left(\frac{\sigma^2 - \gamma}{1 - \gamma m_y}\right)^2 \sum_{\rho, \lambda, \mu} \hat{y}_{\rho}^T M^+ \hat{y}_{\mu} \,\,\, \hat{y}_{\mu}^T \hat{y}_{\lambda}  \,\,\, \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho}	\\
	&= \sigma^2 \left(\frac{\sigma^2 - \gamma}{1 - \gamma m_y}\right)^2 \sum_{\rho, \lambda} \hat{y}_{\rho}^T M^+ \left(\sum_{\mu} \hat{y}_{\mu} \hat{y}_{\mu}^T \right) M^+ \hat{y}_{\lambda} (\hat{y}_{\lambda}^T \hat{y}_{\rho})
\end{align*}
and, since $\vec{x}^T M^+ \hat{y}_{\rho} = \hat{y}_{\rho}^T M^+ \vec{x}$ because $M$ and thus $M^+$ are symmetric,
\begin{align*}
	\braket{\vec{x}^T W^T W \vec{x}} 
	&= \left(\frac{\sigma^2 - \gamma}{1 - \gamma m_y}\right)^2 \sum_{\rho, \lambda} \braket{\vec{x}^T M^+ \hat{y}_{\rho} \hat{y}_{\rho}^T \hat{y}_{\lambda} \hat{y}_{\lambda}^T M^+ \vec{x} }	\\
	&= \left(\frac{\sigma^2 - \gamma}{1 - \gamma m_y}\right)^2 \sum_{\rho, \lambda} \hat{y}_{\rho}^T M^+ \left( \braket{\vec{x} \vec{x}^T} \right) M^+ \hat{y}_{\lambda} (\hat{y}_{\lambda}^T \hat{y}_{\rho})
\end{align*}
Summing the two, factoring out left and right common factors, we find $\sigma^2 \sum_{\mu} \hat{y}_{\mu} \hat{y}_{\mu}^T + \braket{\vec{x}\vec{x}^T} = M$ in the middle, allowing us to use the pseudo-inverse's property $M^+ M M^+ = M^+$ and to recognize $\sum_{\rho, \lambda} \hat{y}_{\rho}^T \hat{y}_{\lambda} \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho} = K m_y$. Hence, we have
\begin{equation*}
	\sigma^2 \sum_{\rho} \hat{y}_{\rho}^T W^T W \hat{y}_{\rho}  + \braket{\vec{x}^T W^T W \vec{x}}  =  \left(\frac{\sigma^2 - \gamma}{1 - \gamma m_y}\right)^2 K m_y
\end{equation*}

Next, we need to evaluate
\begin{equation*}
	\sum_{\rho} \hat{y}_{\rho}^T W \hat{y}_{\rho} = \frac{\sigma^2 - \gamma}{1 - \gamma m_y} \sum_{\rho} \hat{y}_{\rho}^T \sum_{\lambda} \hat{y}_{\lambda} \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho} =  \frac{\sigma^2 - \gamma}{1 - \gamma m_y} K m_y \,\, .
\end{equation*}

Lastly, as we already have evaluated, $\underline{v}^T \underline{c} = \frac{1 - \sigma^2 m_y}{1 - \gamma m_y} \gamma$. 


\subsection{Combining all terms}
Combining the expressions developed above on the same denominator requires a little bit of algebra to cancel out some terms in $\mathcal{L}$. After carrying out this calculation and factorizing as much as possible, we find
\beq
	\mathcal{L}_{v, W} = K \frac{(\sigma^2 - \gamma) (1 - \sigma^2 m_y)}{1 - \gamma m_y}
	\label{eq:loss_evaluated_general}
\eeq
% Former answer I had, before realizing I could factorize further:
%\beq
%	\mathcal{L} = K \left(\frac{1}{1 - \gamma m_y} \right)^2 (\sigma^2 - \gamma) \left(1 + \sigma^2 \gamma m_y^2 - m_y(\sigma^2 + \gamma) \right)
%\eeq

We notice that the loss seems to scale proportionally with the background subspace dimensions, $K$. Terms $\sigma^2$, $\gamma$ do not depend on $K$ but only on the autocorrelation and variance of odor concentrations. The only term that could depend on $K$ is $m_y = \frac{1}{K} \sum_{\rho, \lambda=1}^K (\hat{y}_{\rho}^T \hat{y}_{\lambda}) \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho}$, but we generally expect $K m_y \sim K$, which is especially clear if we assume orthogonality of the $\hat{y}$, such that $m_y = \frac{1}{K} \sum_{\rho=1}^K  \hat{y}_{\rho}^T M^+ \hat{y}_{\rho} \sim \mathcal{O}(1)$. 

Hence, there is not obvious tradeoff between the two strategies -- predictive filtering and manifold learning -- as a function of the background dimension. This makes sense, \textit{a posteriori}. Predictive filtering tries to anticipate $K$ independent, identically distributed odors, hence the squared errors committed on each background components add up in variance. Meanwhile, the error in manifold learning increase with $K$ because a fraction $\sim K$ of the new odor, on average, will lie in the background subspace. Still, we had to carry out the calculation to check the exact scaling of these errors with $K$. 

However, there is a tradeoff between the strategies as a function of the autocorrelation time, encoded in the parameter $\gamma$, and the dimensionality of the olfactory space, which enters $m_y$ through the new odor statistics $\braket{\vec{x} \vec{x}^T}$ in $M$. We expect $\gamma$ to increase with the autocorrelation time scale, and $m_y$ to increase with the olfactory space dimension $N_R$. Hence, as the autocorrelation time increases, $\gamma$ diminishes the relative efficacy of manifold learning by reducing the denominator $1 - \gamma m_y$, while increasing the importance of predictive filtering by reducing the numerator factor $\sigma^2 - \gamma$. This tradeoff will be clearer in the special case studied in section~\ref{sec:special_case}. 


\section{Summary of the general optimal solution}
\label{sec:summary}

We recapitulate the optimization results here. The optimal $\underline{v}$ and $W$ are
\begin{align}
	\underline{v} &= \frac{1 - \sigma^2 m_y }{1 - \gamma m_y} \dul{C}^{-1} \underline{c}  \tag{\ref{eq:v_solution}}  \\
	W &= \frac{\sigma^2 - \gamma}{1 - \gamma m_y} \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T M^+  \tag{\ref{eq:w_solution}}
\end{align}
and they give a minimum loss of
\beq
	\mathcal{L}_{v, W} = K \frac{(\sigma^2 - \gamma) (1 - \sigma^2 m_y)}{1 - \gamma m_y}  \tag{\ref{eq:loss_evaluated_general}}
\eeq
where $K$ is the number of i.i.d. background odors, $\sigma^2$ is the variance of each odor concentration $\nu_{\rho, t}$, and where we have defined background parameters
\begin{align}
	\gamma &= \underline{c}^T \dul{C}^{-1} \underline{c}	  \tag{\ref{eq:gamma_def}}  \\
	m_y &= \frac{1}{K} \sum_{\rho, \lambda=1}^K (\hat{y}_{\rho}^T \hat{y}_{\lambda}) \hat{y}_{\lambda}^T M^+ \hat{y}_{\rho}  \tag{\ref{eq:m_y_def}} \\
	\text{in which } M &= \sigma^2 \sum_{\rho} \hat{y}_{\rho} \hat{y}_{\rho}^T + \braket{\vec{x} \vec{x}^T} \tag{\ref{eq:matrix_m}} \\
	\underline{c} &= \begin{pmatrix}
		C(1) \\
		\ldots \\
		C(t-1)
	\end{pmatrix} \nonumber \\
	\dul{C} &= \begin{pmatrix}
		C(0) & C(1) & C(2) & \ldots & C(t-3) & C(t-2) & C(t-1) \\
		C(1) & C(0) & C(1) & \ldots & C(t-4) & C(t-3) & C(t-2) \\
		\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots & \vdots \\
		C(t-2) & C(t-3) & C(t-4) & \ldots & C(3) & C(2) & C(1) \\
		C(t-1) & C(t-2) & C(t-3) & \ldots & C(2) & C(1) & C(0) \\
	\end{pmatrix} \nonumber \\
	C(s) &= \braket{\nu_{\rho, t'} \nu_{\rho, t' \pm s}} \nonumber \\
\end{align}


\section{Limiting cases: $W=0$ and $\underline{v}=0$}
\subsection{Predictive filtering only: $W=0$}
When $W = 0$, we can directly solve eq.~\ref{eq:v_equation} for $\underline{v}$, finding
\begin{equation*}
	\underline{v}_{\,W=0} = \dul{C}^{-1} \underline{c}
\end{equation*}
which yields a loss  of
\beq
	\mathcal{L}_{v} = K (\sigma^2 - \gamma)  \,\, .
	\label{eq:loss_v}
\eeq
As long as $\gamma < \sigma^2$ -- which should be the case if the autocorrelation function decays with time -- we have $\mathcal{L}_{v, W} < \mathcal{L}_v$ since $\frac{1 - \sigma^2 m_y)}{1 - \gamma m_y} < 1$ in that case. 

\subsection{Manifold learning only: $\underline{v} = 0$}
When $\underline{v} = 0$, we can directly solve eq.~\eqref{eq:w_equation} in terms of $M^+$, resulting in
\begin{equation*}
	W_{v=0} = \sigma^2 \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T M^+
\end{equation*}
which yields a loss of 
\beq
	\mathcal{L}_W = K \sigma^2 ( 1 - \sigma^2 m_y) \, .
\eeq
As long as $m_y < \frac{1}{\sigma^2}$ -- which should be the case since $M^+ \sim 1/\sigma^2$ and $m_y$ is some projection of it on the background subspace -- then $\mathcal{L}_{v, W} < \mathcal{L}_W$, since $\frac{1 - \gamma / \sigma^2}{1 - \gamma m_y} < 1$ in that case. 




\section{Special background choice: exponential kernel, $\hat{x}$ uniform on hypersphere}
\label{sec:special_case}
To make these results more concrete, we now consider a simple case of background statistics where expressions such as $\gamma$, $m_y$, etc. can be computed analytically in terms of interpretable parameters: an exponential autocorrelation function and new odors uniformly sampled on a hypersphere. 

\subsection{Exponential autocorrelation kernel, to evaluate $\gamma$}
We suppose that each odor concentration $\nu_{\rho, t'}$ is independent of other odors and forms a Gaussian process with exponential autocorrelation kernel $C(s) = \braket{\nu_{\rho, t'} \nu_{\rho, t' \pm s}} =\sigma^2 e^{-|s|/\tau}$ with autocorrelation time $\tau$ (\ie the Ornstein-Uhlenbeck process). A small $\tau$ corresponds to fast fluctuations compared to the time scale of learning. In this case, the symmetric Toeplitz matrix $\dul{C}_{ij} = \sigma^2 (e^{-1/\tau})^{|i-j|}$ is a Kac-Murdock-Szeg\"o matrix (form $A_{ij} = r^{|i-j|}$, $r \neq 1$), which has an explicit inverse, provided in~\cite[sec.~1.3]{dow_explicit_2003}. This inverse is the tridiagonal matrix
\beq
\dul{C}^{-1} = \frac{1}{\sigma^2} \frac{1}{1 - e^{-2\tau}} \begin{pmatrix}
		1 & -e^{-1/\tau} & 0 & \ldots &  0 \\
		-e^{-1/\tau} & 1 + e^{-2/\tau} & -e^{-1/\tau} & \ldots &0  \\
		\vdots & \ddots & \ddots & \ddots  & \vdots  \\
		0 & \ldots & -e^{-1/\tau} & 1 + e^{-2/\tau} & -e^{-1/\tau} \\
		0 & \ldots & 0 & -e^{-1/\tau} & 1 \\
	\end{pmatrix} \nonumber \\
\eeq
It is not hard to check that this is indeed the inverse of $\dul{C}$. This allows us to evaluate
\begin{align*}
	\dul{C}^{-1} \underline{c} &= \begin{pmatrix} e^{-1/\tau} \\ 0 \\ \vdots \\ 0 \end{pmatrix} \\
	\text{thus } \gamma &= \underline{c}^T \dul{C}^{-1} \underline{c} = \sigma^2 e^{-2/\tau}  \\
\end{align*}

\subsection{New odors $\hat{x}$ uniform, and $\hat{y}_{\rho}$ orthogonal, to evaluate $m_y$}
Moreover, we suppose that new odors take the form $\vec{x} = \nu_x \hat{x}$, where $\nu_x$ has the same variance as the background odor concentrations, $\braket{\nu_x^2} = \sigma^2$, and where $\hat{x}$ is uniformly sampled on the $N_R$-dimensional unit hypersphere. In that case, by symmetry, the new odor covariance matrix is $\braket{\vec{x} \vec{x}^T} = \frac{\sigma^2}{N_R} \mathbb{I}$ (\ie the identity matrix times $\sigma^2/N_R$). 

Additionally, we assume that background odors are orthogonal to each other: $\hat{y}_{\rho}^T \hat{y}_{\lambda} = \delta_{\rho \lambda}$. 

These choices allow us to compute $M^+$ explicitly. We first note that $M = \braket{\vec{x} \vec{x}^T} + \sigma^2 \sum_{\rho} \hat{y} \hat{y}_{\rho}$ has full rank, and thus $M^+ = M^{-1}$. We can compute that inverse by repeatedly applying the following lemma, proven in~\cite{miller_inverse_1981}, for the inverse of the sum of a full-rank matrix $G$ and a rank-one matrix $H$:
\beq
	(G + H)^{-1} = G^{-1} - \frac{1}{1 + g} G^{-1} H G^{-1} \quad \text{where} \,\,  g = \mathrm{Trace}(H G^{-1}) \,\, .
	\label{eq:miller_lemma}
\eeq
In our case, we consider the sequence of matrices $M_k = \frac{1}{N_R} \mathbb{I} + \sum_{\rho=1}^k \hat{y}_{\rho} \hat{y}_{\rho}^T$, $k \leq K$. For $M_1 = \frac{1}{N_R} + \hat{y}_1 \hat{y}_1^T$, lemma~\eqref{eq:miller_lemma} is applied with $G = \frac{1}{N_R}$, $H = \hat{y}_1 \hat{y}_1^T$, so $G^{-1} = N_R \mathbb{I}$, $g = \mathrm{Tr}( N_R \mathbb{I} \hat{y}_1 \hat{y}_1^T) = N_R \mathrm{Tr}(\hat{y}_1 \hat{y}_1^T) = N_R$. The inverse is thus $M_1^{-1} = N_R \mathbb{I} - \frac{N_R^2}{N_R + 1} \hat{y}_1 \hat{y}_1^T$. 

To continue to $M_2^{-1}$, we now take $G = M_1$ and $H = \hat{y}_2 \hat{y}_2^T$ -- still of rank one, as it should. The assumed orthogonality of $\hat{y}_1$ and $\hat{y}_2$ ensures that only the identity part of $M_1^{-1}$ survives in products like $G^{-1} H G^{-1} = M_1^{-1} \hat{y}_2 \hat{y}_2^T M_1^{-1}$; hence, we find that$g_2 = \mathrm{Tr}(M_1^{-1} \hat{y}_2 \hat{y}_2^T) = N_R$ still, and that $M_2^{-1} = N_R \mathbb{I} + \frac{N_R^2}{N_R + 1} \sum_{\rho=1}^2 \hat{y}_{\rho} \hat{y}_{\rho}^T$. 

We see that we can proceed by induction, up to finding the inverse of the full matrix $M = \sigma^2 M_K$ in this special case; the result is
\beq
	M^+ = M^{-1} = \frac{N_R}{\sigma^2} \left( \mathbb{I} + \frac{N_R}{N_R + 1} \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T  \right)\,\, .
\label{eq:m_inverse}
\eeq
We can thus evaluate the matrix product appearing in the optimal $W$ solution, 
\begin{equation*}
	\sum_{\lambda} \hat{y}_{\lambda} \hat{y}_{\lambda}^T M^+ = \frac{N_R}{\sigma^2} \sum_{\lambda} \hat{y}_{\lambda} \hat{y}_{\lambda}^T - \frac{N_R^2}{\sigma^2 (N_R + 1)} \sum_{\lambda} \hat{y}_{\lambda} \hat{y}_{\lambda}^T =  \frac{N_R}{N_R + 1} \frac{1}{\sigma^2} \sum_{\rho} \hat{y}_{\rho} \hat{y}_{\rho}^T  \, \, ,
\end{equation*}
thanks to  $\hat{y}_{\lambda}^T \hat{y}_{\rho} = \delta_{\lambda \rho}$, which also allows us to compute
\begin{align*}
	m_y &= \frac1K \sum_{\rho, \mu} \hat{y}_{\rho}^T \hat{y}_{\mu} \hat{y}_{\mu}^T M^+ \hat{y}_{\rho} = \frac{N_R}{N_R + 1} \frac{1}{\sigma^2}  \frac1K  \sum_{\rho, \lambda} \hat{y}_{\rho}^T \hat{y}_{\lambda} \hat{y}_{\lambda}^T \hat{y}_{\rho} =  \frac{N_R}{N_R + 1} \frac{1}{\sigma^2}  \frac1K \times K \\
	\Rightarrow m_y &=   \frac{1}{\sigma^2} \frac{N_R}{N_R + 1} \,\, .
\end{align*}


\subsection{Optimal $\underline{v}, W$ and loss function in this background choice}
Inserting the above expressions for $\gamma$, $M^+$, $m_y$, etc. into the optimal solution for the general case, we find
\begin{align}
	\underline{v} &= \frac{1}{1 + N_R(1 - e^{-2/\tau})} \begin{pmatrix} e^{-1/\tau} \\ 0  \\ \vdots  \\ 0 \end{pmatrix}  \label{eq:v_solution_special}  \\
	W &= \frac{N_R (1 - e^{-2/\tau})}{1 + N_R (1 - e^{-2/\tau})} \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T  \label{eq:w_solution_special}
\end{align}
and a minimum loss of 
\beq
	\mathcal{L}_{v, W} = K \sigma^2 \frac{1 - e^{-2/\tau}}{1 + N_R(1 - e^{-2/\tau})}
	\label{eq:loss_evaluated_special}
\eeq

Here, we see clearly  that there is no tradeoff as a function of $K$: both strategies have an error that increases proportionally to $K$. At least, we clearly see the transition from predictive filtering to manifold learning as $N_R$ decreases or $\tau$ decreases. For small correlation times, $1 - e^{-2/\tau} \approx 1$, so the main reduction of the loss comes from the $\frac{1}{N_R +1}$ factor (also the case if $N_R$ is large). Meanwhile, for large correlation $\tau$, the reduction comes from the numerator $1 - e^{-2/\tau} \approx 0$, while the $N_R$ term in the denominator is rendered ineffective -- the same is true if $N_R$ is small. 


\subsection{Special cases $W=0$ and $\underline{v}=0$ in this background choice}
For further comparison, the optimal solutions and loss function in the pure predictive filtering case ($W=0$) with our specific background choice are:
\begin{align}
	\underline{v}_{\,W=0} = \dul{C}^{-1} \underline{c} = \begin{pmatrix} e^{-1/\tau} \\ 0  \\ \vdots  \\ 0 \end{pmatrix} \nonumber \\
	\mathcal{L}_v = K \sigma^2 (1 - e^{-2/\tau})
	\label{eq:loss_special_v_only} \,\, .
\end{align}

In the pure manifold learning case ($\underline{v} = 0$), these are rather
\begin{align}
	W_{v=0} &= \sigma^2 \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T M^+ =  \frac{N_R}{N_R + 1} \sum_{\rho=1}^K \hat{y}_{\rho} \hat{y}_{\rho}^T  \\
	\mathcal{L}_W &= \frac{K \sigma^2}{N_R + 1}
	\label{eq:loss_special_w_only} \,\, .
\end{align}


\subsection{Plots of the loss function versus $N_R$ and $\tau$ for the different strategies}
\label{subsec:plots}
We notice that the loss is proportional to $K \sigma^2$ in all strategies for our special background choice, hence we can illustrate the relative efficacy of predictive filtering and manifold learning by plotting $\mathcal{L} / (K\sigma^2)$ as a function of $N_R$ and $\tau$, the two background parameters for which there is actually a tradeoff between the two strategies. Figure~\ref{fig:loss} shows the single-strategy losses $\mathcal{L}_v$ (eq.~\ref{eq:loss_special_v_only}) and $\mathcal{L}_W$ (eq.~\ref{eq:loss_special_w_only})  compared to the loss for both strategies applied simultaneously, $\mathcal{L}_{v, W}$ ((eq.~\ref{eq:loss_evaluated_special}). 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/loss_lineplots_manifold_vs_predictive.pdf}
	\caption{Loss functions for habituation strategies (each separately and combined) as a function of background correlation time scale $\tau$ (left, $N_R = 50$ fixed) and olfactory space dimensionality $N_R$ (right, $\tau = 100$ fixed). }
	\label{fig:loss}
\end{figure}
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.45\linewidth]{figures/loss_strategy_phase_diagram_heatmap.pdf}
	\caption{Phase diagram of olfactory habituation strategies. The heatmap shows the difference $\Phi$ between the log of two terms: the difference in loss between manifold learning only and the combined strategies, $\mathcal{L}_W - \mathcal{L}_{v,W}$, and the difference between predictive filtering only and the combined strategies, $\mathcal{L}_v - \mathcal{L}_{v,W}$. When $\Phi = \log(\mathcal{L}_W - \mathcal{L}_{v,W}) - \log(\mathcal{L}_v - \mathcal{L}_{v,W})$ is negative, this means that $\mathcal{L}_v$ is further away from $\mathcal{L}_{v, W}$ than $\mathcal{L}_W$, hence manifold learning is the dominant strategy, and vice-versa when $\Phi > 0$ (predictive filtering dominates). }
	\label{fig:phase_diagram}
\end{figure}

We see that for even a small olfactory  space dimension, $N_R = 50$ as in the fruit fly, manifold learning performs much better than predictive filtering even for moderately long correlation time scales -- one time step $\approx 10$ ms in our habituation simulations, usually, because this is the fastest time scale of correlations in the power-law spectrum of turbulent statistics. Hence,  $\tau = 100$ corresponds to $\sim1$ s correlations in fluctuations, which is slow compared to the shortest turbulent fluctuations. 

When $\mathcal{L}_{v, W}$ is close to either $\mathcal{L}_{W}$ or $\mathcal{L}_v$, it means this strategy contributes most of the loss reduction. We can thus draw a small phase diagram of where habituation is dominated by manifold learning (large $N_R$, small $\tau$) or by predictive filtering (small $N_R$, large $\tau$). The (smooth) transition occurs when $\mathcal{L}_W = \mathcal{L}_v$, which happens at $1 - e^{-2/\tau} = \frac{1}{N_R + 1}$, which is at  $\tau \approx 2(N_R + 1)$ for large $N_R$. This is shown in figure~\ref{fig:phase_diagram}. There is a large region (in red) where manifold learning is the most effective strategy. 


% Small bibliography, no need to go into biblatex. 
%\begin{thebibliography}{9}
%\bibitem{dow_explicit_2003}
%Dow, Murray (2003). ``Explicit inverses of {Toeplitz} and associated matrices'', \textit{ANZIAM Journal}, vol. 44, no. E, pp.~E185--E215. \url{doi.org/10.21914/anziamj.v44i0.493}
%
%\bibitem{miller_inverse_1981}
%Miller, Kenneth (1981). ``On the {Inverse} of the {Sum} of {Matrices}'', \textit{Mathematics Magazine}, vol. 54, no. 2, pp.~67--72. \url{doi.org/10.1080/0025570X.1981.11976898}
%\end{thebibliography}
\printbibliography

\end{document}
