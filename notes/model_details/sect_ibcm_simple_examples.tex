% IBCM network on a 2-odor toy model. As a proof of principle that this network has the desired properties. 
\subsection{Toy example: two-odor Gaussian background}
\label{subsect:2d_model}

% Inhibitory part solution
% Analytics, numerics. 
% New odor recognition? Meh, drop it, we are in 2d case anyways. 
% Notice that if d > n_odors, d-n dimensions not specified. Is that a problem? Not sure, we will see. 

% Background process, Ornstein-Uhlenbeck, simulation method. 
\subsubsection{Background process}
\label{subsubsect:2d_background}
We first consider a toy version of an olfactory background, which is a linear combination of two odors with fluctuating proportions. Let the input be

\beq
	\vec{x}(t) = \left(\frac12 + \ovl{\nu}(t)\right) \vec{x}_a + \left(\frac12 - \ovl{\nu}(t)\right) \vec{x}_b
	\label{eq:2d_mixture_process}
\eeq

where $\vec{x}_a$ and $\vec{x}_b$ are constant $n_R$-dimensional vectors and $\ovl{\nu}(t)$ follows the Ornstein-Uhlenbeck process

\beq
	\frac{d \ovl{\nu}}{dt} = -\frac{1}{\tau_b} \ovl{\nu} + \sqrt{\frac{2 \sigma^2}{\tau_b}} \xi(t)
	\label{eq:oh_process_1d}
\eeq

where $\xi(t)$ is white noise, $\tau_b$ is the autocorrelation time scale, and the steady-state distribution of $\ovl{\nu}$ is a zero-mean, $\sigma^2$ variance normal distribution. We typically choose $\sigma^2 = 0.09$ ($\sigma = 0.3$), so fluctuations of $\frac12 \pm \ovl{\nu}$ past $0$ or $1$ are infrequent but possible. 

To simulate the Ornstein-Uhlenbeck (O-U) process $\ovl{\nu}(t)$ numerically, we use an exact update rule for finite time steps $\Delta t$ derived from the analytical solution of the O-U process, taking the last time step as a new deterministic initial condition~\cite[eq.~2.47]{gillespie_mathematics_1996}:

\beq
	\ovl{\nu}(t + \Delta t) = \ovl{\nu}(t) e^{-\Delta t / \tau_b} + \sqrt{\sigma^2 \left(1 - e^{-2 \Delta t / \tau_b}  \right)}\, \xi(t)
	\label{eq:exact_update_rule_o-u}
\eeq

where $\xi(t)$ is white noise. The coefficients of $\ovl{\nu}(t)$ and $\xi(t)$ can be computed in advance. This rule ensures a steady-state distribution of $\ovl{\nu}$ with the desired variance $\sigma^2$ even though the simulation time step is on the order of $\tau_b$. 


% IBCM part solution
\subsubsection{Analytical results: IBCM fixed points}
\label{subsubsect:2d_model_analytical}

To understand analytically how this binary mixture background can be inhibited by the proposed network, we start by calculating the steady-state of the IBCM neurons' $\vec{m}^j$ synaptic weights; the inhibitory weights $W$ follow from them. The fixed points equations to solve for the IBCM neurons are, from equation \eqref{eq:ibcm_fixed_points}, 

\beq
	0 = \esper{\{ \vec{\ovl{m}}^j \cdot \vec{x}(t) \}^2 \vec{x}(t)} - \esper{\{ \vec{\ovl{m}}^j \cdot \vec{x}(t) \} \ovl{\Theta}^j \vec{x}(t)} \quad \forall j \in \{1, 2, \ldots, n_I \} \, \, .
	\label{eq:ibcm_fixed_points_developed}
\eeq

Since those $M$ equations are identical, we can drop the $j$ indices and concentrate on one of them. Some approximations are necessary to make analytical progress. 
The first approximation we make is to replace $\ovl{\Theta} = \esper{\ovl{c}^2}$, that is, we neglect correlations between $\Theta$ and $\ovl{\nu}$ or $\vec{m}$. 
The second approximation is that correlations between $\vec{\ovl{m}}^j$ and $\ovl{\nu}(t)$ are negligible; this is fair if time scales $1/\mu$, $\tau_{\Theta}$ and $\tau_b$ are well separated. Third, we assume the variance of $\vec{\ovl{m}}^j$ will be small, since variations in $\vec{\ovl{m}}$ would come from rapid changes in $\ovl{\nu}(t)$ and ensuing fluctuations in $\ovl{\Theta}$. Hence, we can treat $\vec{\ovl{m}} = \esper{\vec{\ovl{m}}}$ as constant in the equation. 

Moreover, we rewrite $\vec{x}(t) = \vec{x}_d + \ovl{\nu}(t) \vec{x}_s$, where $\vec{x}_d = \frac12 (\vec{x}_a + \vec{x}_b)$ (deterministic part) and $\vec{x}_s = \vec{x}_a - \vec{x}_b$ (stochastic part). From these new vectors appear the dot products $\ovl{c}_d = \esper{\vec{\ovl{m}}} \cdot \vec{x}_d$ and $\ovl{c}_s = \esper{\vec{\ovl{m}}} \cdot \vec{x}_s$, such that $\ovl{c} = \ovl{c}_d + \ovl{\nu}(t) \ovl{c}_s$. We can solve for the two dot products $\ovl{c}_d$ and $\ovl{c}_s$ because they specify the fixed points completely when there are $K=2$ background components. In light of our approximations, they are constant with respect to the average operator in equation \eqref{eq:ibcm_fixed_points_developed}, which becomes

\begin{align}
	0 &= \esper{(\ovl{c}_d + \ovl{\nu} \ovl{c}_s)^2 (\vec{x}_d + \ovl{\nu} \vec{x}_s)} - \esper{(\ovl{c}_d + \ovl{\nu} \ovl{c}_s)^2}\esper{(\ovl{c}_d + \ovl{\nu} \ovl{c}_s) (\vec{x}_d + \ovl{\nu} \vec{x}_s)}  \nonumber \\
	0 &= (\ovl{c}_d^2 + \sigma^2 \ovl{c}_s^2 - \ovl{c}_d^3 - \ovl{c}_d \ovl{c}_s^2 \sigma^2) \vec{x}_d + (2 \ovl{c}_s \ovl{c}_d - \ovl{c}_d^2 \ovl{c}_s -\sigma^2 \ovl{c}_s^3)\sigma^2 \vec{x}_s  \label{eq:fixed_points_2d_developed}
\end{align}

Since $\vec{x}_d$ and $\vec{x}_s$ are linearly independent, both coefficients must be zero, leading to a system of two nonlinear equations for $\ovl{c}_d$ and $\ovl{c}_s$:

\begin{align}
	0 &= \ovl{c}_d^3 - \ovl{c}_d^2 - \sigma^2 \ovl{c}_s^2 + \ovl{c}_d \ovl{c}_s^2 \sigma^2 \\
	0 &= \sigma^2 \ovl{c}_s ( \ovl{c}_d^2 + \sigma^2 \ovl{c}_s^2 - 2\ovl{c}_d)
\end{align}

From these two equations, we find the following solutions{\protect \footnote{
There is also a solution $s=d=0$ corresponding to $\vec{m} = 0$, which is unstable.
}}:

\beq
	\ovl{c}_d =  \vec{\ovl{m}} \cdot \vec{x}_d = 1 \,\, \mathrm{and} \,\, \ovl{c}_s = \vec{\ovl{m}} \cdot \vec{x}_s = \pm \frac{1}{\sigma}
	\label{eq:ibcm_2d_solution_sd}
\eeq

or, in terms of the dot products with $\vec{x}_a$ and $\vec{x}_b$:

\beq
	\vec{\ovl{m}}^{\pm} \cdot \vec{x}_a = 1 \pm \frac{1}{2 \sigma}  \,\, \mathrm{and} \,\, 1 \mp \frac{1}{2 \sigma}
	\label{eq:ibcm_2d_solution_ab}
\eeq

From the two dot products, we can build $\vec{\ovl{m}}$ and then $\vec{m}$ from equation \eqref{eq:inverse_circulant_mvecs}. 
Hence, we find two different stable fixed points, which we call $\vec{\ovl{m}}^+$ and $\vec{\ovl{m}}^-$ to indicate which sign the dot product with $\vec{x}_s$ takes. To understand better what these fixed points imply, let's compute the response of an IBCM neuron at either fixed point to some stimulus $\vec{x}(t)$:

\beq
	\ovl{c}^{\pm} = \vec{\ovl{m}}^{\pm} \cdot (\vec{x}_s + \ovl{\nu}(t) \vec{x}_d) = 1 \pm \frac{\ovl{\nu}}{\sigma}
	\label{eq:ibcm_2d_response}
\eeq

We notice that $\ovl{c} = 0$ when $\ovl{\nu} = \mp \sigma$, that is, \emph{the IBCM neuron is non-responsive to an odor component one standard deviation away on one side of the average background, while it responds strongly to odors on the other side of the average}. Thus, the model's specificity property, well-studied for alternating backgrounds, is preserved for inputs with continuous variations, now with vectors one standard deviation away from the average representing the different components that can be ``selected'' by the IBCM neuron. In other words, the neuron becomes selective for $\vec{x}(\ovl{\nu} = \pm \sigma)$. 

%Since there are $K = 2$ linearly independent vectors in the background, we can use $n_R=2$ dimensions: the effective space of $\vec{m}$ vectors is the same vector space as $\vec{x}$, since $\frac{d\vec{m}}{dt} \propto \vec{x}(t)$. If the neurons have in reality more synapses, we can redefine coordinates to have 2 dimensions. The remaining $n_R-2$ dimensions of $\vec{m}$ stay constant over time and are determined by the initial conditions. 
% The part of $\vec{m}$ in the orthogonal complement of this vector space remains constant over time and is determined by the initial conditions. 
%In general, with a $n_B$-dimensional vector space of $\vec{x}$, each fixed point is in reality a $(n_R-n_B)$-dimensional manifold of fixed points. 

\subsubsection{Analytical results: convergence time}
\label{subsubsect:convergence_time_2d}
For our habituation model to be realistic, it must converge reasonably fast to a steady-state. The convergence time of a single IBCM neuron can be estimated analytically at least in the two-odor toy model; this analysis reveals the main parameters influencing how long it takes to habituate to a fluctuating background (assuming the convergence of inhibitory weights $W$ occurs at a similar pace, which is the case numerically)\protect{\footnote{
The following calculation was devised by Gautam Reddy. 
}}.

To begin with, we make a quasi-static approximation on the threshold $\Theta$, assuming it averages over the fast background fluctuations but also converges fast enough to track the slow variations of $\vec{m}$:
\begin{equation*}
	\Theta \approx \mathbb{E}_{\ovl{\nu}}\left[c^2\right] = \mathbb{E}_{\ovl{\nu}}\left[(\vec{m} \cdot \vec{x}_d + \ovl{\nu} \vec{m} \cdot \vec{x}_s)^2\right] = c_d^2 + \sigma^2 c_s^2
\end{equation*}
where we made use of $\esper{\ovl{\nu}} = 0$ and we have defined variables $c_d = \vec{m} \cdot \vec{x}_d$ and $c_s = \vec{m} \cdot \vec{x}_s$ for the dot products of $\vec{m}(t)$ with the constant vectors $\vec{x}_d = \frac12 \vec{x}_a + \frac12 \vec{x}_b$ and $\vec{x}_s = \vec{x}_a - \vec{x}_b$. Then, the trick is to derive dynamical equations (on the $\mu$ time scale) for those variables by taking the dot product of $\frac{d \vec{m}}{dt}$ with $\vec{x}_s$ and $\vec{x}_a$, averaging over fast time scales of $\ovl{\nu}(t)$, and using the quasi-static $\Theta$ above. For $c_s$ for instance, we calculate (using $c = c_d + \ovl{\nu} c_s$ and the orthogonality of $\vec{x}_d$ and $\vec{x}_s$ that holds when $\vec{x}_a$ and $\vec{x}_b$ have an equal norm)

\begin{align}
	\frac{d c_d}{dt} &= \esper{ \frac{d \vec{m}}{dt} \cdot \vec{x}_d} = \mu \esper{(c_d + \ovl{\nu} c_s)(c_d + \ovl{\nu} c_s - \Theta)} x_d^2  \nonumber \\
	&= \mu \left(c_d^2 + \sigma^2 c_s^2 - c_d \left(c_d^2 + \sigma^2 c_s^2 \right) \right) x_d^2 \nonumber \\
	&= \mu x_d^2 (1 - c_d)\left(c_d^2 + \sigma^2 c_s^2 \right) \,\,\, .
	\label{eq:c_d_dynamics}
\end{align}

By a similar calculation, we find for $c_s$

\beq
	\frac{d c_s}{dt} = \mu \sigma^2 x_s^2 c_s \left(2 c_d - \left(c_d^2 + \sigma^2 c_s^2 \right) \right) \,\,\, .
	\label{eq:c_s_dynamics}
\eeq

% TODO: finish typesetting analysis in March15 entry, two phases, etc. See also correction made on August 15. 
From equations \eqref{eq:c_d_dynamics} and \eqref{eq:c_s_dynamics}, we can conclude there will be two phases to the dynamics if the initial values of $c_s(0) = \epsilon_s$ and $c_d(0) = \epsilon_s$ are small, and $\sigma^2$ is small also. The only positive term in $\frac{d c_s}{dt}$ contains $c_d$; hence, as long as $c_d$ is small, $c_s$ will remain close to zero. The first phase therefore consists in the growth of $c_d$ to its steady-state value of $1$, while $c_s$ remains approximately equal to its initial value, $\epsilon_s$. We call $t_d$ its duration. After $c_d$ has converged, the second phase consists in the growth of $c_s$. We call $t_s$ the duration of that phase. Hence, $c_s$ reaches steady-state after a total time of $t_d + t_s$. 

We compute $t_d$ (first phase duration) by integrating equation \eqref{eq:c_d_dynamics} from 0 to some fraction $\alpha$ (close to unity) of the steady-state $c_d = 1$, with the assumption that $c_s^2$ is approximately constant and sub-dominant in that phase, i.e. $c_s^2 \approx \epsilon_s^2 \approx 0$. We find

\begin{align}
	\int_{\epsilon_d}^\alpha \frac{d c_d}{c_d^2(1 - c_d)} &= \int_0^{t_d} \mu x_d^2 dt  \nonumber \\
	\Rightarrow t_d &= \frac{1}{\mu x_d^2} \left[\frac{1}{\epsilon_d} - \frac{1}{\alpha} + \ln \left(\frac{\alpha(1 - \epsilon_d)}{\epsilon_d(1 - \alpha)}  \right)  \right]
	\label{eq:t_d}
\end{align}
where $x_d^2 = \| \vec{x}_d \|^2$. 

Then, once $c_d \approx 1$, the second phase starts. We neglect sub-dominant terms and we integrate from $t_d$ (time at which $c_d \approx 1$ but $c_s \approx \epsilon_s$ still) to $t_d + t_s$. We integrate $c_s$ from $\epsilon_s$ to $\pm \alpha / \sigma$:  depending on the sign of the initial value $\epsilon_s$, the system goes to either fixed point $\pm 1 / \sigma$ (same sign as the initial value). Hence, 

\begin{align}
	\frac{d c_s}{dt} &\approx \mu x_s^2 \sigma^2 c_s   \nonumber  \\
	\Rightarrow \int_{\epsilon_s}^{\pm\alpha/\sigma} \frac{d c_s}{c_s}	&= \mu x_s^2 \sigma^2 c_s \int_{t_d}^{t_s + t_d} dt  \nonumber \\
	\Rightarrow t_s = \frac{1}{\mu x_s^2 \sigma^2} \ln{\left|\frac{\alpha}{\sigma \epsilon_s}  \right|}
	\label{eq:t_s}
\end{align}

As we will show in section \ref{subsubsect:2d_model_simulations}, the approximations \eqref{eq:t_d} and \eqref{eq:t_s} hold quite well in a range of initial values $\epsilon_s$ between $0.01$ and $0.2$. Importantly, they show that convergence time is faster if initial conditions are larger (note inverse $\epsilon$ terms) and the noise $\sigma^2$ is larger. It means that initial conditions must be chosen carefully to avoid very small dot products with inputs initially. Then, larger background fluctuations seem to encourage faster convergence too, at least in this simple case. This is a surprising property of the IBCM model: fluctuations drive the dynamics. 




\subsubsection{Analytical results: PN inhibition}
\label{subsubsect:2d_model_analytical2}
From the IBCM steady-state solution, we can also compute the steady-state inhibitory weights $\vec{w}_j$ learnt in response to the background odors. Setting the average of equation \eqref{eq:learning_rule_w} to zero, taking $R$ to be the identity function (so $R'$ can be ignored), and writing out $\vec{s} = \vec{x} - W\ovl{M}\vec{x}$, 
\beq
	0 = \alpha \esper{\ovl{c}^j (\vec{x} - W\ovl{M}\vec{x})} - \beta \esper{ \vec{w}_j}  \,\,\, \forall j \quad .
	\label{eq:w_ss_2d}
\eeq

As before, we make a few approximations. First, we assume there are $n_I=2$ IBCM neurons, one at each fixed point $\pm$. Second, we treat the steady-state $\vec{\ovl{m}}$ as a constant, so at any time $t$, the IBCM neuron activity $\ovl{c}$ is given by equation \eqref{eq:ibcm_2d_response}. Third, we neglect correlations between fast $\ovl{\nu}(t)$ fluctuations and $\vec{w}$, so $\esper{\vec{w} \ovl{\nu}} = \esper{\vec{w}} \esper{\ovl{\nu}} = 0$. Fourth, we treat $\vec{w}$ as a constant equal to its average. We can then obtain a set of two equations that we can solve for $\esper{\vec{w}_+}$ and $\esper{\vec{w}_-}$, the inhibitory weights of the IBCM neurons at steady-states $+$ and $-$, respectively. Dropping expectation values on $\vec{w}_{\pm}$, this gives

\begin{align*}
	\frac{d \esper{w}_+}{dt} = 0 &= \alpha \esper{\left(1 + \frac{\ovl{\nu}(t)}{\sigma} \right) \left[\vec{x}_d + \ovl{\nu}(t) \vec{x}_s - \vec{w}_+ \left(1 + \frac{\ovl{\nu}(t)}{\sigma}\right) - \vec{w}_-  \left(1 - \frac{\ovl{\nu}(t)}{\sigma}\right) \right] } - \beta \vec{w}_+ 		 \\
	\frac{d \esper{w}_-}{dt} = 0 &= \alpha \esper{\left(1 - \frac{\ovl{\nu}(t)}{\sigma} \right) \left[\vec{x}_d + \ovl{\nu}(t) \vec{x}_s - \vec{w}_+ \left(1 + \frac{\ovl{\nu}(t)}{\sigma}\right) - \vec{w}_-  \left(1 - \frac{\ovl{\nu}(t)}{\sigma}\right) \right] } - \beta \vec{w}_- 		 \\
\end{align*}

Solving for $\vec{w}_+$ and $\vec{w}_-$ gives answers summarized as
\beq
	\esper{\vec{w}_{\pm}} = \frac{\alpha}{2 \alpha + \beta} \left( \vec{x}_d \pm \sigma \vec{x}_s \right) 
	\label{eq:inhibition_weights_2d_solution}
\eeq

Hence, each IBCM neuron inhibits exactly the off-average component for which it neuron is selective, that is, $\vec{x}(\ovl{\nu} = \pm \sigma)$. Combining the two IBCM neurons, the background, at the projection neuron layer, is reduced instantaneously to

\begin{align}
	\vec{s}(t)&= \vec{x}_d + \ovl{\nu}(t) \vec{x}_s - \ovl{c}^+ \vec{w}_+ - \ovl{c}^- \vec{w}_- \nonumber \\
			&= \vec{x}_d - \frac{\alpha}{2\alpha + \beta} \left(1 + \frac{\ovl{\nu}}{\sigma} \right) \left( \vec{x}_d + \sigma \vec{x}_s \right) - \frac{\alpha}{2\alpha + \beta} \left(1 - \frac{\ovl{\nu}}{\sigma} \right) \left( \vec{x}_d - \sigma \vec{x}_s \right)   \nonumber \\
	\vec{s}(t) &= \frac{\beta}{2 \alpha + \beta} \vec{x}(t)
		\label{eq:inhibition_pn_2d}
\end{align}
which has an average and an average squared norm of
\begin{align*}
	\esper{\vec{s}}&= \frac{\beta}{2 \alpha + \beta} \esper{\vec{x}} \\
	\esper{\vec{s}^T \vec{s}} &= \frac{\beta^2}{(2\alpha + \beta)^2} \esper{\vec{x}^T \vec{x}}
\end{align*}
Hence, by learning $n_B=2$ linearly independent components $\vec{w}^{\pm}$ that are one standard deviation away from the average background, the network is able to suppress any $\vec{x}(t)$ from that background, in real time, to a fraction $\frac{\beta}{2\alpha + \beta} = \frac{1}{11}$ of its original amplitude. Therefore, not only the average, but also the variance of the background is reduced: background fluctuations are actively suppressed by the IBCM-inhibitory neuron pairs. However, new odors are not suppressed in the same way, because they most likely have a component not in the vector space of learnt background components (and if not, they would be confused with the background anyways). 

%% TODO: show this more explicitly, detect new odors in 2D as well. 

\subsubsection{Numerical simulations}
\label{subsubsect:2d_model_simulations}

We simulated our IBCM network with a binary background mixture of components $\vec{x}_a$ and $\vec{x}_b$ depicted on figure \ref{subfig:components_2d}, which are linearly independent but not orthogonal. We used $n_R=2$ and $n_I=2$ neurons, since extra dimensions in $\vec{m}$ are constant over time and one neuron per fixed point is sufficient to achieve inhibition of $K=2$ odors. The expected steady-state synaptic vector weights are shown on figure \ref{subfig:components_2d} as well. 

\begin{figure}
  \begin{subfigure}[b]{.35\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/two_ibcm_neurons_background_mbar_w_vectors.pdf}
	\caption{Background components and steady-state weights ($\vec{m}$ and $\vec{w}$)}
	\label{subfig:components_2d}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{.64\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/two_ibcm_neurons_opposite_fixed_pts_statistics.pdf}
	\caption{Fixed point statistics}
	\label{subfig:ibcm_2neuron_distribution}
  \end{subfigure}
\caption{Two-dimensional toy model setup for numerical simulations and statistics of fixed points reached in 100 different simulations with different uniformly random initial conditions. Depicted are cases where both neurons select the same component. }
\label{fig:ibcm_2d_model}
\end{figure}

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/two_ibcm_neurons_mbar_dot_products_simulation.pdf}
	\caption{Simulated time series of $\vec{\ovl{m}}^j$}
	\label{subfig:ibcm_mbar_2d_simulation}
  \end{subfigure} \hfill
  \begin{subfigure}[b]{.49\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/ibcm_inhibition_2d_analytical_numerical_w_fixed_points.pdf}
	\caption{Simulated time series of $\vec{w}_j$ }
	\label{subfig:ibcm_w_2d_simulation}
  \end{subfigure}
  \caption{Simulated time series of $\vec{\ovl{m}}^j$ and $\vec{w}_j$, and comparison to analytical predictions of fixed points $\vec{\ovl{m}}_{\pm}$ and $\vec{w}_{\pm}$. }
\label{fig:ibcm_2d_simulations}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{.9\textwidth}
	\includegraphics[width=0.95\linewidth]{figures/inhibition_gaussian_background_neurons_2odors.pdf}
	\caption{Inhibition of each ORN}
	\label{subfig:ibcm_orn_inhibition_2d}
  \end{subfigure} \hfill
  \begin{subfigure}[b]{.9\textwidth}
	\includegraphics[width=0.7\linewidth]{figures/inhibition_gaussian_background_squarednorm_2odors.pdf}
	\caption{Inhibition of the squared norm of ORN activity }
	\label{subfig:ibcm_orn_norm_2d}
  \end{subfigure}
	\caption{Two-dimensional background odor vector fluctuations before inhibition and after inhibition by the IBCM network. Elements of $\vec{s}$ are reduced to $9.0~\%$ and $9.3~\%$ of the ORN activities, $\vec{x}$, close to the prediction $\frac{\beta}{2\alpha + \beta} = 1/11 = 9.1~\%$. Similarly, the squared norm $\vec{s}^T \vec{s}$ is reduced to $1.1~\%$ of $\vec{x}^T \vec{x}$, close to the predicted $1 / 11^2 = 0.8~\%$. Parameter values used: $\mu = 1/400$, $\tau_{\Theta} = 300$, $\eta = 0.1$, $\alpha = 5 \beta = 0.00025$. }
	\label{fig:background_inhibition_2d}
\end{figure}


\begin{figure}[tbp]
	\centering
	\begin{subfigure}[b]{.85\textwidth}
	\includegraphics[width=0.95\linewidth]{figures/ibcm_neuron_2d_convergence_time_example.pdf}
	\caption{Two-phase convergence prediction compared to simulation}
	\label{subfig:convergence_sample}
  \end{subfigure} \hfill
  \begin{subfigure}[b]{.85\textwidth}
	\includegraphics[width=0.95\linewidth]{figures/ibcm_neuron_2d_convergence_time_scaling.pdf}
	\caption{Convergence times for various $\epsilon_d$, $\epsilon_s$. }
	\label{subfig:convergence_scaling}
  \end{subfigure}
	\caption{Comparing analytical predictions for convergence times $t_d$ and $t_s$ with simulations. Parameters used: $\mu = 1/1000$, $\tau_{\Theta} = 200$, $\sigma^2 = 0.09$. }
	\label{fig:convergence_time_2d}
\end{figure}


We show that our fixed point predictions match numerical results,  for $\vec{\ovl{m}}$ in figure \ref{subfig:ibcm_mbar_2d_simulation}and for $\vec{w}$ in figure \ref{subfig:ibcm_w_2d_simulation}. We also show in figure \ref{subfig:ibcm_2neuron_distribution} that feedforward inhibition encourages the two neurons to converge to fixed points of opposite specificity (i.e. one to $\vec{\ovl{m}}^+$ and one to $\vec{\ovl{m}}^-$), when they are randomly initialized near the origin. They converge to the same fixed point only when both initial $\vec{m}$ vectors lie outside of the range of typical $\vec{x}$ fluctuations. 

Moreover, in figure \ref{fig:background_inhibition_2d}, we show the effectiveness of background inhibition following equation \eqref{eq:learning_rule_w} by the two IBCM neurons. As predicted by equation \eqref{eq:inhibition_pn_2d}, the average and standard deviation of each PN activity (elements of $\vec{s}$) are reduced by a factor close to $\frac{\beta}{2\alpha + \beta}$ compared to ORN activities (elements of $\vec{x}$). The squared norm $\vec{s}^T \vec{s}$ is reduced by a factor close to $\left(\frac{\beta}{2\alpha + \beta}\right)^2$, as predicted. Small discrepancies are due to neglecting fluctuations of $\vec{\ovl{m}}$ and $\vec{w}$ in our analytical calculations. 

Lastly, figure \ref{fig:convergence_time_2d} compares the predicted two-phase convergence times in equations \eqref{eq:t_d} and \eqref{eq:t_s} to simulations carried for various initial values $\epsilon_d = c_d(0) = \vec{m}(0) \cdot \vec{x}_d$ and $\epsilon_s = c_s(0) = \vec{m}(0) \cdot \vec{x}_s$, where $\vec{x}_d = \frac12 \left(\vec{x}_a + \vec{x}_b\right)$ and $\vec{x}_s = \vec{x}_a - \vec{x}_b$. The agreement is good considering the rough approximations made. In particular, the analytical treatment predicts that $t_d$ depends only on $\epsilon_d$, and $t_s$, only on $\epsilon_s$, which is approximately verified. 



% IBCM network on a 3-odor, symmetric Gaussian case
	% Analytics, numerics
\subsection{General fluctuating backgrounds: Gaussian case}
\label{subsect:gaussian_case}

\subsubsection{Background process}
\label{subsubsect:gaussian_back}

We now consider a more general olfactory background process in arbitrary input space dimension $n_R$ and with some number of odorants $n_B$. We denote by $\vec{\nu}(t)$ the $n_B$-dimensional vector specifying the concentration (appropriately scaled) of each odorant at time $t$, and by $\vec{x}_\gamma$ the $n_R$-dimensional vector encoding the strength at which odorant $\gamma$ activates each olfactory receptor kind. We assume that the total ORN response to the mixture of odorants is a linear combination of  the constant vectors $\vec{x}_\gamma$ with the fluctuating concentrations as coefficients:

\beq
	\vec{x}_b(t) = \sum_{\gamma} \nu_{\gamma} \vec{x}_{\gamma} = A \vec{\nu}
	\label{eq:general_linear_background}
\eeq

where $A$ is a matrix in which the column $\gamma$ contains $\vec{x}_{\gamma}$. 

The process $\vec{\nu}(t)$ can be any stochastic process; in this section, we treat the case where it is a gaussian process, more precisely a multivariate Ornstein-Uhlenbeck process. 

% TODO: write out details of MV Ornstein-Uhlenbeck. 

\subsubsection{Analytical results: IBCM fixed points}
\label{subsubsect:fixed_points_gauss}

We suppose, for simplicity\protect{\footnote{
Note that this could always be achieved by rotating and rescaling appropriately the $\vec{x}_{\gamma}$ vectors, i.e. by solving in terms of the principal components of the gaussian background. 
}}, that all odorant concentrations are independent (correlation $\rho = 0$) and identically distributed with mean $\braket{\nu}$ and variance $\sigma^2$.

% TODO: write out details of analytical derivation

In the end, we find that fixed points are defined by two equations constraining the $\bar{c}_{\gamma}$, 

\begin{align}
	\sum_{\gamma} \bar{c}_\gamma &=   \frac{1}{\braket{\nu}}		\label{eq:sum_cgammas}	\\
	\sum_{\gamma} \bar{c}_{\gamma}^2 &= \frac{1}{\sigma^2}		\label{eq:sum_cgammas2}
\end{align}


\subsubsection{Numerical simulations: background inhibition}
\label{subsubsect:simul_gauss_inhib}


\subsubsection{Numerical simulations: new odor recognition}
\label{subsubsect:simul_gauss_new}

% Fixed points
% Inhibition
% Numerical results: symmetric, 3-odor
% Performance for new odor recognition