\subsection{Network definitions: locality-sensitive hashing of odors}
\label{subsect:network_definitions}

\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{figures/shen2020_inhibition_network_diagram.png}
	\caption{Olfactory network model proposed by \cite{shen_habituation_2020} to explain odor habituation and odor recognition. The input layer represents ORNs, the intermediate layer, glomeruli, and the third layer, Kenyon cells, where odors are converted to neural tags and recognized. The inhibition neuron LN1 subtracts the average background from the glomeruli activity (middle layer), while the inhibitory neuron APL normalizes Kenyon cells' activities. Figure reproduced from the 2020 PNAS paper \cite{shen_habituation_2020}. }
	\label{fig:basic_network}
\end{figure}

The network proposed by \cite{shen_habituation_2020}, which will serve as our starting point, is shown in figure \ref{fig:basic_network}. It comprises three main layers:

\begin{enumerate}
	\item Each node in the input layer represents the activity of olfactory receptor neurons (ORN) having the same receptor.  It contains $n_R$ nodes, where $n_R=50$ typically in the fly. Odor inputs are therefore represented by vectors of $n_R$ elements that we will call $\vec{x}$, in which the $i$th element $x_i$ represents the activity of the $i$th ORN type.
	\item Nodes in the intermediate layer represent glomeruli associated each to a different ORN type. These are projection neurons in \textit{Drosophila}, tufted and mitral cells in mouse. It also contains $n_R$ nodes. We call $\vec{s}$ the vector describing the activity of nodes in that layer. 
	\item The third layer represents Kenyon cells in the mushroom body (fly), or pyramidal cells in the piriform cortex (mouse). This layer is high-dimensional, containing $n_K = 2000 = 40n_R$ cells. It is sparsely connected to the previous layer: on average, each KC receives input from $6/50 = 12 \%$ of the glomeruli. This layer achieves a sparse encoding of odors, which has been shown to act like a locality-sensitive hashing algorithm for odors \cite{dasgupta_neural_2017}. Each odor $\vec{x}$ ends up being mapped to a neural \emph{tag} $z$, \textit{i.e.}, a set of the indexes of the $5 \%$ most active KCs.  
	%In other words, many different input odors $\vec{x}$ are mapped to unique codes in the Kenyon cells layer, and similar odors $\vec{x}$ receive similar codes. 
\end{enumerate}

The three layers are supplemented by a lateral inhibitory neuron (LN1), which achieves habituation, and an anterior paired lateral inhibitory neuron (APL), which normalizes the last layer's activity to generate a neural tag where 5 \% of KCs are active. The details of the inhibition process are relevant here. The LN1 neuron receives inputs from all ORN types in the first layer and inhibits the projection neuron layer (glomeruli) according to:

\beq
	\vec{s} = \max{(\vec{x} - \vec{w}, 0)}
	\label{eq:pnas_inhibition}
\eeq
where $\max$ is applied element-wise to the vector $\vec{x} - \vec{w}$. In the early habituation phase, $w_i \leq x_i \, \forall i$, so we can simplify this to a simple subtraction. The inhibition weights vector $\vec{w}$ are learned by the lateral neuron according to what the authors call a Hebbian update rule:

\beq
	w_i(t+1) = w_i(t) + \alpha s_i(t) - \beta w_i(t) = w_i(t) + \alpha \max{(x_i(t) - w_i(t), 0)} - \beta w_i(t) 
	\label{eq:pnas_habituation_discrete}
\eeq


\subsection{Average background subtraction}
\label{subsect:average_subtraction}
If the network is exposed to a constant background odor $\vec{x}^B$, the inhibition vector $\vec{w}$ converges, based on equation~\eqref{eq:pnas_habituation_discrete}, to $\vec{w}_{ss} = \frac{\alpha}{\alpha + \beta} \vec{x}^B$ (the max operator is unnecessary because $w_i < x_i^B$). Hence, the background is suppressed down to $\vec{s} = \vec{x}^B - \frac{\alpha}{\alpha + \beta} \vec{x}^B = \frac{\beta}{\alpha + \beta} \vec{x}^B = \frac16 \vec{x}^B$ and a new odor, even if it appears at a level of 20 \% of the mixture concentration, is still recognized. 

If the background vector $\vec{x}^B(t)$ varies randomly over time, the inhibition mechanism for $\vec{w}$ amounts to computing to be the average background over a time window of duration $\frac{1}{\beta}$. To understand this property, we can simplify equation~\eqref{eq:pnas_habituation_discrete} by neglecting the $\max$ in the expression for $\vec{s}$ and taking the continuous time limit:

\beq
	\frac{d \vec{w}}{dt} = \alpha \vec{x}(t) - (\alpha + \beta) \vec{w}(t)
	\label{eq:pnas_habituation_continuous}
\eeq

where $\vec{x}(t)$ is a stochastic process. The formal solution of this equation is 
\begin{equation*}
	\vec{w}(t) = \alpha \int_0^t dt' e^{-(\alpha + \beta) (t - t')} \vec{x}(t')
\end{equation*} 
if $\vec{x}(t)$ started at some definite value $\vec{x}(0)$. Assuming the background $\vec{x}(t)$ is a stationary process before a new odor is introduced, the steady-state average value of $\vec{w}$ is therefore $\braket{\vec{w}} = \frac{\alpha}{\alpha + \beta} \braket{x}$, as it should. If the background process has an autocorrelation time scale $\tau_b \ll \frac{1}{\alpha}$, such that its elements approximately obey $\braket{\Delta x_i(t_1) \Delta x_i(t_2)} = \sigma_{x_i}^2 e^{-(t_2 - t_1)/\tau_b}$, then the variance of the inhibition weights $w_i$ is, to leading order, 

\beq 
	\variance{w_i} = \alpha \tau_b \, \frac{\alpha}{\alpha + \beta}  \sigma_{x_i}^2
\eeq

Hence, the inhibition weights do not deviate much from the average background $\braket{x}$, since their variance is suppressed by a factor $\alpha \tau_b \ll 1$ compared to the background's variance $\sigma_{x_i}^2$. In other words, this model computes $\frac{\alpha}{\alpha + \beta}$ the average background $\braket{x}$ and subtracts it from $\vec{x}$ to obtain the projection neuron layer $\vec{s}$. 

Therefore, the variance of the projection neurons is not reduced compared to the variance of the background, hence fluctuations are not suppressed and will still mix with a new odor and hide it:

\begin{align}
	\braket{\vec{s}} &= \braket{\vec{x}} - \braket{\vec{w}} = \frac{\beta}{\alpha + \beta} \braket{\vec{x}} \\
	\variance{s_i} &\sim \sigma_{x_i}^2 \,\,\, \forall i
\end{align}

\subsection{Metric for new odor detection}
\label{subsect:metric_new_odor}
The performance of an inhibition model for new odor detection is quantified as follows. The network is allowed to evolve in response to a background process $\vec{x}^B$ (constant in the original paper) over a long enough time for habituation to occur (i.e. many times $\frac{1}{\alpha}$). Then, at the next time step, a new odor $\vec{x}^n$ is added to the olfactory input, typically by taking a linear combination $\vec{x}^{\mathrm{mix}} = (1-c) \vec{x}^B + c\vec{x}^n$, where $c$ is some constant scalar in $[0, 1]$, typically $c < 0.5$ (i.e. the new odor is not dominant). 

The performance of the network is quantified by the similarity of the neural tag corresponding to $\vec{x}^{\mathrm{mix}}$, call it $z^{\mathrm{mix}}$, and the neural tag of the new odor alone (the target), call it $z^n$. The measure of similarity is the Jaccard similarity:

\beq
	J(z^{\mathrm{mix}}, z^n) = \frac{\card{z^{\mathrm{mix}} \cap z^n}}{\card{z^{\mathrm{mix}} \cup z^n}}
	\label{eq:jaccard_def}
\eeq

In other words, this similarity index (between 0 and 1) measures how many KC neurons the two tags have in common, compared to how many KC they activate in total. It is $0$ if no KC is shared and $1$ if the sets $z^n$ and $z^{\mathrm{mix}}$ are identical. 


\subsection{Shortcomings of the average inhibition model}
\label{subsect:shortcomings}

\begin{figure}
  \begin{subfigure}[b]{.49\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/habituation_constant_background_data.pdf}
	\caption{Constant background}
	\label{fig:jaccard_constant_background}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{.49\textwidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/habituation_fluctuating_proportion.pdf}
	\caption{Fluctuating binary background}
	\label{fig:jaccard_binary_background}
  \end{subfigure}
\caption{Jaccard similarity between neural tags of the background plus new odor mixture ($80 \%$ background, $20 \%$ new odor) and of the new odor alone (target), without and with habituation by average background subtraction. On the left, the background is a constant odor vector, while on the right, the background is a fluctuating binary mixture, $\vec{x}^B = \nu \vec{x}^{B1} + (1 - \nu) \vec{x}^{B2}$}
\label{fig:jaccard_average_inhibition}
\end{figure}

Figure \ref{fig:jaccard_constant_background} shows the performance of the average inhibition model proposed in \cite{shen_habituation_2020} when the background is a constant odor (no fluctuations). It increases the median similarity $J(z^n, z^{\mathrm{mix}})$ to . Odor vectors are taken from ORN activation data from \cite{hallem_coding_2006}; the statistics reported are computed over all pairs of odorants taken as $\vec{x}^B$ and $\vec{x}^{\mathrm{n}}$. 

However, the performance drops drastically when the background is a fluctuating mixture of two odorants, $\vec{x}^B = \nu \vec{x}^{B1} + (1-\nu) \vec{x}^{B2}$, where $\nu$ obeys a Ornstein-Uhlenbeck process with steady-state variance $\sigma^2_{\nu}$ and average $\esper{\nu} = 0.5$, clipped between 0 and 1. Even for small fluctuations, $\sigma_{\nu}^2 = 0.04$, the median similarity $J(z^{\mathrm{n}}, z^{\mathrm{mix}})$ drops to $0.38$, as shown in figure \ref{fig:jaccard_binary_background}. The off-average fluctuations of the background mixture can completely mask the new odor, so a simple average background subtraction model is not robust enough. 