{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations with OSN adaptation\n",
    "\n",
    "Same model of OSN saturation as in ``nonlinear_osn_turbulent_illustration.ipynb``, \n",
    "\n",
    "$$ s_i(t) = F_\\mathrm{max} \\frac{\\sum_\\gamma K_{i \\gamma} c_\\gamma}{\\exp{(\\epsilon_i(t))} + \\sum_\\gamma K_{i \\gamma} c_\\gamma} $$\n",
    "\n",
    "but with modified IBCM and BioPCA integration functions that promote $\\epsilon_i(t)$ to dynamical variables with feedback from OSN activity $s_i(t)$ and a target amplitude\n",
    "\n",
    "$$ \\frac{\\mathrm{d} \\epsilon_i(t)}{\\mathrm{d} t} = \\frac{1}{\\tau_\\mathrm{a}} \\left( s_i(t) - s_{i, 0} \\right) $$\n",
    "\n",
    "where we also clip (i.e. stop updating beyond this range) $\\epsilon \\in [\\epsilon_L, \\epsilon_H]$ to prevent divergences arising from a continued excess or deficit of OSN activity. In other words, adaptation only occurs on a finite range.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    ibcm_respond_new_odors,  # Unchanged if we give inputs nonlinearized with correct epsilon\n",
    "    compute_mbars_cgammas_cbargammas,  # Unchanged\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    build_lambda_matrix,  \n",
    "    biopca_respond_new_odors  # Unchanged if we give nonlinearized inputs with correct epsilon\n",
    ")\n",
    "# Do not consider average or idealized subtraction here\n",
    "\n",
    "from modelfcts.checktools import (\n",
    "    check_conc_samples_powerlaw_exp1,\n",
    "    analyze_pca_learning  # Unchanged if give pre-computed nonlinear inputs\n",
    ")\n",
    "from utils.metrics import jaccard, l2_norm\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_inverse_transform, \n",
    "    truncexp1_density, \n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform,\n",
    "    inverse_transform_tanhcdf\n",
    ")\n",
    "# re-use functions for nonlinear OSNs, will need to put \n",
    "# updated epsilon in back_params at each step\n",
    "from modelfcts.nonlin_adapt_osn import (  \n",
    "    generate_odor_tanhcdf, \n",
    "    combine_odors_affinities, \n",
    "    update_powerlaw_times_concs_affinities,\n",
    "    sample_background_powerlaw_nl_osn\n",
    ")\n",
    "from modelfcts.backgrounds import (  #\n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw,   # unchanged\n",
    "    update_tc_odor  # unchanged\n",
    ")\n",
    "from modelfcts.tagging import (  # unchanged\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import moving_average\n",
    "\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"nonlin_adapt\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"nonlin_adapt\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)\n",
    "    \n",
    "models = list(model_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\",  \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\",\n",
    "    \"optimal\": \"Optimal\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\",\n",
    "    \"optimal\": \"xkcd:powder blue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main new simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_ibcm_adaptation(vari_inits, update_bk, bk_init,\n",
    "    ibcm_params, inhib_params, bk_params, adapt_params, tmax, dt,\n",
    "    seed=None, noisetype=\"normal\", skp=1, **options):\n",
    "    r\"\"\" See docs of integrate_inhib_ibcm_network_options. Differences:\n",
    "    \n",
    "    Args:\n",
    "        vari_inits, update_bk, bk_init, ibcm_params, inhib_params, bk_params, \n",
    "        adapt_params, tmax, dt, seed=None, noisetype=\"normal\", skp=1, **options\n",
    "        \n",
    "        adapt_params (list of 3 floats, 1 vector): epsilon adaptation time scale, \n",
    "            lower and upper limits on epsilon, target osn activities.  \n",
    "            \n",
    "        Moreover, we assume that bk_params[-2] is the vector of epsilons\n",
    "    \n",
    "    Returns:\n",
    "        [tseries, bk_series, bkvec_series, eps_series, m_series,\n",
    "        cbar_series, theta_series, w_series, y_series]\n",
    "        \n",
    "        eps_series: shaped [n_times, n_s], the valud of each OSN type's\n",
    "            epsilon at each time point. \n",
    "    \"\"\"\n",
    "    # Get some of the keyword arguments\n",
    "    saturation = options.get(\"saturation\", \"linear\")\n",
    "    variant = options.get(\"variant\", \"intrator\")\n",
    "    activ_fct = str(options.get(\"activ_fct\", \"ReLU\")).lower()\n",
    "    decay = options.get(\"decay\", False)\n",
    "    w_norms = options.get(\"w_norms\", (2, 2))\n",
    "\n",
    "    # Legacy option to just pass initial M\n",
    "    if isinstance(vari_inits, np.ndarray):\n",
    "        m_init = vari_inits\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "        w_init = np.zeros([n_orn, n_neu])\n",
    "        theta_init = None\n",
    "    elif isinstance(vari_inits, list) and len(vari_inits) == 1:\n",
    "        m_init = np.asarray(vari_inits[0])\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "        w_init = np.zeros([n_orn, n_neu])\n",
    "        theta_init = None\n",
    "    else:\n",
    "        m_init, theta_init, w_init = vari_inits\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_orn == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    alpha, beta = inhib_params\n",
    "    learnrate, tavg, coupling, lambd, sat, ktheta, decay_relative = ibcm_params\n",
    "    # Compensate for lambda different from 1, if applicable\n",
    "    mu_abs = learnrate / lambd\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt*skp)\n",
    "\n",
    "    # Check that the biggest matrices, W or M, will not use too much memory\n",
    "    if tseries.shape[0] * n_orn * n_neu > 5e8 / 8:  # 500 MB per series max\n",
    "        raise ValueError(\"Excessive memory use by saved series; increase skp\")\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_orn])\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])\n",
    "    w_series = np.zeros([tseries.shape[0], n_orn, n_neu])  # Inhibitory weights\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_orn])  # Input vecs, convenient to compute inhibited output\n",
    "    y_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    theta_series = np.zeros([tseries.shape[0], n_neu])\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    m = m_init.copy()\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    c = m.dot(bkvec)  # un-inhibited neuron activities\n",
    "    # Initialize neuron activity with m and background at time zero\n",
    "    cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "    if saturation == \"tanh\":\n",
    "        sat_abs = sat * lambd\n",
    "        cbar = sat_abs * np.tanh(cbar / sat_abs)\n",
    "    else: sat_abs = None\n",
    "    if theta_init is None:\n",
    "        # Important to initialize cbar2_avg to non-zero values, because we divide by this!\n",
    "        cbar2_avg = np.maximum(cbar*cbar / lambd, learnrate*lambd)\n",
    "    else:\n",
    "        cbar2_avg = theta_init.copy()\n",
    "    wmat = w_init.copy()\n",
    "    yvec = bk_vec_init - wmat.dot(cbar)\n",
    "    if activ_fct == \"relu\":\n",
    "        relu_inplace(yvec)\n",
    "    elif activ_fct == \"identity\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation fct: {}\".format(activ_fct))\n",
    "        \n",
    "    # New parameters and initialization for nonlinear OSN model, epsilon\n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    eps_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    # epsilon gets initialized to midpoint between min and max\n",
    "    epsvec = np.full(n_orn, 0.5*(eps_min + eps_max))\n",
    "    assert bk_params[-2].shape == epsvec.shape, \"Ensure vector of epsilons is in bk_params[-2]\"\n",
    "    bk_params[-2] = epsvec\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    cbar_series[0] = cbar\n",
    "    bk_series[0] = bk_vari\n",
    "    m_series[0] = m_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    y_series[0] = yvec\n",
    "    theta_series[0] = cbar2_avg\n",
    "    w_series[0] = wmat\n",
    "    eps_series[0] = epsvec\n",
    "\n",
    "    # Generate noise samples in advance, by chunks of at most 2e7 samples\n",
    "    if noisetype == \"normal\":\n",
    "        sample_fct = rng.standard_normal\n",
    "    elif noisetype == \"uniform\":\n",
    "        sample_fct = rng.random\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    max_chunk_size = int(2e7)\n",
    "    # step multiple at which we run out of noises and need to replenish\n",
    "    kchunk = max_chunk_size // bk_vari.size\n",
    "    max_n_steps = len(tseries)*skp-1  # vs total number of steps\n",
    "\n",
    "    t = 0\n",
    "    newax = np.newaxis\n",
    "    for k in range(0, max_n_steps):\n",
    "        t += dt\n",
    "        # Replenish noise samples if necessary\n",
    "        if k % kchunk == 0:\n",
    "            steps_left = max_n_steps - k\n",
    "            noises = sample_fct(size=(min(kchunk, steps_left), *bk_vari.shape))\n",
    "        \n",
    "        ### Inhibitory  weights\n",
    "        # They depend on cbar and yvec at time step k, which are still in cbar, yvec\n",
    "        # cbar, shape [n_neu], should broadcast against columns of wmat,\n",
    "        # while yvec, shape [n_orn], should broadcast across rows (copied on each column)\n",
    "        if w_norms[0] == 2:  # default L2 norm, nice and smooth\n",
    "            alpha_term = alpha * cbar[newax, :] * yvec[:, newax]\n",
    "        elif w_norms[0] == 1:  # L1 norm\n",
    "            aynorm = alpha * l1_norm(yvec)\n",
    "            alpha_term = aynorm * cbar[newax, :] * np.sign(yvec[:, newax])\n",
    "        elif w_norms[0] > 2:  # Assuming some Lp norm with p > 2\n",
    "            # Avoid division by zero for p > 2 by clipping ynorm\n",
    "            ynorm = max(1e-9, lp_norm(yvec, p=w_norms[0]))\n",
    "            yterm = np.sign(yvec) * np.abs(yvec/ynorm)**(w_norms[0]-1) * ynorm\n",
    "            alpha_term = alpha * cbar[newax, :] * yterm[:, newax]\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        if w_norms[1] == 2:\n",
    "            beta_term = beta * wmat\n",
    "        elif w_norms[1] == 1:\n",
    "            beta_term = beta * l1_norm(wmat.ravel()) * np.sign(wmat)\n",
    "        elif w_norms[1] > 2:\n",
    "            wnorm = max(1e-9, lp_norm(wmat.ravel(), p=w_norms[1]))\n",
    "            wterm = np.sign(wmat) * np.abs(wmat/wnorm)**(w_norms[1]-1)\n",
    "            beta_term = beta * wterm * wnorm\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        wmat += dt * (alpha_term - beta_term)\n",
    "\n",
    "        ### IBCM neurons\n",
    "        # Phi function for each neuron.\n",
    "        if variant == \"intrator\":\n",
    "            phiterms_vec = cbar * (cbar - cbar2_avg)\n",
    "        #  Law and Cooper modification for faster convergence.\n",
    "        elif variant == \"law\":\n",
    "            phiterms_vec = cbar * (cbar - cbar2_avg) / (ktheta + cbar2_avg/lambd)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown variant: {}\".format(variant))\n",
    "\n",
    "        if saturation == \"tanh\":\n",
    "            phiterms_vec *=  1.0 - (cbar/sat_abs)**2\n",
    "        # Now, careful with broadcast: for each neuron (dimension 0 of m and cbar), we need a scalar element\n",
    "        # of phiterms_vec times the whole bkvec, for dimension 1 of m.\n",
    "        # This can be done vectorially with a dot product (n_neu, 1)x(1, n_components)\n",
    "        rhs_scalar = phiterms_vec - coupling*(np.sum(phiterms_vec) - phiterms_vec)\n",
    "        # Euler integrator and learning rate\n",
    "        # learnrate_t = learnrate if t < 150000 else learnrate / 5\n",
    "        # Reducing learning rate after a while may help.\n",
    "        # Consider feedback on mu through some metric of how well neurons\n",
    "        # are inhibiting the background, e.g. s average activity.\n",
    "        m += mu_abs*dt*rhs_scalar[:, np.newaxis].dot(bkvec[np.newaxis, :])\n",
    "        # In principle, should add low decay to background subspace\n",
    "        # To make sure 1) only learn the background space, 2) de-habituate after\n",
    "        # The decay term is proportional to m, not m^2 like the IBCM term\n",
    "        # so we needed to divide learnrate by Lambda for the IBCM term\n",
    "        # but not for this linear decay term, which should use just learnrate\n",
    "        if decay and variant == \"law\":\n",
    "            m -= dt * decay_relative * learnrate / (ktheta + cbar2_avg[:, np.newaxis]/lambd) * m\n",
    "        elif decay and variant == \"intrator\":\n",
    "            m -= dt * decay_relative * learnrate * m\n",
    "        # Now, update to time k+1 the threshold (cbar2_avg) using cbar at time k\n",
    "        # to be used to update m in the next time step\n",
    "        cbar2_avg += dt * (cbar*cbar / lambd - cbar2_avg)/tavg\n",
    "        \n",
    "        # Adapt OSNs in response to background at time t\n",
    "        epsvec += dt / tau_eps * (bkvec - osn_targets)\n",
    "        epsvec = np.clip(epsvec, a_min=eps_min, a_max=eps_max)\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k % kchunk], dt)\n",
    "        \n",
    "        # Store updated epsilon in bk_params for the next background update\n",
    "        bk_params[-2] = epsvec\n",
    "\n",
    "        # Then, compute activity of IBCM neurons at next time step, k+1,\n",
    "        # with the updated background and synaptic weight vector m\n",
    "        # Compute un-inhibited activity of each neuron with current input (at time k)\n",
    "        # With many simulations in parallel, there seemed to be a bottleneck here\n",
    "        # and also at yvec calculation: turns out it's because of BLAS multithreading\n",
    "        # So for multiprocessing, this function should be launched in a threadpool_limits\n",
    "        c = m.dot(bkvec)\n",
    "        cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "        if saturation == \"tanh\":\n",
    "            cbar = sat_abs * np.tanh(cbar / sat_abs)\n",
    "        # np.sum(c) is a scalar and c a vector, so it broadcasts properly.\n",
    "\n",
    "        # Lastly, projection neurons at time step k+1\n",
    "        yvec = bkvec - wmat.dot(cbar)\n",
    "        if activ_fct == \"relu\":\n",
    "            relu_inplace(yvec)\n",
    "\n",
    "        # Save current state only if at a multiple of skp\n",
    "        if (k % skp) == (skp - 1):\n",
    "            knext = (k+1) // skp\n",
    "            w_series[knext] = wmat\n",
    "            m_series[knext] = m\n",
    "            bk_series[knext] = bk_vari\n",
    "            bkvec_series[knext] = bkvec\n",
    "            cbar_series[knext] = cbar  # Save activity of neurons at time k+1\n",
    "            y_series[knext] = yvec\n",
    "            theta_series[knext] = cbar2_avg\n",
    "            eps_series[knext] = epsvec\n",
    "\n",
    "    return [tseries, bk_series, bkvec_series, eps_series, m_series,\n",
    "            cbar_series, theta_series, w_series, y_series]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_biopca_adaptation(ml_inits, update_bk, bk_init,\n",
    "                biopca_params, inhib_params, bk_params, adapt_params, tmax, dt,\n",
    "                seed=None, noisetype=\"normal\", skp=1, **model_options):\n",
    "    \"\"\"\n",
    "     See docs of integrate_inhib_biopca_network_skip. Differences:\n",
    "    \n",
    "    Args:\n",
    "        ml_inits, update_bk, bk_init, biopca_params, inhib_params, bk_params, \n",
    "        adapt_params, tmax, dt, seed=None, noisetype=\"normal\", skp=1, **model_options\n",
    "        \n",
    "        adapt_params (list of 3 floats, 1 vector): epsilon adaptation time scale, \n",
    "            lower and upper limits on epsilon, target osn activities.  \n",
    "            \n",
    "        Moreover, we assume that bk_params[-2] is the vector of epsilons\n",
    "    \n",
    "    Returns:\n",
    "        [tseries, bk_series, bkvec_series, eps_series, m_series,\n",
    "        cbar_series, theta_series, w_series, y_series]\n",
    "        \n",
    "        eps_series: shaped [n_times, n_s], the valud of each OSN type's\n",
    "            epsilon at each time point. \n",
    "    \"\"\"\n",
    "    remove_mean = model_options.get(\"remove_mean\", False)\n",
    "    remove_lambda = model_options.get(\"remove_lambda\", False)\n",
    "    activ_fct = str(model_options.get(\"activ_fct\", \"ReLU\")).lower()\n",
    "    w_norms = model_options.get(\"w_norms\", (2, 2))\n",
    "    m_init, l_init = ml_inits\n",
    "    # Note: keep lambda matrix as 1d diagonal only, replace dot products by:\n",
    "    # Lambda.dot(A): Lambda_ii applied to row i, replace by Lambda_diag[:, None]*A element-wise\n",
    "    # A.dot(Lambda): Lambda_ii applied to column i, just A*Lambda broadcasts right\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_orn = m_init.shape[1]  # Number of input neurons N_ORN\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_orn == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    alpha, beta = inhib_params\n",
    "    # xrate will be a dummy value if remove_mean == False\n",
    "    mrate, lrate, lambda_max, lambda_range, xrate = biopca_params\n",
    "    lrate_l = lrate / lambda_max**2\n",
    "\n",
    "    # Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "    lambda_diag = build_lambda_matrix(lambda_max, lambda_range, n_neu)\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt*skp)\n",
    "\n",
    "    # Check that the biggest matrices, W or M, will not use too much memory\n",
    "    if tseries.shape[0] * n_orn * n_neu > 5e8 / 8:  # 500 MB per series max\n",
    "        raise ValueError(\"Excessive memory use by saved series; increase skp\")\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_orn])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of projections\n",
    "    w_series = np.zeros([tseries.shape[0], n_orn, n_neu])  # Inhibitory weights W (N_DxN_I)\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_orn])  # Input vecs, convenient to compute inhibited output\n",
    "    y_series = np.zeros([tseries.shape[0], n_orn])  # series of proj. neurons\n",
    "    if remove_mean:\n",
    "        xmean_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    else:\n",
    "        xmean_series = None\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities (before applying L)\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L)\n",
    "    wmat = w_series[0].copy()  # Initialize with null inhibition\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    yvec = bk_vec_init.copy()\n",
    "    if activ_fct == \"relu\":\n",
    "        relu_inplace(yvec)\n",
    "    elif activ_fct == \"identity\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation function: {}\".format(activ_fct))\n",
    "    if remove_mean:\n",
    "        xmean = np.zeros(bkvec.shape)\n",
    "    else:\n",
    "        xmean = 0.0\n",
    "\n",
    "    # Inverse of diagonal of L is used a few times per iteration\n",
    "    # Indices to access diagonal and off-diagonal elements of L\n",
    "    # Will be used often, so prepare in advance. Replace dot product\n",
    "    # with diagonal matrix by element-wise products.\n",
    "    diag_idx = np.diag_indices(l_init.shape[0])\n",
    "    inv_l_diag = 1.0 / l_init[diag_idx]  # 1d flattened diagonal\n",
    "    # Use this difference the only time M_d is needed per iteration\n",
    "    # Faster to re-invert inv_l_diag than to slice lmat again\n",
    "    # l_offd = lmat - dflt(1.0 / inv_l_diag)  # is faster than\n",
    "    # l_offd = lmat - dflt(lmat[diag_idx])\n",
    "    newax = np.newaxis\n",
    "    dflt = np.diagflat\n",
    "\n",
    "    # Initialize neuron activity with m and background at time zero\n",
    "    c = inv_l_diag * (mmat.dot(bkvec - xmean))\n",
    "    cbar = c - inv_l_diag*np.dot(lmat-dflt(1.0 / inv_l_diag), c)\n",
    "    if remove_lambda:\n",
    "        cbar = cbar / lambda_diag\n",
    "        \n",
    "    # New parameters and initialization for nonlinear OSN model, epsilon\n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    eps_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    # epsilon gets initialized to midpoint between min and max\n",
    "    epsvec = np.full(n_orn, 0.5*(eps_min + eps_max))\n",
    "    assert bk_params[-2].shape == epsvec.shape, \"Ensure vector of epsilons is in bk_params[-2]\"\n",
    "    bk_params[-2] = epsvec\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    cbar_series[0] = cbar\n",
    "    bk_series[0] = bk_vari\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    y_series[0] = yvec\n",
    "    eps_series[0] = epsvec\n",
    "    \n",
    "    # Generate noise samples in advance, by chunks of at most 2e7 samples\n",
    "    if noisetype == \"normal\":\n",
    "        sample_fct = rng.standard_normal\n",
    "    elif noisetype == \"uniform\":\n",
    "        sample_fct = rng.random\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    max_chunk_size = int(2e7)\n",
    "    # step multiple at which we run out of noises and need to replenish\n",
    "    kchunk = max_chunk_size // bk_vari.size\n",
    "    max_n_steps = len(tseries)*skp-1  # vs total number of steps\n",
    "\n",
    "    t = 0\n",
    "    newax = np.newaxis\n",
    "    for k in range(0, max_n_steps):\n",
    "        # Replenish noise samples if necessary\n",
    "        if k % kchunk == 0:\n",
    "            steps_left = max_n_steps - k\n",
    "            noises = sample_fct(size=(min(kchunk, steps_left), *bk_vari.shape))\n",
    "        \n",
    "        # Learning the mean: independent of everything else.\n",
    "        if remove_mean:\n",
    "            xmean = xmean + dt * xrate * (bkvec - xmean)\n",
    "        # Else, xmean stays 0\n",
    "\n",
    "        ### Inhibitory  weights\n",
    "        # They depend on cbar and yvec at time step k, which are still in cbar, yvec\n",
    "        # cbar, shape [n_neu], should broadcast against columns of wmat,\n",
    "        # while yvec, shape [n_orn], should broadcast across rows (copied on each column)\n",
    "        if w_norms[0] == 2:  # default L2 norm, nice and smooth\n",
    "            alpha_term = alpha * cbar[newax, :] * yvec[:, newax]\n",
    "        elif w_norms[0] == 1:  # L1 norm\n",
    "            aynorm = alpha * l1_norm(yvec)\n",
    "            alpha_term = aynorm * cbar[newax, :] * np.sign(yvec[:, newax])\n",
    "        elif w_norms[0] > 2:  # Assuming some Lp norm with p > 2\n",
    "            # Avoid division by zero for p > 2 by clipping ynorm\n",
    "            ynorm = max(1e-9, lp_norm(yvec, p=w_norms[0]))\n",
    "            yterm = np.sign(yvec) * np.abs(yvec/ynorm)**(w_norms[0]-1) * ynorm\n",
    "            alpha_term = alpha * cbar[newax, :] * yterm[:, newax]\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        if w_norms[1] == 2:\n",
    "            beta_term = beta * wmat\n",
    "        elif w_norms[1] == 1:\n",
    "            beta_term = beta * l1_norm(wmat.ravel()) * np.sign(wmat)\n",
    "        elif w_norms[1] > 2:\n",
    "            wnorm = max(1e-9, lp_norm(wmat.ravel(), p=w_norms[1]))\n",
    "            wterm = np.sign(wmat) * np.abs(wmat/wnorm)**(w_norms[1]-1)\n",
    "            beta_term = beta * wterm * wnorm\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        wmat += dt * (alpha_term - beta_term)\n",
    "\n",
    "        ### Online PCA weights\n",
    "        # Synaptic plasticity: update mmat, lmat to k+1 based on cbar at k\n",
    "        mmat += dt * mrate * (cbar[:, newax].dot(bkvec[newax, :]) - mmat)\n",
    "        lmat += dt * mrate * lrate_l * (cbar[:, newax].dot(cbar[newax, :])\n",
    "                        - lambda_diag[:, newax] * lmat * lambda_diag)\n",
    "        # Update too the variable saving the inverse of the diagonal of L\n",
    "        inv_l_diag = 1.0 / lmat[diag_idx]\n",
    "\n",
    "        t += dt\n",
    "        \n",
    "        # Adapt OSNs in response to background at time t\n",
    "        epsvec += dt / tau_eps * (bkvec - osn_targets)\n",
    "        epsvec = np.clip(epsvec, a_min=eps_min, a_max=eps_max)\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k % kchunk], dt)\n",
    "        \n",
    "        # Store updated epsilon in bk_params for the next background update\n",
    "        bk_params[-2] = epsvec\n",
    "\n",
    "        # Neural dynamics (two-step) at time k+1, to be used in next step\n",
    "        c = inv_l_diag * (mmat.dot(bkvec - xmean))  # L_d^(-1) M^T x\n",
    "        # Lateral inhibition between neurons\n",
    "        cbar = c - inv_l_diag*np.dot(lmat - dflt(1.0/inv_l_diag), c)\n",
    "        if remove_lambda:\n",
    "            # Remove the Lambda scale of eigenvectors, so the W matrix does\n",
    "            # not need to compensate too much.\n",
    "            # So we use Lambda^{-1}L^{-1}M as a projector as prescribed in Minden 2018\n",
    "            cbar = cbar / lambda_diag\n",
    "\n",
    "        # Lastly, projection neurons at time step k+1.\n",
    "        # xmean is 0 if we don't remove the mean\n",
    "        yvec = bkvec - xmean - wmat.dot(cbar)\n",
    "        if activ_fct == \"relu\":\n",
    "            relu_inplace(yvec)\n",
    "\n",
    "        # Save current state only if at a multiple of skp\n",
    "        if (k % skp) == (skp - 1):\n",
    "            knext = (k+1) // skp\n",
    "            w_series[knext] = wmat\n",
    "            m_series[knext] = mmat\n",
    "            l_series[knext] = lmat\n",
    "            bk_series[knext] = bk_vari\n",
    "            bkvec_series[knext] = bkvec\n",
    "            cbar_series[knext] = cbar  # Save activity of neurons at time k+1\n",
    "            y_series[knext] = yvec\n",
    "            eps_series[knext] = epsvec\n",
    "            if remove_mean:\n",
    "                xmean_series[knext] = xmean\n",
    "    return (tseries, bk_series, bkvec_series, eps_series, m_series, l_series,\n",
    "                xmean_series, cbar_series, w_series, y_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly number\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "# Common parameters for toy and full simulations\n",
    "inhib_rates = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Simulation skipping, 50 is enough for plots\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "combine_fct = combine_odors_affinities\n",
    "update_fct = update_powerlaw_times_concs_affinities\n",
    "\n",
    "# Scale of affinity vectors: default\n",
    "kscale = 5e-4  # default is 5e-4\n",
    "\n",
    "# OSN target activity and epsilon ranges\n",
    "# TODO; maybe adjust given the odor vectors, f_max scale, etc. \n",
    "target_osn_activ = np.full(n_dimensions, 1.0 / np.sqrt(n_dimensions))\n",
    "adaptation_params = [\n",
    "    25.0,  # tau_adapt = 250 ms\n",
    "    1.0,  # eps_min, allow quite low\n",
    "    10.0,  # eps_max\n",
    "    target_osn_activ \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_back_params(adapt_params, rgen, n_comp, n_dim):\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms = default_background_params(n_comp)\n",
    "    \n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    epsils_vec = np.full(n_dim, 0.5 * (eps_min + eps_max))\n",
    "    back_comps = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Same adjustment of the OSN amplitude as in the performance recognition tests\n",
    "    raw_conc_factor = 2.5\n",
    "    raw_ampli = 2.5\n",
    "    np_statistic = np.mean  # np.mean, np.median, np.amax\n",
    "\n",
    "    raw_osn_activ = np_statistic(combine_fct(np.full(n_comp, raw_conc_factor * avg_whiff_conc), \n",
    "                                        back_comps, epsils_vec, fmax=1.0))\n",
    "    max_osn_ampli = raw_ampli / (raw_osn_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(max_osn_ampli)\n",
    "    back_pms.append(epsils_vec)\n",
    "    back_pms.append(back_comps)\n",
    "\n",
    "    # Initialization\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps, epsils_vec, fmax=max_osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_pms, init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters, same for each tested epsilon\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.001  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.7/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.5\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global IBCM parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_adapt(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing IBCM simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "    \n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen.standard_normal(size=[n_i_ibcm, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_ibcm_adaptation(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params_local, \n",
    "                adapt_params, duration, deltat, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_odors_in_manifold(back_pms, conc_ser, new_conc_rel, rgen, n_ex=2, n_samp=10):\n",
    "    \"\"\"Mix n_ex new odors with n_samp background samples each. \n",
    "    Returns a 3d-array of mixtures, indexed [n_ex, n_samp, n_dim], \n",
    "    and the new odor vectors, a 2d array indexed [n_ex, n_dim]. \n",
    "    \"\"\"\n",
    "    back_odors = back_pms[-1]\n",
    "    n_comp, n_dim = back_odors.shape[0], back_odors.shape[1]\n",
    "    max_ampli = back_pms[-3]\n",
    "    new_odors = generate_odor_tanhcdf((n_ex, n_dim), rgen, unit_scale=kscale)\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_conc = avg_whiff_conc * new_conc_rel\n",
    "    non_null_concs = conc_ser[np.any(conc_ser > 0.0, axis=1)]\n",
    "    epsils_vec = back_pms[-2]\n",
    "    back_concs = non_null_concs[rgen.choice(non_null_concs.shape[0], size=n_ex*n_samp, replace=True)]\n",
    "    back_concs = back_concs.reshape(n_ex, n_samp, n_comp)\n",
    "    all_mixed_samples = []\n",
    "    for i in range(n_ex):\n",
    "        joint_kmats = np.concatenate([back_odors, new_odors[i:i+1]], axis=0)\n",
    "        mixed_samples_i = []\n",
    "        for j in range(n_samp):\n",
    "            joint_concs = np.concatenate([back_concs[i, j:j+1], np.full((1, 1), new_conc)], axis=1)\n",
    "            mixed_samples_i.append(combine_fct(joint_concs, joint_kmats, epsils_vec, fmax=max_ampli))\n",
    "        mixed_samples_i = np.concatenate(mixed_samples_i, axis=0)\n",
    "        all_mixed_samples.append(mixed_samples_i)\n",
    "        \n",
    "    return np.stack(all_mixed_samples, axis=0), new_odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def analyze_clean_ibcm_simul(results_raw, back_pms, rgenseed, n_ex=2, n_samp=10, t_mix=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "    Returns:\n",
    "        cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, eps_ser, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    back_components = back_pms[-1]\n",
    "    basis = back_components / l2_norm(back_components, axis=1)[:, None] \n",
    "    \n",
    "\n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                mser_ibcm, coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = nuser_ibcm[:, :, 1]\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    ysernorm_ibcm = l2_norm(yser_ibcm, axis=1)\n",
    "    \n",
    "    # Examples of mixing new odors with the background\n",
    "    rgen = np.random.default_rng(np.random.SeedSequence(rgenseed).spawn(2)[1])\n",
    "    back_pms_local = list(back_pms)\n",
    "    back_pms_local[-2] = eps_ser[t_mix]\n",
    "    mixres = mix_new_odors_in_manifold(back_pms_local, conc_ser, 1.0, rgen, n_ex=n_ex, n_samp=n_samp)\n",
    "    mixed_samples, new_odors = mixres\n",
    "    results_clean = (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, moments_conc, \n",
    "                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "                     conc_ser, mixed_samples, new_odors)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, res_ibcm_clean):\n",
    "    (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, \n",
    "         moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc, \n",
    "         back_comps, conc_ser, _, _) = res_ibcm_clean\n",
    "\n",
    "    # Plot of cbars gamma series\n",
    "    fig , ax, _ = plot_cbars_gamma_series(tser_common, cbars_gamma, \n",
    "                            skp=2, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plots of neuron specificities\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(correl_c_conc.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    print(\"Odor specificities:\", specif_gammas)\n",
    "    split_val = 2.5\n",
    "    for comp in range(n_components):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_ibcm_raw[2], res_ibcm_raw[8], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # TODO: plot epsilon vector series?\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global BioPCA parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation_adapt(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing BioPCA simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background parameters, give same rgenseed as IBCM to have same background\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "        \n",
    "    init_synapses_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_biopca_adaptation(\n",
    "                ml_inits_pca, update_fct, init_back, biopca_rates, \n",
    "                inhib_rates, back_params_local, adapt_params, duration, deltat, \n",
    "                seed=simseed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    We do not need to save odor vectors (back_components), \n",
    "    since the IBCM simulation will provide them for both models. \n",
    "    \n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, cbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "            learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, eps_ser, mser_pca, lser_pca, xser_pca, \n",
    "         cbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (bkvecsernorm_pca, eps_ser, ysernorm_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_biopca_results(res_biopca_raw, res_biopca_clean):\n",
    "    (bkvecsernorm_pca, epser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "    learnt_pca, off_diag_l_avg_abs, align_error_ser) = res_biopca_clean\n",
    "\n",
    "    # Plot learnt vs true PCA\n",
    "    fig, axes = plot_pca_results(tser_common/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "    axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "    axes[0].get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 2.5*plt.rcParams[\"figure.figsize\"][1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot level of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_biopca_raw[2], res_biopca_raw[9], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional plotting functions, to visualize the manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will make similar plots, define functions\n",
    "def plot_manifold(bkser, bkvecs, conc_ser, view_params, \n",
    "                  mixed_new_odors=None, new_odor_vec=None, dims=(0, 1, 2)):\n",
    "    # Plot 2D manifold in a 3D slice,\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    # Too many combinations for 6 odors, maybe just highlight\n",
    "    # single-odor axes\n",
    "    where_each = (conc_ser > 0).astype(bool)\n",
    "    n_odors = where_each.shape[1]\n",
    "    locations = {}\n",
    "    # Track places with 0 or 1 odor\n",
    "    any_single_odor = np.all(where_each == False, axis=1)  # start with places with 0 odor\n",
    "    for i in range(n_odors):\n",
    "        mask = np.zeros((1, n_odors), dtype=bool)\n",
    "        mask[0, i] = True\n",
    "        locations[\"Odor {}\".format(i)] = np.all(where_each == mask, axis=1)\n",
    "        any_single_odor += locations[\"Odor {}\".format(i)]  # add places with odor i only\n",
    "    locations[\"2+ odors\"] = np.logical_not(any_single_odor)  # 2+ odors anywhere else\n",
    "    single_odor_colors = sns.color_palette(\"colorblind\", n_colors=n_odors)\n",
    "    all_colors = {\"Odor {}\".format(i): single_odor_colors[i] for i in range(n_odors)}\n",
    "    all_colors[\"2+ odors\"] = \"grey\"\n",
    "    \n",
    "    orig = np.zeros([3, 6])\n",
    "    locations_order = [\"2+ odors\"] + [\"Odor {}\".format(i) for i in range(n_odors)]\n",
    "    for lbl in locations_order:\n",
    "        alpha = 0.3 if lbl.startswith(\"2+\") else 1.0\n",
    "        slc = locations[lbl]\n",
    "        tskp = 5 if lbl.startswith(\"2+\") else 1\n",
    "        zshift = 0.03 if lbl.startswith(\"2+\") else 0.0\n",
    "        shift = 0.03 if lbl.startswith(\"2+\") else 0.0\n",
    "        lbl_append = \"\"# if lbl.startswith(\"2+\") else \" alone\"\n",
    "        bk_subset = [bkser[slc, d].copy() + shift for d in dims]\n",
    "        bk_subset[2] -= zshift\n",
    "        ax.scatter(bk_subset[0][::tskp], bk_subset[1][::tskp], bk_subset[2][::tskp], \n",
    "                   s=4, lw=0.3, label=lbl+lbl_append, color=all_colors[lbl], alpha=alpha)\n",
    "    vecs = bkvecs / l2_norm(bkvecs, axis=1)[:, None]\n",
    "    print(vecs.shape)\n",
    "    ax.quiver(*orig, *(vecs[:, dims].T), color=\"k\", lw=1.5, arrow_length_ratio=0.2)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=25)\n",
    "    \n",
    "    # Also show what adding a new odor can do -- out of the manifold?\n",
    "    new_odor_lbl = \"+ new odor\"\n",
    "    if mixed_new_odors is not None:\n",
    "        n_new_odors = mixed_new_odors.shape[0]\n",
    "        new_odors_palette = sns.dark_palette(\"r\", n_colors=n_new_odors+1)[1:]\n",
    "        for i in range(n_new_odors):\n",
    "            lbl = new_odor_lbl + \" {}\".format(\"abcdefghijklmnop\".upper()[i])\n",
    "            all_colors[lbl] = new_odors_palette[i]\n",
    "            ax.scatter(mixed_new_odors[i, :, dims[0]], mixed_new_odors[i, :, dims[1]], \n",
    "                        mixed_new_odors[i, :, dims[2]], s=6, lw=0.3, \n",
    "                        label=lbl+lbl_append, color=all_colors[lbl], alpha=1.0)\n",
    "            if new_odor_vec is not None:\n",
    "                vec = new_odor_vec[i] / l2_norm(new_odor_vec[i])\n",
    "                ax.quiver(*orig, *(vec[list(dims)]), color=all_colors[lbl], \n",
    "                          lw=1.5, arrow_length_ratio=0.2)\n",
    "    # No new odors shown\n",
    "    else:\n",
    "        n_new_odors = 0\n",
    "        \n",
    "\n",
    "    # Labeling\n",
    "    for lbl, f in enumerate([ax.set_xlabel, ax.set_ylabel, ax.set_zlabel]):\n",
    "        # z label gets caught in the zlbl variable at the last iteration\n",
    "        zlbl = f(\"OSN {} (of {})\".format(lbl+1, bkser.shape[1]), labelpad=-17.5)\n",
    "    for f in [ax.set_xticks, ax.set_yticks, ax.set_zticks]:\n",
    "        f([])\n",
    "    for f in [ax.set_xticklabels, ax.set_yticklabels, ax.set_zticklabels]:\n",
    "        f([], pad=0.1)\n",
    "    view_params.setdefault(\"azim\", 240)\n",
    "    view_params.setdefault(\"elev\", 3)\n",
    "    ax.view_init(**view_params)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # Move the label for 2+ odors to before the new odors\n",
    "    if n_new_odors > 0:\n",
    "        handles.insert(-n_new_odors-1, handles[0])\n",
    "        labels.insert(-n_new_odors-1, labels[0])\n",
    "    else:\n",
    "        handles.append(handles[0])\n",
    "        labels.append(labels[0])\n",
    "    handles.pop(0)\n",
    "    labels.pop(0)\n",
    "    leg = ax.legend(handles=handles, labels=labels, \n",
    "        frameon=True, ncol=1, loc=\"upper left\", bbox_to_anchor=(0.85, 1.0), \n",
    "        title=\"Odor presence\", title_fontsize=6)\n",
    "    #loc=\"upper right\", bbox_to_anchor=(0.0, 1.0), frameon=False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Need to adjust the tightbox to remove whitespace above and below manually. \n",
    "    #ax.set_aspect(\"equal\")\n",
    "    fig.tight_layout()\n",
    "    tightbox = fig.get_tightbbox()\n",
    "    tightbox._bbox.y0 = tightbox._bbox.y0*1.1   #bottom\n",
    "    tightbox._bbox.y1 = tightbox._bbox.y1 + 0.7*tightbox._bbox.y0  # top\n",
    "    tightbox._bbox.x0 = tightbox._bbox.x0 * 0.6  # position of left side\n",
    "\n",
    "    return fig, ax, tightbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the background process\n",
    "def pairplots_background(bkser, bkvecs, epsser=None, mixed_new_odors=None, new_odor_vec=None):\n",
    "    # Background vectors time series with mixed concentrations\n",
    "    tslice = slice(0, None, 30)\n",
    "    \n",
    "    # Scale odor affinities K_{i \\gamma} by the average saturation threshold \n",
    "    # of the OSN, exp(\\epsilon_i), to get the effective affinity scale, \n",
    "    # before normalizing each odor vector K_\\gamma\n",
    "    vecs = bkvecs / l2_norm(bkvecs, axis=1)[:, None]\n",
    "    if epsser is not None:\n",
    "        mean_eps = np.mean(epsser, axis=0)\n",
    "        vecs_eff = bkvecs / np.exp(mean_eps[None, :])\n",
    "        vecs_eff = vecs_eff / l2_norm(vecs_eff, axis=1)[:, None]\n",
    "    else:\n",
    "        vecs_eff = None\n",
    "        \n",
    "    n_comp = bkvecs.shape[0]\n",
    "    n_cols = 6\n",
    "    n_plots = 48 // 2\n",
    "    n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "    fig.set_size_inches(n_cols*1.0, n_rows*1.0)\n",
    "    single_odor_colors = sns.color_palette(\"colorblind\", n_colors=n_comp)\n",
    "    all_colors = {\"Odor {}\".format(i): single_odor_colors[i] for i in range(n_comp)}\n",
    "    if mixed_new_odors is not None:\n",
    "        n_new_odors = mixed_new_odors.shape[0]\n",
    "        new_odors_palette = sns.dark_palette(\"r\", n_colors=n_new_odors+1)[1:]\n",
    "    for i in range(n_plots):\n",
    "        ax = axes.flat[i]\n",
    "        ax.scatter(bkser[tslice, 2*i+1], bkser[tslice, 2*i], \n",
    "                   s=9, alpha=0.5, color=\"k\")\n",
    "        for j in range(n_comp):\n",
    "            ax.plot(*zip([0.0, 0.0], vecs[j, 2*i:2*i+2][::-1]), lw=1.5, \n",
    "                    color=single_odor_colors[j])\n",
    "            if epsser is not None:\n",
    "                ax.plot(*zip([0.0, 0.0], vecs_eff[j, 2*i:2*i+2][::-1]), lw=1.5, ls=\"--\",\n",
    "                    color=single_odor_colors[j])\n",
    "        if mixed_new_odors is not None:\n",
    "            for j in range(n_new_odors):\n",
    "                clr = new_odors_palette[j]\n",
    "                ax.scatter(mixed_new_odors[j, :, 2*i+1], mixed_new_odors[j, :, 2*i], \n",
    "                       s=6, alpha=1.0, color=clr)\n",
    "        if new_odor_vec is not None:\n",
    "            for j in range(n_new_odors):\n",
    "                vec = new_odor_vec[j] / l2_norm(new_odor_vec[j])\n",
    "                ax.plot(*zip([0.0, 0.0], vec[2*i:2*i+2][::-1]), lw=2.0, \n",
    "                    color=new_odors_palette[j])\n",
    "        ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        axes.flat[i].set_axis_off()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_seed = 0x7170b82d905839ddda1def555e3a43508\n",
    "simul_seed = 0x52e7bfc4e1f58395730de6afff855abc\n",
    "\n",
    "# IBCM\n",
    "back_ibcm, res_ibcm = run_ibcm_simulation_adapt(adaptation_params, n_components,  \n",
    "                         n_dimensions, main_seed, simul_seed)\n",
    "res_ibcm_clean = analyze_clean_ibcm_simul(res_ibcm, back_ibcm, main_seed)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca, res_biopca = run_biopca_simulation_adapt(adaptation_params, n_components,\n",
    "                            n_dimensions, main_seed, simul_seed)\n",
    "res_biopca_clean = analyze_clean_biopca_simul(res_biopca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm, res_ibcm_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca, res_biopca_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axes zoom effect from Matplotlib documentation\n",
    "from matplotlib.transforms import (Bbox, TransformedBbox,\n",
    "                                   blended_transform_factory)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (BboxConnector,\n",
    "                                                   BboxConnectorPatch,\n",
    "                                                   BboxPatch)\n",
    "def connect_bbox(bbox1, bbox2,\n",
    "                 loc1a, loc2a, loc1b, loc2b,\n",
    "                 prop_lines, prop_patches=None):\n",
    "    if prop_patches is None:\n",
    "        prop_patches = {\n",
    "            **prop_lines,\n",
    "            \"alpha\": prop_lines.get(\"alpha\", 1) * 0.2,\n",
    "            \"clip_on\": False,\n",
    "        }\n",
    "\n",
    "    c1 = BboxConnector(\n",
    "        bbox1, bbox2, loc1=loc1a, loc2=loc2a, clip_on=False, **prop_lines)\n",
    "    c2 = BboxConnector(\n",
    "        bbox1, bbox2, loc1=loc1b, loc2=loc2b, clip_on=False, **prop_lines)\n",
    "\n",
    "    bbox_patch1 = BboxPatch(bbox1, **prop_patches, color=\"grey\")\n",
    "    bbox_patch2 = BboxPatch(bbox2, **prop_patches, color=\"grey\")\n",
    "\n",
    "    p = BboxConnectorPatch(bbox1, bbox2,\n",
    "                           loc1a=loc1a, loc2a=loc2a, loc1b=loc1b, loc2b=loc2b,\n",
    "                           clip_on=False,\n",
    "                           **prop_patches)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p\n",
    "\n",
    "def zoom_effect01(ax1, ax2, xmin, xmax, **kwargs):\n",
    "    \"\"\"\n",
    "    Connect *ax1* and *ax2*. The *xmin*-to-*xmax* range in both Axes will\n",
    "    be marked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax1\n",
    "        The main Axes.\n",
    "    ax2\n",
    "        The zoomed Axes.\n",
    "    xmin, xmax\n",
    "        The limits of the colored area in both plot Axes.\n",
    "    **kwargs\n",
    "        Arguments passed to the patch constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    bbox = Bbox.from_extents(xmin, 0, xmax, 1)\n",
    "\n",
    "    mybbox1 = TransformedBbox(bbox, ax1.get_xaxis_transform())\n",
    "    mybbox2 = TransformedBbox(bbox, ax2.get_xaxis_transform())\n",
    "\n",
    "    prop_patches = {**kwargs, \"ec\": \"none\", \"alpha\": 0.2}\n",
    "\n",
    "    c1, c2, bbox_patch1, bbox_patch2, p = connect_bbox(\n",
    "        mybbox1, mybbox2,\n",
    "        loc1a=3, loc2a=2, loc1b=4, loc2b=1,\n",
    "        prop_lines=kwargs, prop_patches=prop_patches)\n",
    "\n",
    "    #ax1.add_patch(bbox_patch1)\n",
    "    ax2.add_patch(bbox_patch2)\n",
    "    ax2.add_patch(c1)\n",
    "    ax2.add_patch(c2)\n",
    "    ax2.add_patch(p)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what individual OSN epsilons are doing on fast and long time scales\n",
    "def plot_eps_series(eps_ser, n_shown, tzoom_interv, skp_n=1, smoothsize=201):\n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    axes = [fig.add_subplot(gs[0, :3])]\n",
    "    axes.append(fig.add_subplot(gs[1, :3], sharey=axes[0]))\n",
    "    axleg = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])\n",
    "    eps_ser_ibcm = res_ibcm[3]\n",
    "    eps_ser_ibcm_smooth = moving_average(eps_ser_ibcm, kernelsize=smoothsize, boundary=\"free\")\n",
    "    tscale = deltat * 10.0 / 1000.0 / 60.0  # minutes\n",
    "    #eps_ser_biopca = res_biopca[3]\n",
    "\n",
    "    osn_colors = sns.cubehelix_palette(n_colors=n_shown, \n",
    "            start=0.0, rot=1.0, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=True)\n",
    "\n",
    "    tsl_local = slice(*tzoom_interv, 1)  # limit * skp * 10 = milliseconds, skp=50 default\n",
    "    tsl_global = slice(0, None, 20)\n",
    "    n_labels = 6\n",
    "    skp_lbl = (n_shown // skp_n) // n_labels\n",
    "    for i in range(0, n_shown, skp_n):\n",
    "        lbl = \"{}\".format(i) if (i // skp_n) % skp_lbl == 0 else \"\"\n",
    "        axes[0].plot(tser_common[tsl_local]*tscale, eps_ser_ibcm[tsl_local, i], \n",
    "                     alpha=0.7, lw=0.75, color=osn_colors[i], label=lbl)\n",
    "        axes[1].plot(tser_common[tsl_global]*tscale, eps_ser_ibcm_smooth[tsl_global, i], \n",
    "                     alpha=0.7, lw=0.75, color=osn_colors[i])\n",
    "\n",
    "    axes[0].set(ylabel=r\"$\\epsilon_i(t)$\")\n",
    "    axes[1].set(xlabel=\"Time (min)\", ylabel=r\"Smoothed $\\epsilon_i(t)$\")\n",
    "    axleg.legend(*axes[0].get_legend_handles_labels(), frameon=False, title=\"OSN\")\n",
    "    axleg.set_axis_off()\n",
    "    t1, t2 = tser_common[tzoom_interv[0]]*tscale, tser_common[tzoom_interv[1]]*tscale\n",
    "    zoom_effect01(axes[0], axes[1], t1, t2, lw=0.8)\n",
    "    fig.tight_layout(h_pad=-0.1)\n",
    "    return fig, axes, axleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, axleg = plot_eps_series(eps_ser_ibcm, 50, (600, 840), skp_n=4, smoothsize=101)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, moments_conc, \n",
    "#                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "#                     conc_ser, mixed_samples, new_odors)\n",
    "bkser_ibcm = res_ibcm_clean[2]\n",
    "bkvecs_ibcm = res_ibcm_clean[9]\n",
    "concser_ibcm = res_ibcm_clean[10]\n",
    "fig, ax, box = plot_manifold(bkser_ibcm, bkvecs_ibcm, concser_ibcm, {\"azim\":220, \"elev\":30}, dims=(0, 1, 2))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pairplots_background(bkser_ibcm, bkvecs_ibcm, eps_ser_ibcm)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
