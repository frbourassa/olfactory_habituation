{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations with OSN adaptation\n",
    "\n",
    "Same model of OSN saturation as in ``nonlinear_osn_turbulent_illustration.ipynb``, \n",
    "\n",
    "$$ s_i(t) = F_\\mathrm{max} \\frac{\\sum_\\gamma K_{i \\gamma} c_\\gamma}{\\exp{(\\epsilon_i(t))} + \\sum_\\gamma K_{i \\gamma} c_\\gamma} $$\n",
    "\n",
    "but with modified IBCM and BioPCA integration functions that promote $\\epsilon_i(t)$ to dynamical variables with feedback from OSN activity $s_i(t)$ and a target amplitude\n",
    "\n",
    "$$ \\frac{\\mathrm{d} \\epsilon_i(t)}{\\mathrm{d} t} = \\frac{1}{\\tau_\\mathrm{a}} \\left( s_i(t) - s_{i, 0} \\right) $$\n",
    "\n",
    "where we also clip (i.e. stop updating beyond this range) $\\epsilon \\in [\\epsilon_L, \\epsilon_H]$ to prevent divergences arising from a continued excess or deficit of OSN activity. In other words, adaptation only occurs on a finite range.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    ibcm_respond_new_odors,  # Unchanged if we give inputs nonlinearized with correct epsilon\n",
    "    compute_mbars_cgammas_cbargammas,  # Unchanged\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    build_lambda_matrix,  \n",
    "    biopca_respond_new_odors  # Unchanged if we give nonlinearized inputs with correct epsilon\n",
    ")\n",
    "# Do not consider average or idealized subtraction here\n",
    "\n",
    "from modelfcts.checktools import (\n",
    "    check_conc_samples_powerlaw_exp1,\n",
    "    analyze_pca_learning  # Unchanged if give pre-computed nonlinear inputs\n",
    ")\n",
    "from utils.metrics import jaccard, l2_norm\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_inverse_transform, \n",
    "    truncexp1_density, \n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform,\n",
    "    inverse_transform_tanhcdf\n",
    ")\n",
    "# re-use functions for nonlinear OSNs, will need to put \n",
    "# updated epsilon in back_params at each step\n",
    "from modelfcts.nonlin_adapt_osn import (  \n",
    "    generate_odor_tanhcdf, \n",
    "    combine_odors_affinities, \n",
    "    update_powerlaw_times_concs_affinities,\n",
    "    sample_background_powerlaw_nl_osn\n",
    ")\n",
    "from modelfcts.backgrounds import (  #\n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw,   # unchanged\n",
    "    update_tc_odor  # unchanged\n",
    ")\n",
    "from modelfcts.tagging import (  # unchanged\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "\n",
    "\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"nonlin_adapt\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"nonlin_adapt\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)\n",
    "    \n",
    "models = list(model_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\",  \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\",\n",
    "    \"optimal\": \"Optimal\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\",\n",
    "    \"optimal\": \"xkcd:powder blue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main new simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_ibcm_adaptation(vari_inits, update_bk, bk_init,\n",
    "    ibcm_params, inhib_params, bk_params, adapt_params, tmax, dt,\n",
    "    seed=None, noisetype=\"normal\", skp=1, **options):\n",
    "    r\"\"\" See docs of integrate_inhib_ibcm_network_options. Differences:\n",
    "    \n",
    "    Args:\n",
    "        vari_inits, update_bk, bk_init, ibcm_params, inhib_params, bk_params, \n",
    "        adapt_params, tmax, dt, seed=None, noisetype=\"normal\", skp=1, **options\n",
    "        \n",
    "        adapt_params (list of 3 floats, 1 vector): epsilon adaptation time scale, \n",
    "            lower and upper limits on epsilon, target osn activities.  \n",
    "            \n",
    "        Moreover, we assume that bk_params[-2] is the vector of epsilons\n",
    "    \n",
    "    Returns:\n",
    "        [tseries, bk_series, bkvec_series, eps_series, m_series,\n",
    "        cbar_series, theta_series, w_series, y_series]\n",
    "        \n",
    "        eps_series: shaped [n_times, n_s], the valud of each OSN type's\n",
    "            epsilon at each time point. \n",
    "    \"\"\"\n",
    "    # Get some of the keyword arguments\n",
    "    saturation = options.get(\"saturation\", \"linear\")\n",
    "    variant = options.get(\"variant\", \"intrator\")\n",
    "    activ_fct = str(options.get(\"activ_fct\", \"ReLU\")).lower()\n",
    "    decay = options.get(\"decay\", False)\n",
    "    w_norms = options.get(\"w_norms\", (2, 2))\n",
    "\n",
    "    # Legacy option to just pass initial M\n",
    "    if isinstance(vari_inits, np.ndarray):\n",
    "        m_init = vari_inits\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "        w_init = np.zeros([n_orn, n_neu])\n",
    "        theta_init = None\n",
    "    elif isinstance(vari_inits, list) and len(vari_inits) == 1:\n",
    "        m_init = np.asarray(vari_inits[0])\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "        w_init = np.zeros([n_orn, n_neu])\n",
    "        theta_init = None\n",
    "    else:\n",
    "        m_init, theta_init, w_init = vari_inits\n",
    "        n_neu = m_init.shape[0]  # Number of neurons\n",
    "        n_orn = m_init.shape[1]\n",
    "\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_orn == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    alpha, beta = inhib_params\n",
    "    learnrate, tavg, coupling, lambd, sat, ktheta, decay_relative = ibcm_params\n",
    "    # Compensate for lambda different from 1, if applicable\n",
    "    mu_abs = learnrate / lambd\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt*skp)\n",
    "\n",
    "    # Check that the biggest matrices, W or M, will not use too much memory\n",
    "    if tseries.shape[0] * n_orn * n_neu > 5e8 / 8:  # 500 MB per series max\n",
    "        raise ValueError(\"Excessive memory use by saved series; increase skp\")\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_orn])\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])\n",
    "    w_series = np.zeros([tseries.shape[0], n_orn, n_neu])  # Inhibitory weights\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_orn])  # Input vecs, convenient to compute inhibited output\n",
    "    y_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    theta_series = np.zeros([tseries.shape[0], n_neu])\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    m = m_init.copy()\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    c = m.dot(bkvec)  # un-inhibited neuron activities\n",
    "    # Initialize neuron activity with m and background at time zero\n",
    "    cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "    if saturation == \"tanh\":\n",
    "        sat_abs = sat * lambd\n",
    "        cbar = sat_abs * np.tanh(cbar / sat_abs)\n",
    "    else: sat_abs = None\n",
    "    if theta_init is None:\n",
    "        # Important to initialize cbar2_avg to non-zero values, because we divide by this!\n",
    "        cbar2_avg = np.maximum(cbar*cbar / lambd, learnrate*lambd)\n",
    "    else:\n",
    "        cbar2_avg = theta_init.copy()\n",
    "    wmat = w_init.copy()\n",
    "    yvec = bk_vec_init - wmat.dot(cbar)\n",
    "    if activ_fct == \"relu\":\n",
    "        relu_inplace(yvec)\n",
    "    elif activ_fct == \"identity\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Unknown activation fct: {}\".format(activ_fct))\n",
    "        \n",
    "    # New parameters and initialization for nonlinear OSN model, epsilon\n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    eps_series = np.zeros([tseries.shape[0], n_orn])\n",
    "    # epsilon gets initialized to midpoint between min and max\n",
    "    epsvec = np.full(n_orn, 0.5*(eps_min + eps_max))\n",
    "    assert bk_params[-2].shape == epsvec.shape, \"Ensure vector of epsilons is in bk_params[-2]\"\n",
    "    bk_params[-2] = epsvec\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    cbar_series[0] = cbar\n",
    "    bk_series[0] = bk_vari\n",
    "    m_series[0] = m_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    y_series[0] = yvec\n",
    "    theta_series[0] = cbar2_avg\n",
    "    w_series[0] = wmat\n",
    "    eps_series[0] = epsvec\n",
    "\n",
    "    # Generate noise samples in advance, by chunks of at most 2e7 samples\n",
    "    if noisetype == \"normal\":\n",
    "        sample_fct = rng.standard_normal\n",
    "    elif noisetype == \"uniform\":\n",
    "        sample_fct = rng.random\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    max_chunk_size = int(2e7)\n",
    "    # step multiple at which we run out of noises and need to replenish\n",
    "    kchunk = max_chunk_size // bk_vari.size\n",
    "    max_n_steps = len(tseries)*skp-1  # vs total number of steps\n",
    "\n",
    "    t = 0\n",
    "    newax = np.newaxis\n",
    "    for k in range(0, max_n_steps):\n",
    "        t += dt\n",
    "        # Replenish noise samples if necessary\n",
    "        if k % kchunk == 0:\n",
    "            steps_left = max_n_steps - k\n",
    "            noises = sample_fct(size=(min(kchunk, steps_left), *bk_vari.shape))\n",
    "        \n",
    "        ### Inhibitory  weights\n",
    "        # They depend on cbar and yvec at time step k, which are still in cbar, yvec\n",
    "        # cbar, shape [n_neu], should broadcast against columns of wmat,\n",
    "        # while yvec, shape [n_orn], should broadcast across rows (copied on each column)\n",
    "        if w_norms[0] == 2:  # default L2 norm, nice and smooth\n",
    "            alpha_term = alpha * cbar[newax, :] * yvec[:, newax]\n",
    "        elif w_norms[0] == 1:  # L1 norm\n",
    "            aynorm = alpha * l1_norm(yvec)\n",
    "            alpha_term = aynorm * cbar[newax, :] * np.sign(yvec[:, newax])\n",
    "        elif w_norms[0] > 2:  # Assuming some Lp norm with p > 2\n",
    "            # Avoid division by zero for p > 2 by clipping ynorm\n",
    "            ynorm = max(1e-9, lp_norm(yvec, p=w_norms[0]))\n",
    "            yterm = np.sign(yvec) * np.abs(yvec/ynorm)**(w_norms[0]-1) * ynorm\n",
    "            alpha_term = alpha * cbar[newax, :] * yterm[:, newax]\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        if w_norms[1] == 2:\n",
    "            beta_term = beta * wmat\n",
    "        elif w_norms[1] == 1:\n",
    "            beta_term = beta * l1_norm(wmat.ravel()) * np.sign(wmat)\n",
    "        elif w_norms[1] > 2:\n",
    "            wnorm = max(1e-9, lp_norm(wmat.ravel(), p=w_norms[1]))\n",
    "            wterm = np.sign(wmat) * np.abs(wmat/wnorm)**(w_norms[1]-1)\n",
    "            beta_term = beta * wterm * wnorm\n",
    "        else:\n",
    "            raise ValueError(\"Cannot deal with Lp norms with p < 0 or non-int\")\n",
    "\n",
    "        wmat += dt * (alpha_term - beta_term)\n",
    "\n",
    "        ### IBCM neurons\n",
    "        # Phi function for each neuron.\n",
    "        if variant == \"intrator\":\n",
    "            phiterms_vec = cbar * (cbar - cbar2_avg)\n",
    "        #  Law and Cooper modification for faster convergence.\n",
    "        elif variant == \"law\":\n",
    "            phiterms_vec = cbar * (cbar - cbar2_avg) / (ktheta + cbar2_avg/lambd)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown variant: {}\".format(variant))\n",
    "\n",
    "        if saturation == \"tanh\":\n",
    "            phiterms_vec *=  1.0 - (cbar/sat_abs)**2\n",
    "        # Now, careful with broadcast: for each neuron (dimension 0 of m and cbar), we need a scalar element\n",
    "        # of phiterms_vec times the whole bkvec, for dimension 1 of m.\n",
    "        # This can be done vectorially with a dot product (n_neu, 1)x(1, n_components)\n",
    "        rhs_scalar = phiterms_vec - coupling*(np.sum(phiterms_vec) - phiterms_vec)\n",
    "        # Euler integrator and learning rate\n",
    "        # learnrate_t = learnrate if t < 150000 else learnrate / 5\n",
    "        # Reducing learning rate after a while may help.\n",
    "        # Consider feedback on mu through some metric of how well neurons\n",
    "        # are inhibiting the background, e.g. s average activity.\n",
    "        m += mu_abs*dt*rhs_scalar[:, np.newaxis].dot(bkvec[np.newaxis, :])\n",
    "        # In principle, should add low decay to background subspace\n",
    "        # To make sure 1) only learn the background space, 2) de-habituate after\n",
    "        # The decay term is proportional to m, not m^2 like the IBCM term\n",
    "        # so we needed to divide learnrate by Lambda for the IBCM term\n",
    "        # but not for this linear decay term, which should use just learnrate\n",
    "        if decay and variant == \"law\":\n",
    "            m -= dt * decay_relative * learnrate / (ktheta + cbar2_avg[:, np.newaxis]/lambd) * m\n",
    "        elif decay and variant == \"intrator\":\n",
    "            m -= dt * decay_relative * learnrate * m\n",
    "        # Now, update to time k+1 the threshold (cbar2_avg) using cbar at time k\n",
    "        # to be used to update m in the next time step\n",
    "        cbar2_avg += dt * (cbar*cbar / lambd - cbar2_avg)/tavg\n",
    "        \n",
    "        # Adapt OSNs in response to background at time t\n",
    "        epsvec += dt / tau_eps * (bkvec - osn_targets)\n",
    "        epsvec = np.clip(epsvec, a_min=eps_min, a_max=eps_max)\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k % kchunk], dt)\n",
    "        \n",
    "        # Store updated epsilon in bk_params for the next background update\n",
    "        bk_params[-2] = epsvec\n",
    "\n",
    "        # Then, compute activity of IBCM neurons at next time step, k+1,\n",
    "        # with the updated background and synaptic weight vector m\n",
    "        # Compute un-inhibited activity of each neuron with current input (at time k)\n",
    "        # With many simulations in parallel, there seemed to be a bottleneck here\n",
    "        # and also at yvec calculation: turns out it's because of BLAS multithreading\n",
    "        # So for multiprocessing, this function should be launched in a threadpool_limits\n",
    "        c = m.dot(bkvec)\n",
    "        cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "        if saturation == \"tanh\":\n",
    "            cbar = sat_abs * np.tanh(cbar / sat_abs)\n",
    "        # np.sum(c) is a scalar and c a vector, so it broadcasts properly.\n",
    "\n",
    "        # Lastly, projection neurons at time step k+1\n",
    "        yvec = bkvec - wmat.dot(cbar)\n",
    "        if activ_fct == \"relu\":\n",
    "            relu_inplace(yvec)\n",
    "\n",
    "        # Save current state only if at a multiple of skp\n",
    "        if (k % skp) == (skp - 1):\n",
    "            knext = (k+1) // skp\n",
    "            w_series[knext] = wmat\n",
    "            m_series[knext] = m\n",
    "            bk_series[knext] = bk_vari\n",
    "            bkvec_series[knext] = bkvec\n",
    "            cbar_series[knext] = cbar  # Save activity of neurons at time k+1\n",
    "            y_series[knext] = yvec\n",
    "            theta_series[knext] = cbar2_avg\n",
    "            eps_series[knext] = epsvec\n",
    "\n",
    "    return [tseries, bk_series, bkvec_series, eps_series, m_series,\n",
    "            cbar_series, theta_series, w_series, y_series]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly number\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "# Common parameters for toy and full simulations\n",
    "inhib_rates = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Simulation skipping, 50 is enough for plots\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "combine_fct = combine_odors_affinities\n",
    "update_fct = update_powerlaw_times_concs_affinities\n",
    "\n",
    "# Scale of affinity vectors: default\n",
    "kscale = 5e-4  # default is 5e-4\n",
    "\n",
    "# OSN target activity and epsilon ranges\n",
    "# TODO; maybe adjust given the odor vectors, f_max scale, etc. \n",
    "target_osn_activ = np.full(n_dimensions, 1.0 / np.sqrt(n_dimensions))\n",
    "adaptation_params = [\n",
    "    25.0,  # tau_adapt = 250 ms\n",
    "    3.0,  # eps_min\n",
    "    10.0,  # eps_max\n",
    "    target_osn_activ \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_back_params(adapt_params, rgen, n_comp, n_dim):\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms = default_background_params(n_comp)\n",
    "    \n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    epsils_vec = np.full(n_dim, 0.5 * (eps_min + eps_max))\n",
    "    back_comps = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Same adjustment of the OSN amplitude as in the performance recognition tests\n",
    "    raw_conc_factor = 2.5\n",
    "    raw_ampli = 2.5\n",
    "    np_statistic = np.mean  # np.mean, np.median, np.amax\n",
    "\n",
    "    raw_osn_activ = np_statistic(combine_fct(np.full(n_comp, raw_conc_factor * avg_whiff_conc), \n",
    "                                        back_comps, epsils_vec, fmax=1.0))\n",
    "    max_osn_ampli = raw_ampli / (raw_osn_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(max_osn_ampli)\n",
    "    back_pms.append(epsils_vec)\n",
    "    back_pms.append(back_comps)\n",
    "\n",
    "    # Initialization\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps, epsils_vec, fmax=max_osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_pms, init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters, same for each tested epsilon\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00075  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.7/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global IBCM parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_adapt(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing IBCM simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "    \n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen.standard_normal(size=[n_i_ibcm, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_ibcm_adaptation(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params_local, \n",
    "                adapt_params, duration, deltat, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_odors_in_manifold(back_pms, conc_ser, new_conc_rel, rgen, n_ex=2, n_samp=10):\n",
    "    \"\"\"Mix n_ex new odors with n_samp background samples each. \n",
    "    Returns a 3d-array of mixtures, indexed [n_ex, n_samp, n_dim], \n",
    "    and the new odor vectors, a 2d array indexed [n_ex, n_dim]. \n",
    "    \"\"\"\n",
    "    back_odors = back_pms[-1]\n",
    "    n_comp, n_dim = back_odors.shape[0], back_odors.shape[1]\n",
    "    max_ampli = back_pms[-3]\n",
    "    new_odors = generate_odor_tanhcdf((n_ex, n_dim), rgen, unit_scale=kscale)\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_conc = avg_whiff_conc * new_conc_rel\n",
    "    non_null_concs = conc_ser[np.any(conc_ser > 0.0, axis=1)]\n",
    "    epsils_vec = back_pms[-2]\n",
    "    back_concs = non_null_concs[rgen.choice(non_null_concs.shape[0], size=n_ex*n_samp, replace=True)]\n",
    "    back_concs = back_concs.reshape(n_ex, n_samp, n_comp)\n",
    "    all_mixed_samples = []\n",
    "    for i in range(n_ex):\n",
    "        joint_kmats = np.concatenate([back_odors, new_odors[i:i+1]], axis=0)\n",
    "        mixed_samples_i = []\n",
    "        for j in range(n_samp):\n",
    "            joint_concs = np.concatenate([back_concs[i, j:j+1], np.full((1, 1), new_conc)], axis=1)\n",
    "            mixed_samples_i.append(combine_fct(joint_concs, joint_kmats, epsils_vec, fmax=max_ampli))\n",
    "        mixed_samples_i = np.concatenate(mixed_samples_i, axis=0)\n",
    "        all_mixed_samples.append(mixed_samples_i)\n",
    "        \n",
    "    return np.stack(all_mixed_samples, axis=0), new_odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def analyze_clean_ibcm_simul(results_raw, back_pms, rgenseed, n_ex=2, n_samp=10, t_mix=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "    Returns:\n",
    "        cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, eps_ser, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    back_components = back_pms[-1]\n",
    "    basis = back_components / l2_norm(back_components, axis=1)[:, None] \n",
    "    \n",
    "\n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                mser_ibcm, coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = nuser_ibcm[:, :, 1]\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    ysernorm_ibcm = l2_norm(yser_ibcm, axis=1)\n",
    "    \n",
    "    # Examples of mixing new odors with the background\n",
    "    rgen = np.random.default_rng(np.random.SeedSequence(rgenseed).spawn(2)[1])\n",
    "    back_pms_local = list(back_pms)\n",
    "    back_pms_local[-2] = eps_ser[t_mix]\n",
    "    mixres = mix_new_odors_in_manifold(back_pms_local, conc_ser, 1.0, rgen, n_ex=n_ex, n_samp=n_samp)\n",
    "    mixed_samples, new_odors = mixres\n",
    "    results_clean = (cbars_gamma, wser_ibcm, bkvecser_ibcm, ysernorm_ibcm, moments_conc, \n",
    "                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "                     conc_ser, mixed_samples, new_odors)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, res_ibcm_clean):\n",
    "    (cbars_gamma, wser_ibcm, bkvecser_ibcm, ysernorm_ibcm, \n",
    "         moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc, \n",
    "         back_comps, conc_ser, _, _) = res_ibcm_clean\n",
    "\n",
    "    # Plot of cbars gamma series\n",
    "    fig , ax, _ = plot_cbars_gamma_series(tser_common, cbars_gamma, \n",
    "                            skp=2, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plots of neuron specificities\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(correl_c_conc.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    print(\"Odor specificities:\", specif_gammas)\n",
    "    split_val = 2.5\n",
    "    for comp in range(n_components):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_ibcm_raw[2], res_ibcm_raw[7], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global BioPCA parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def run_biopca_simulation_epsil(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing BioPCA simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background parameters, give same rgenseed as IBCM to have same background\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "        \n",
    "    init_synapses_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_ibcm_adaptation(\n",
    "                ml_inits_pca, update_fct, init_back, biopca_rates, \n",
    "                inhib_rates, back_params_local, duration, deltat, \n",
    "                seed=simseed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_seed = 0x7170b82d905839fffa1def666e3a7343\n",
    "simul_seed = 0x52e7bfc4e1f58395730de6afff855abc\n",
    "\n",
    "# IBCM\n",
    "back_ibcm, res_ibcm = run_ibcm_simulation_adapt(adaptation_params, n_components,  \n",
    "                         n_dimensions, main_seed, simul_seed)\n",
    "res_ibcm_clean = analyze_clean_ibcm_simul(res_ibcm, back_ibcm, main_seed)\n",
    "\n",
    "# BioPCA\n",
    "#back_biopca_low, res_biopca_low = run_biopca_simulation_epsil(epsil_low, n_components,\n",
    "#                            n_dimensions, seed_low, simul_seed_low)\n",
    "#res_biopca_clean_low = analyze_clean_biopca_simul(res_biopca_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm, res_ibcm_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
