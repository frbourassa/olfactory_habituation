{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbulent backgrounds with nonlinear OSN model\n",
    "\n",
    "## Full model\n",
    "We use a simplified version of the model from Kadakia and Emonet, because OSN adaptation is easy to include into it afterwards. \n",
    "\n",
    "The olfactory receptors (OR) have Orco co-receptors. Each OR-Orco complex has an active and an inactive state, with affinities $K^*_{i \\mu}$, $K_{i \\mu}$ respectively for odor $\\mu$, receptor index $i$. Given odor concentrations $c_\\mu$, the quasi-static OSN firing rate in response is\n",
    "\n",
    "$$ A_i = F_\\mathrm{max} \\left[1 + e^{\\epsilon_i(t)} \\left( \\frac{1 + \\sum_\\mu K_{i \\mu} c_\\mu }{1 + \\sum_\\mu K^*_{i \\mu} c_\\mu }  \\right)  \\right]^{-1} $$\n",
    "\n",
    "where $\\epsilon_i(t)$ is the free energy difference between the unbound states in the inactive and active conformations for OR type $i$, in units of $k_\\mathrm{B} T$. This free energy difference changes with feedback from OSN activity, with an adaptation time scale of $250$ ms, but here, we will keep $\\epsilon_i$ fixed; the point of the model is to have a nonlinear receptor activation with easy inclusion of adaptation later. \n",
    "\n",
    "## Simplified form\n",
    "\n",
    "The full activation function can be simplified in the regime where $K c \\ll 1$, which is used by Kadakia and Emonet (in scaled units, they set $K = 0.01$ compared to $K^*$ in the range $10^3-10^4$). Assuming also that $K^* c \\gg 1$, we can neglect the $+1$ and simplify to\n",
    "\n",
    "$$ A_i = F_\\mathrm{max}  \\frac{ \\sum_\\mu K^*_{i \\mu} c_\\mu}{e^{\\epsilon_i} + \\sum_\\mu K^*_{i \\mu} c_\\mu} $$\n",
    "\n",
    "which amounts to a Michaelis-Menten function of the linear combination of odor activations, $\\sum_\\mu K^*_{i \\mu} c_\\mu$, with Michaelis-Menten constant $e^{\\epsilon_i}$ controlling the level of OSN saturation and thus of nonlinearity. Then, odors defined by a single vector of active complex affinities, $\\mathbf{K}^*_\\mu$. \n",
    "\n",
    "\n",
    "## Implementation and affinities distribution\n",
    "\n",
    "The affinities $K^*_{i \\mu}$ and $K_{i \\mu}$ are sampled i.i.d. from a distribution of OSN affinities (inverse EC50s) measured experimentally by Si et al., *Neuron*, 2019. We fit this empirical distribution of affinities with a complementary cumulative distribution function of the form\n",
    "\n",
    "$$ G_X(x) = \\mathbb{P}[X > x] = \\tanh\\left(\\frac{1}{b x^{\\alpha}} \\right) $$\n",
    "\n",
    "which has a power-law tail $G_X \\sim x^{-\\alpha}$ for large $x$, but also a cutoff at low $x$ very similar to the experimental data -- much better, at any rate, than a pure power law. See the fits in the notebook ``si2019_hill_tanh_distribution_fits.ipynb``. \n",
    "\n",
    "Looking at the code from Kadakia and Emonet, \n",
    "https://github.com/elifesciences-publications/ORN-WL-gain-control/blob/master/src/four_state_receptor_CS.py ,\n",
    "the $K$ parameters are all equal to $1/10^2 = 0.01$ exactly. We do the same, since this ensures each odor has its $K^*$ larger than $K$. Note this puts the model in the large $K^*$, small $K$ regime highlighted above, so $K$ could be neglected entirely if we wanted. \n",
    "\n",
    "The free energy default value is $5.0$ in Kadakia and Emonet's code, without variance between odors. \n",
    "\n",
    "## Purpose of this notebook\n",
    "We generate illustrations of the nonlinear manifold and simulation examples of habituation at different levels of nonlinearity. The full simulations to assess performance vs nonlinearity are in the script ``secondary_scripts/run_performance_nl_osn.py``. We save lines and data for final plotting of the supplementary figure, including:\n",
    " - Examples of the simplified OSN activation curves, for different $K^*$ and $\\epsilon_i$. \n",
    " - A 2D background manifold plotted in 3D, \n",
    " - The Si et al., 2019 CCDF and the best parameters for the tanh fit are saved in ``si2019_hill_tanh_distribution_fits.ipynb``\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    compute_optimal_matrices,\n",
    "    compute_optimal_matrix_fromsamples\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    sample_ss_conc_powerlaw,\n",
    "    update_tc_odor, \n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform, \n",
    "    power_range_inverse_transform\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm, rms_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main new functions: background update with OSN model\n",
    "from modelfcts.distribs import inverse_transform_tanhcdf\n",
    "from modelfcts.nonlin_adapt_osn import (\n",
    "    generate_odor_tanhcdf, \n",
    "    combine_odors_affinities, \n",
    "    update_powerlaw_times_concs_affinities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"nonlin_adapt\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"nonlin_adapt\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)\n",
    "    \n",
    "models = list(model_colors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\",  \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\",\n",
    "    \"optimal\": \"Optimal\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\",\n",
    "    \"optimal\": \"xkcd:powder blue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters common to all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for toy and full simulations\n",
    "inhib_rates = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "combine_fct = combine_odors_affinities\n",
    "update_fct = update_powerlaw_times_concs_affinities\n",
    "\n",
    "# Scale of affinity vectors: default\n",
    "kscale = 5e-4  # default is 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_initialization_toy(n_dim, n_comp, rgen, epsil=4.5):\n",
    "    \"\"\" Common template for background initialization \"\"\"\n",
    "    # Seed for background simulation, to make sure all models are the same\n",
    "    simseed = seed_from_gen(rgen)\n",
    "    back_pms = default_background_params(n_comp)\n",
    "\n",
    "    # Background odors: epsils controls the nonlinearity strength, will be adjusted\n",
    "    eps_vec = np.full(n_dim, epsil)\n",
    "    back_comp = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "\n",
    "    raw_conc = 1.5\n",
    "    raw_ampli = 5.0\n",
    "    np_stat = np.amax  # np.mean, np.median, np.amax\n",
    "    raw_activ = np_stat(combine_fct(np.full(n_comp, raw_conc * avg_whiff), \n",
    "                                        back_comp, eps_vec, fmax=1.0))\n",
    "    osn_ampli = raw_ampli / (raw_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(osn_ampli)\n",
    "    back_pms.append(eps_vec)\n",
    "    back_pms.append(back_comp)\n",
    "\n",
    "    # In the small conc. approx, the odor vectors are (K^* - K)/2\n",
    "    s_vecs = back_comp / l2_norm(back_comp, axis=1)[:, None]\n",
    "\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comp, eps_vec, fmax=osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_list = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_comp, back_pms, init_list, s_vecs, simseed, osn_ampli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal linear manifold learning matrix $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_back(back_odors, new_odors, cser, newconc, fmax, epsils=5.0):\n",
    "    n_new = new_odors.shape[0]\n",
    "    assert n_new == cser.shape[0]  # one new odor per back sample\n",
    "    all_mixvecs = []\n",
    "    for n in range(n_new):\n",
    "        joint_concs = np.concatenate([cser[n], np.full(1, newconc)])\n",
    "        joint_components = np.concatenate(\n",
    "            [back_odors, new_odors[n:n+1]], axis=0)\n",
    "        mixvecs = combine_fct(joint_concs, \n",
    "                    joint_components, epsils, fmax=fmax)\n",
    "        all_mixvecs.append(mixvecs)\n",
    "    mixvecs = np.stack(all_mixvecs, axis=0)\n",
    "    return mixvecs\n",
    "\n",
    "def get_optimal_mat_p(bkvecser, concser, back_pms, new_concs_rel, \n",
    "                      sd=0xe20d4b26b1c7e9b943cd23f4c9d15dca):\n",
    "    \"\"\" Compute the optimal linear manifold learning matrix P, \n",
    "    using a previously simulated background\"\"\"\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_concs = avg_whiff_conc * new_concs_rel\n",
    "    n_new_concs = len(new_concs)\n",
    "    osn_ampli = back_pms[-3]\n",
    "    eps_vec = back_pms[-2]\n",
    "    back_comp = back_pms[-1]\n",
    "\n",
    "    # Compute optimal W matrix for all new odors possible\n",
    "    # Need samples from the background (use provided bkser)\n",
    "    # and samples from mixtures of background + new odor\n",
    "    # (generate from back. conc. series in nuser_ibcm)\n",
    "    dummy_rgen = np.random.default_rng(sd)\n",
    "    # New odors, each with a subset of the background samples\n",
    "    n_samp, n_dims = bkvecser.shape[0], bkvecser.shape[1]\n",
    "    new_odors_from_distrib = generate_odor_tanhcdf(\n",
    "        [n_samp, n_dims], dummy_rgen, unit_scale=kscale)\n",
    "\n",
    "    optimal_matrices = []\n",
    "    for newconc in new_concs:\n",
    "        # Mix new odors at newconc with background\n",
    "        s_new_mix = mix_new_back(back_comp, new_odors_from_distrib, \n",
    "                                 concser, newconc, osn_ampli, epsils=eps_vec)\n",
    "        mat = compute_optimal_matrix_fromsamples(bkvecser, s_new_mix)\n",
    "        optimal_matrices.append(mat)\n",
    "\n",
    "    return optimal_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D background manifold illustration\n",
    "Show also the 2D plane learned by the optimal linear manifold learning matrix $P$. \n",
    "\n",
    "We do not need to habituation IBCM, BioPCA, etc. on this manifold: we can just do an average subtraction simulation, extract the background series, compute the optimal $P$ for that background and random new odors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background initialization is specific to this simulation\n",
    "n_dimensions_toy = 25  \n",
    "n_components_toy = 2  # Number of background odors\n",
    "\n",
    "skp = 20 * int(1.0 / deltat)\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta_toy = np.random.default_rng(seed=0x220369e90599ffa80a743d99ac942f28)\n",
    "\n",
    "res = background_initialization_toy(n_dimensions_toy, n_components_toy, rgen_meta_toy, epsil=4.5)\n",
    "(back_components_toy, back_params_toy, init_bk_list_toy, \n",
    "         s_gamma_vecs_toy, simul_seed_toy, osn_ampli_toy) = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy average subtraction simulation, to get a background sample\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_avg_toy = np.zeros([1, n_dimensions_toy])\n",
    "\n",
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_avg_toy, update_fct, init_bk_list_toy, \n",
    "                [], inhib_rates, back_params_toy, duration, deltat,\n",
    "                seed=simul_seed_toy, noisetype=\"uniform\", skp=skp*2, **avg_options\n",
    ")\n",
    "tser_toy, tcser_toy, bkvecser_toy, _, _ = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal matrix P for this process\n",
    "optim_p = get_optimal_mat_p(bkvecser_toy, tcser_toy[:, :, 1], back_params_toy, np.ones(1), \n",
    "                      sd=0xe20d4b26b1c7e9b943cd23f4c9d15dca)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D manifold in a 3D slice,\n",
    "dims = (0, 2, 4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "where_01 = (tcser_toy[:, :, 1] > 0).astype(bool)\n",
    "locations = {\n",
    "    \"Both odors\": np.all(where_01, axis=1),  #all\n",
    "    \"Odor 0\": (where_01[:, 0] & ~where_01[:, 1]),  # 0\n",
    "    \"Odor 1\": (~where_01[:, 0] & where_01[:, 1])  # 1\n",
    "}\n",
    "all_colors = {\n",
    "    \"Both odors\": \"xkcd:purple\",\n",
    "    \"Odor 0\": \"xkcd:blue\",\n",
    "    \"Odor 1\": \"xkcd:red\",\n",
    "\n",
    "}\n",
    "\n",
    "# Background odors plane\n",
    "vecs = np.zeros(s_gamma_vecs_toy.shape)\n",
    "scale = 2.0\n",
    "orig = np.zeros([3, n_components_toy])\n",
    "for i in range(n_components_toy):\n",
    "    vecs[i] = s_gamma_vecs_toy[i] / np.sqrt(np.sum(s_gamma_vecs_toy[i]**2)) * scale\n",
    "    \n",
    "scale2 = 1.75\n",
    "topvecs = vecs.T\n",
    "s1, s2 = np.meshgrid(np.arange(0, 1, 0.2)*scale2, np.arange(0, 1, 0.2)*scale2)\n",
    "plane = s1[None, :, :] * topvecs[dims, 0:1, None] + s2[None, :, :] * topvecs[dims, 1:2, None]\n",
    "x, y, z = plane[0], plane[1], plane[2]\n",
    "ls = mpl.colors.LightSource(170, 45)\n",
    "rgb = ls.shade(z, cmap=mpl.cm.Greys, vert_exag=0.1, blend_mode='soft')\n",
    "#ax.plot_surface(x, y, z-0.05, color=\"grey\", alpha=0.3, linewidth=0.5, lightsource=ls)\n",
    "ax.plot_surface(x, y, z-0.05, color=\"grey\", alpha=0.3, rstride=1, cstride=1, facecolors=rgb,\n",
    "                       linewidth=0, antialiased=False, shade=True)\n",
    "\n",
    "for lbl in [\"Odor 0\", \"Odor 1\", \"Both odors\"]:\n",
    "    slc = locations[lbl]\n",
    "    ax.scatter(bkvecser_toy[slc, dims[0]], bkvecser_toy[slc, dims[1]], \n",
    "               bkvecser_toy[slc, dims[2]], s=4, lw=0.3, label=lbl, color=all_colors[lbl])\n",
    "\n",
    "ax.quiver(*orig, *(vecs[:, dims].T), color=\"k\", lw=1.5, arrow_length_ratio=0.2)\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=50)\n",
    "\n",
    "# Labeling\n",
    "for lbl, f in enumerate([ax.set_xlabel, ax.set_ylabel, ax.set_zlabel]):\n",
    "    zlbl = f(\"OSN {}\".format(lbl+1), labelpad=-17.5)\n",
    "for f in [ax.set_xticks, ax.set_yticks, ax.set_zticks]:\n",
    "    f([])\n",
    "for f in [ax.set_xticklabels, ax.set_yticklabels, ax.set_zticklabels]:\n",
    "    f([], pad=0.1)\n",
    "\n",
    "ax.view_init(azim=240, elev=35)\n",
    "\n",
    "#leg = ax.legend(loc=\"upper right\", bbox_to_anchor=(0.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Need to adjust the tightbox to remove whitespace above and below manually. \n",
    "#ax.set_aspect(\"equal\")\n",
    "fig.tight_layout()\n",
    "tightbox = fig.get_tightbbox()\n",
    "tightbox._bbox.y0 = tightbox._bbox.y0*1.6\n",
    "tightbox._bbox.y1 = tightbox._bbox.y1 - 0.8*tightbox._bbox.y0\n",
    "tightbox._bbox.x0 = tightbox._bbox.x0 * 0.7\n",
    "\n",
    "figname = \"background_manifold_2d_example.pdf\"\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(\"..\", \"figures\", \"nonlin_adapt\", figname), \n",
    "                transparent=True, bbox_inches=tightbox, bbox_extra_artists=(zlbl, leg))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples and surface\n",
    "if do_save_outputs:\n",
    "    fname = pj(outputs_folder, \"2d_manifold_nonlinear_osn.npz\")\n",
    "    np.savez_compressed(fname, conc_ser=tcser_toy[:, :, 1], bkvecser=bkvecser_toy[:, dims], \n",
    "                        plane=plane, vecs=s_gamma_vecs_toy[:, dims]*scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full simulation examples at different $\\epsilon$s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global model parameters for full simulations\n",
    "\n",
    "We will run simulations for different $\\epsilon$ values, with adjusted OSN amplitudes, so place most of the background odors generation in a simulation initialization function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly number\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "# Simulation skipping, 50 is enough for plots\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x624b611c73395cfc2578bb37a35b20d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_back_params(epsil, rgen, n_comp, n_dim):\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms = default_background_params(n_comp)\n",
    "\n",
    "    epsils_vec = np.full(n_dimensions, epsil)\n",
    "    back_comps = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Same adjustment of the OSN amplitude as in the performance recognition tests\n",
    "    raw_conc_factor = 2.0\n",
    "    raw_ampli = 2.5\n",
    "    np_statistic = np.mean  # np.mean, np.median, np.amax\n",
    "\n",
    "    raw_osn_activ = np_statistic(combine_fct(np.full(n_comp, raw_conc_factor * avg_whiff_conc), \n",
    "                                        back_comps, epsils_vec, fmax=1.0))\n",
    "    max_osn_ampli = raw_ampli / (raw_osn_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(max_osn_ampli)\n",
    "    back_pms.append(epsils_vec)\n",
    "    back_pms.append(back_comps)\n",
    "\n",
    "    # Initialization\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps, epsils_vec, fmax=max_osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_pms, init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation simulations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters, same for each tested epsilon\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00075  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.7/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation at a given $\\epsilon$\n",
    "\n",
    "Uses global IBCM parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_epsil(epsil, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing IBCM simulation for epsilon =\", epsil)\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(epsil, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "    \n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen_meta.standard_normal(size=[n_i_ibcm, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params_local, duration, \n",
    "                deltat, seed=simseed, noisetype=\"uniform\",  \n",
    "                skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation for epsilon =\", epsil, \"in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving functions for IBCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_ibcm_simul(results_raw, back_params):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "    Returns:\n",
    "        cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    back_components = back_params[-1]\n",
    "    epsil = back_params[-2]\n",
    "    basis = back_components / l2_norm(back_components, axis=1)[:, None] \n",
    "\n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                results_raw[3], coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = nuser_ibcm[:, :, 1]\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    ysernorm_ibcm = l2_norm(yser_ibcm, axis=1)\n",
    "    bkvecsernorm_ibcm = l2_norm(bkvecser_ibcm, axis=1)\n",
    "    \n",
    "    results_clean = (cbars_gamma, wser_ibcm, bkvecsernorm_ibcm, ysernorm_ibcm, \n",
    "                     moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_ibcm_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save cbar gamma series, that's all we really need for the figures\n",
    "    # Will run a separate short, non-skipped simulation to plot mixed concentrations\n",
    "    all_saved_series = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        (_, cbars_gamma, _, bkvecsernorm_ibcm,\n",
    "             ysernorm_ibcm, _, _, _) = all_results_clean[simname]\n",
    "        fullname = \"cbars_gamma_ser_\" + simname\n",
    "        all_saved_series[fullname] = cbars_gamma\n",
    "        # For habituation plots, save norm of background vs y\n",
    "        fullname = \"bkvec_norm_ser_\" + simname\n",
    "        all_saved_series[fullname] = bkvecsernorm_ibcm\n",
    "        fullname = \"y_norm_ser_\" + simname\n",
    "        all_saved_series[fullname] = ysernorm_ibcm\n",
    "    np.savez_compressed(fname, **all_saved_series)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, res_ibcm_clean):\n",
    "    (cbars_gamma, wser_ibcm, bkvecsernorm_ibcm, ysernorm_ibcm, \n",
    "    moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc) = res_ibcm_clean\n",
    "\n",
    "    # Plot of cbars gamma series\n",
    "    fig , ax, _ = plot_cbars_gamma_series(tser_common, cbars_gamma, \n",
    "                            skp=2, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plots of neuron specificities\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(correl_c_conc.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    print(\"Odor specificities:\", specif_gammas)\n",
    "    split_val = 2.5\n",
    "    for comp in range(n_components):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_ibcm_raw[2], res_ibcm_raw[7], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 100000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA initialization and simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 5e-5  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation_epsil(epsil, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing BioPCA simulation for epsilon =\", epsil)\n",
    "    # Initialize background parameters, give same rgenseed as IBCM to have same background\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(epsil, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "        \n",
    "    init_synapses_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back, biopca_rates, \n",
    "                inhib_rates, back_params_local, duration, deltat, \n",
    "                seed=simseed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation for epsilon =\", epsil, \"in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving functions for BioPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, cbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecsernorm_pca, ysernorm_pca, wser_pca,\n",
    "            true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, mser_pca, lser_pca, xser_pca, \n",
    "         cbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (bkvecsernorm_pca, ysernorm_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_biopca_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save true and learnt PCA, that's all we really need\n",
    "    true_learnt_pcas = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        (bkvecsernorm_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "         learnt_pca, off_diag_l_avg_abs, align_error_ser) = all_results_clean[simname]\n",
    "        fullname = \"true_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = true_pca[0]\n",
    "        fullname = \"learnt_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = learnt_pca[0]\n",
    "        fullname = \"pca_align_error_\" + simname\n",
    "        true_learnt_pcas[fullname] = align_error_ser\n",
    "        fullname = \"bkvec_norm_ser_\" + simname\n",
    "        true_learnt_pcas[fullname] = bkvecsernorm_pca\n",
    "        fullname = \"y_norm_ser_\" + simname\n",
    "        true_learnt_pcas[fullname] = ysernorm_pca\n",
    "        print(learnt_pca[1].shape)\n",
    "    np.savez_compressed(fname, **true_learnt_pcas)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_biopca_results(res_biopca_raw, res_biopca_clean):\n",
    "    (bkvecsernorm_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "    learnt_pca, off_diag_l_avg_abs, align_error_ser) = res_biopca_clean\n",
    "\n",
    "    # Plot learnt vs true PCA\n",
    "    fig, axes = plot_pca_results(tser_common/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "    axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "    axes[0].get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 2.5*plt.rcParams[\"figure.figsize\"][1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot level of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_biopca_raw[2], res_biopca_raw[8], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 100000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations at weak nonlinearity (moderate $\\epsilon = 6.0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tser_common = np.arange(0.0, duration, deltat*skp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_moderate = 6.5\n",
    "seed_moderate = 0x3e94bc0db982c06f17bf8db6e2a0afc5\n",
    "simul_seed_moderate = 0x95aeaedb6bca5aabb0e69e132ac9d557\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_mod, res_ibcm_mod = run_ibcm_simulation_epsil(epsil_moderate, n_components,  \n",
    "                         n_dimensions, seed_moderate, simul_seed_moderate)\n",
    "res_ibcm_clean_moderate = analyze_clean_ibcm_simul(res_ibcm_mod, back_ibcm_mod)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_mod, res_biopca_mod = run_biopca_simulation_epsil(epsil_moderate, n_components,\n",
    "                            n_dimensions, seed_moderate, simul_seed_moderate)\n",
    "res_biopca_clean_moderate = analyze_clean_biopca_simul(res_biopca_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_mod, res_ibcm_clean_moderate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_mod, res_biopca_clean_moderate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations at strong nonlinearity (small $\\epsilon = 3.0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_strong = 3.5\n",
    "seed_strong = 0xb57184f5b6a9d2d998b4fb016f366ef4\n",
    "simul_seed_strong = 0x424f10cb5dce0d8a10b31bd7d818e135\n",
    "\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_strong, res_ibcm_strong = run_ibcm_simulation_epsil(epsil_strong, n_components,  \n",
    "                         n_dimensions, seed_strong, simul_seed_strong)\n",
    "res_ibcm_clean_strong = analyze_clean_ibcm_simul(res_ibcm_strong, back_ibcm_strong)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_strong, res_biopca_strong = run_biopca_simulation_epsil(epsil_strong, n_components,\n",
    "                            n_dimensions, seed_strong, simul_seed_strong)\n",
    "res_biopca_clean_strong = analyze_clean_biopca_simul(res_biopca_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_strong, res_ibcm_clean_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_strong, res_biopca_clean_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations in the near-linear regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_low = 10.0\n",
    "seed_low = 0xe3a6a311d15ebcce72e08621bc900933\n",
    "simul_seed_low = 0x2ac46b6a33a7a949557ad7c06b9ecea8\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_low, res_ibcm_low = run_ibcm_simulation_epsil(epsil_low, n_components,  \n",
    "                         n_dimensions, seed_low, simul_seed_low)\n",
    "res_ibcm_clean_low = analyze_clean_ibcm_simul(res_ibcm_mod, back_ibcm_mod)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_low, res_biopca_low = run_biopca_simulation_epsil(epsil_low, n_components,\n",
    "                            n_dimensions, seed_low, simul_seed_low)\n",
    "res_biopca_clean_low = analyze_clean_biopca_simul(res_biopca_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_low, res_ibcm_clean_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison fo background inhibition?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ynorm_series = {\n",
    "    \"ibcm\": l2_norm(yser_ibcm), \n",
    "    \"biopca\": l2_norm(yser_pca), \n",
    "    \"avgsub\": l2_norm(yser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"orthogonal\": l2_norm(yser_orthogonal),\n",
    "    \"optimal\": l2_norm(yser_optimal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(ynorm_series[a], **std_options)) for a in ynorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(ynorm_series[a], **mean_options) for a in ynorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "ynorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + ynorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + ynorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results for final plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
