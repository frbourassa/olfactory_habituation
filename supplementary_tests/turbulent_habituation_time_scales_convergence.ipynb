{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to turbulent backgrounds as a function of model rates\n",
    "\n",
    "For IBCM, measure the robustness of alignment to one odor using, for each neuron, the difference between its maximum and second largest (or minimum?) alignments, or some similar metric. Then average across neurons to measure the full network convergence. \n",
    "\n",
    "For BioPCA, measure convergence to eigenvalues ? Or alignment subspace error as usual? \n",
    "\n",
    "Ideally, we would also measure new odor recognition performance? This would require a new simulation script and massive simulations, one ensemble per choice of $\\tau_\\Theta, \\mu$ for IBCM, and similarly for BioPCA... \n",
    "\n",
    "Things to check:\n",
    " - Convergence of IBCM as as function of $\\mu$ (or equivalently the OSN input amplitude? Or some moment of the background? not sure) and $\\tau_\\Theta$. \n",
    " - Convergence as a function of the number of odors, the strength of turbulence\n",
    " \n",
    "## Planning ahead\n",
    "Try with full-fledged turbulent backgrounds. Possibly need to run multiple background seeds, average convergence for each choice of rates or background size/turbulence strength. \n",
    " \n",
    "If it gets too murky, use a simpler background: the outcomes were very clear for a 2D toy model background, perhaps with 3+ odors but milder fluctuations it is also clearer. Easier to tune the variance of a O-U background than the turbulent process. Also, would allow us to use the Intrator default model version, rather than the Law and Cooper modification, which speeds up convergence in a hardly predictable way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse, special\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_hgammas_hbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw, \n",
    "    generate_odorant\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"convergence\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"convergence\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background generation and initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combi(concs, backs):\n",
    "    \"\"\" concs: shaped [..., n_odors]\n",
    "        backs: 2D array, shaped [n_odors, n_osn]\n",
    "    \"\"\"\n",
    "    return concs.dot(backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global choice of background and odor mixing functions\n",
    "update_fct = update_powerlaw_times_concs\n",
    "combine_fct = linear_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will later explore the effect of varying these parameters on the convergence, \n",
    "# but put the default ones in a function\n",
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background initialization, given parameters and a seeded random generator\n",
    "def initialize_given_background(back_pms, rgen, n_comp, n_dim):\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-1], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    back_comps = back_pms[-1]\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps)\n",
    "    # background random variables are first in the list of initial values\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM simulation functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def run_ibcm_simulation(ibcm_rates_loc, back_params_loc, inhib_rates_loc, \n",
    "                        options_loc, dimensions, initseed, simseed, \n",
    "                        duration_loc=360000.0, dt_loc=1.0, skp_loc=20):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            ibcm_rates_loc\n",
    "            back_params_loc\n",
    "            inhib_rates_loc: alpha, beta\n",
    "            options_loc: model options\n",
    "            dimensions: [n_components (n_b), n_dimensions (n_s), n_neurons (n_i)]\n",
    "            initseed: int seed for background and weights initialization\n",
    "            simseed: int seed for simulation\n",
    "            \n",
    "    \"\"\"\n",
    "    learnrate = ibcm_rates_loc[0]\n",
    "    tauavg = ibcm_rates_loc[1]\n",
    "    print(\"Initializing IBCM simulation for mu =\", learnrate, \", theta =\", tauavg)\n",
    "    \n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    n_comp, n_dim, n_i = dimensions\n",
    "    \n",
    "    # Create random generator with initseed for initial value choice\n",
    "    # Initialize background first with it, so a given seed always gives same \n",
    "    # initial background state\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    init_back = initialize_given_background(back_params_loc, rgen_init, n_comp, n_dim)\n",
    "\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen_init.standard_normal(size=[n_i, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates_loc, inhib_rates_loc, back_params_loc, \n",
    "                duration_loc, dt_loc, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_loc, **options_loc\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze to establish convergence to specific fixed points. \n",
    "def analyze_ibcm_simulation(sim_results, ibcm_rates_loc, back_pms, \n",
    "                            skp_loc=20, dt=1.0, duration_loc=360000.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sim_results = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            hbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "        ibcm_rates_loc: learnrate_ibcm, tau_avg_ibcm, coupling_eta_ibcm, ...\n",
    "            \n",
    "    Returns:\n",
    "        alignment_gaps, indexed [neuron]\n",
    "        specif_gammas, indexed [neuron]\n",
    "        gamma_vari, indexed [neuron, component]\n",
    "    \"\"\"\n",
    "    coupling = ibcm_rates_loc[2]\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "        hbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = sim_results\n",
    "    # Calculate hgammas_bar and mbars\n",
    "    transient = int(5/6*duration_loc / dt) // skp_loc\n",
    "    basis = back_pms[-1]\n",
    "    \n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, hbars_gamma = compute_mbars_hgammas_hbargammas(mser_ibcm, coupling, basis)\n",
    "    hbars_gamma_mean = np.mean(hbars_gamma[transient:], axis=0)\n",
    "    # Sorted odor indices, from min to max, of odor alignments for each neuron\n",
    "    aligns_idx_sorted = np.argsort(hbars_gamma_mean, axis=1) \n",
    "    specif_gammas = np.argmax(hbars_gamma_mean, axis=1)\n",
    "    assert np.all(specif_gammas == aligns_idx_sorted[:, -1])\n",
    "    \n",
    "    \n",
    "    # Gap between first and second largest alignments for each neuron\n",
    "    n_i = hbars_gamma_mean.shape[0]\n",
    "    alignment_gaps = (hbars_gamma_mean[np.arange(n_i), specif_gammas]\n",
    "                     - hbars_gamma_mean[np.arange(n_i), aligns_idx_sorted[:, -2]])\n",
    "    \n",
    "    # Variance (fluctuations) of hbars gamma in the last 20 minutes of the simul\n",
    "    # Increases when the learning rate increases\n",
    "    last_steps = int(2.0*duration_loc/3.0 / dt) // skp_loc\n",
    "    hbars_gamma_vari = np.var(hbars_gamma[last_steps:], axis=0)\n",
    "    \n",
    "    return hbars_gamma, alignment_gaps, specif_gammas, hbars_gamma_vari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analyze_ibcm_one_back_seed(\n",
    "        ibcm_rates_loc, back_rates, inhib_rates_loc, \n",
    "        options_loc, dimensions, seedseq, \n",
    "        duration_loc=360000.0, dt_loc=1.0, skp_loc=20, full_returns=False\n",
    "    ):\n",
    "    \"\"\" Given IBCM model rates and background parameters except\n",
    "    background odors (but incl. number odors, c0), and a main seed sequence, \n",
    "    run and analyze convergence of IBCM on the background generated from that seed. \n",
    "    The seedseq should itself have been spawned from a root seed to have a distinct\n",
    "    one per run; this still makes seeds reproducible yet distinct for different runs. \n",
    "    The seedseq here is spawned again for a background gen. seed and a simul. seed. \n",
    "    \n",
    "    Args:\n",
    "        dimensions: gives [n_components, n_dimensions, n_i_ibcm]\n",
    "    \n",
    "    Returns:\n",
    "        iff full_return:\n",
    "            gaps, specifs, hgamvari, hgammas_ser, sim_results\n",
    "        else:\n",
    "            gaps, specifs, hgamvari, None, None\n",
    "        alignment_gaps: indexed [neuron]\n",
    "        specif_gammas: indexed [neuron]\n",
    "        gamma_vari: indexed [neuron, component]\n",
    "    \"\"\"\n",
    "    #print(\"Initializing IBCM simulation...\")\n",
    "    # Get dimensions\n",
    "    n_comp, n_dim, n_i = dimensions\n",
    "    \n",
    "    # Spawn back. generation seed and simul seed\n",
    "    initseed, simseed = seedseq.spawn(2)\n",
    "    \n",
    "    # Duplicate back params before appending locally-generated odor vectors to them\n",
    "    back_pms_loc = list(back_rates)\n",
    "    \n",
    "    # Create background\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    back_comps_loc = generate_odorant((n_comp, n_dim), rgen_init)\n",
    "    back_comps_loc = back_comps_loc / l2_norm(back_comps_loc, axis=1)[:, None]\n",
    "\n",
    "    # Add odors to the list of background parameters\n",
    "    back_pms_loc.append(back_comps_loc)\n",
    "\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    init_back = initialize_given_background(back_pms_loc, rgen_init, n_comp, n_dim)\n",
    "\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    lambd_loc = ibcm_rates_loc[3]\n",
    "    init_synapses_ibcm = 0.2*rgen_init.standard_normal(size=[n_i, n_dim])*lambd_loc\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    #print(\"Running IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates_loc, inhib_rates_loc, back_pms_loc, \n",
    "                duration_loc, dt_loc, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_loc, **options_loc\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    #print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Now analyze IBCM simul for convergence\n",
    "    #print(\"Starting to analyze IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    hgammas_ser, gaps, specifs, hgamvari = analyze_ibcm_simulation(sim_results, \n",
    "                        ibcm_rates_loc, back_pms_loc, skp_loc=skp_loc, duration_loc=duration_loc)\n",
    "    tend = perf_counter()\n",
    "    #print(\"Finished analyzing IBCM simulation\")\n",
    "    \n",
    "    # Doesn't return full c gamma series, only the summary statistics of convergence\n",
    "    if full_returns:\n",
    "        hgammas_ser_ret = hgammas_ser\n",
    "        sim_results_ret = sim_results\n",
    "    else:\n",
    "        hgammas_ser_ret = None\n",
    "        sim_results_ret = None\n",
    "    \n",
    "    return gaps, specifs, hgamvari, hgammas_ser_ret, sim_results_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPCA simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation(biopca_rates_loc, back_params_loc, inhib_rates_loc, \n",
    "                        options_loc, dimensions, initseed, simseed, \n",
    "                        duration_loc=360000.0, dt_loc=1.0, skp_loc=20):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            biopca_rates_loc\n",
    "            back_params_loc\n",
    "            inhib_rates_loc: alpha, beta\n",
    "            options_loc: model options\n",
    "            dimensions: [n_components (n_b), n_dimensions (n_s), n_neurons (n_i)]\n",
    "            initseed: int seed for background and weights initialization\n",
    "            simseed: int seed for simulation\n",
    "            \n",
    "    \"\"\"\n",
    "    print(\"Initializing BioPCA simulation...\")\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    n_comp, n_dim, n_i = dimensions\n",
    "    \n",
    "    # Create random generator with initseed for initial value choice\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    init_back = initialize_given_background(back_params_loc, rgen_init, n_comp, n_dim)\n",
    "\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_pca = rgen_init.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen_init.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back, \n",
    "                biopca_rates_loc, inhib_rates_loc, back_params_loc, \n",
    "                duration_loc, dt_loc, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_loc, **options_loc\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of analyses to do on BioPCA results\n",
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    We do not need to save odor vectors (back_components), \n",
    "    since the IBCM simulation will provide them for both models. \n",
    "    \n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, hbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "            learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, eps_ser, mser_pca, lser_pca, xser_pca, \n",
    "         hbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (bkvecsernorm_pca, eps_ser, ysernorm_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy a simplification for once: we do not need to consider other models like average subtraction, optimal $P$, orthogonal projection since all we care about in this notebook is the convergence of the two biologically plausible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, hbars_gamma, skp=20):\n",
    "    # tseries, bk_series, bkvec_series, m_series,\n",
    "    # hbar_series, theta_series, w_series, y_series\n",
    "    tser, bkser, bkvecser, _, _, _, _, yser = res_ibcm_raw\n",
    "    tser_scaled = tser *  10.0 / 60.0  # in min\n",
    "    # Plot of hbars gamma series\n",
    "    fig , ax, _ = plot_hbars_gamma_series(tser_scaled, hbars_gamma, \n",
    "                            skp=skp, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser, bkvecser, yser, skp=skp)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation parameter choices\n",
    "\n",
    "## Simulation parameters common to all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for all simulations\n",
    "# Dimensions: 25 is enough?\n",
    "n_dimensions = 25\n",
    "n_components = 4  # try with 3 for simplicity by default\n",
    "\n",
    "# Inhibition W learning and decay rates\n",
    "inhib_rates_default = [0.0001, 0.00002]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration and integration time step\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Saving every skp simulation point, 50 is enough for plots, \n",
    "# here use 20 to get convergence time accurately\n",
    "skp_default = 20 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp_default)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Default model rates\n",
    "learnrate_ibcm = 0.0005 #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates_default = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "meta_seedseq = np.random.SeedSequence(0xa5ad7857493a6fc471b0283b8db69268)\n",
    "#initial_seed = 0x36d8487b210fe0277493149915aba4f2\n",
    "#simul_seed = 0xb1b9996e9f459ffc4ec9fc95c2db5583\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "back_rates_default = default_background_params(n_components)\n",
    "# Try changing background rates here if desired\n",
    "# Concentration scale c0: multiply up-down to change convergence dynamics\n",
    "# just as well as the learning rate, although with a different scaling. \n",
    "back_rates_default[4][:] = 0.6\n",
    "back_rates_default[1][:] = 500.0  # whiff duration\n",
    "back_rates_default[3][:] = 800.0  # blank duration\n",
    "dimensions_ibcm = [n_components, n_dimensions, n_i_ibcm]\n",
    "\n",
    "#ibcm_rates_loc, back_rates, inhib_rates_loc, \n",
    "#        options_loc, dimensions, seedseq, \n",
    "#        duration_loc=360000.0, dt_loc=1.0, skp_loc=20, full_return=False\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "all_res = run_analyze_ibcm_one_back_seed(ibcm_rates_default, back_rates_default, inhib_rates_default, \n",
    "                        ibcm_options, dimensions_ibcm, meta_seedseq,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)\n",
    "\n",
    "gaps, specifs, hgamvari, hgammas_ser, ibcm_results = all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "plot_ibcm_results(ibcm_results, hgammas_ser, skp=skp_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some other convergence analysis results\n",
    "print(\"{} out of {} odors covered\".format(np.unique(specifs).size, n_components))\n",
    "print(\"Variance of the largest hbar_gamma for each neuron:\\n\", hgamvari[np.arange(n_i_ibcm), specifs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(n_i_ibcm), gaps, marker=\"o\", mfc=\"w\", ms=8)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.annotate(str(specifs[i]), xy=(i, gaps[i]), ha=\"center\", va=\"center\")\n",
    "ax.set_ylim([0.0, gaps.max()*1.1])\n",
    "ax.set(ylabel=\"Alignment gap\", xlabel=\"Neuron\", xticks=np.arange(0, n_i_ibcm, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_conc = np.mean(ibcm_results[1][:, :, 1])\n",
    "conc_scale = back_rates_default[4].mean()\n",
    "moments_conc = [\n",
    "    mean_conc/conc_scale, \n",
    "    np.var(ibcm_results[1][:, :, 1])/conc_scale**2,\n",
    "    np.mean((ibcm_results[1][:, :, 1] - mean_conc)**3.0)/conc_scale**3\n",
    "]\n",
    "print(moments_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop main simulations\n",
    "For a grid of $\\mu, \\tau_\\Theta$ choices, \n",
    "\n",
    "Later on, do the same for 3, 4, 5, 6, 7, 8 + odors\n",
    "\n",
    "And do the same for different whiff and blank durations (scale both up or down?)\n",
    "\n",
    "And do the same for the concentration amplitude, controlled by $c_0$, to show it is equivalent to scaling $\\mu$?\n",
    "\n",
    "Still need to figure out analytically which moment of the background (not just its scale) determines the time scale... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threadpoolctl import threadpool_limits\n",
    "from utils.cpu_affinity import count_parallel_cpu, count_threads_per_process\n",
    "import multiprocessing\n",
    "from simulfcts.idealized_recognition import func_wrapper_threadpool\n",
    "\n",
    "#def func_wrapper_threadpool(func, threadlim, *args, **kwargs):\n",
    "#    with threadpool_limits(limits=threadlim, user_api='blas'):\n",
    "#        res = func(*args, **kwargs)\n",
    "#    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_seed_results(res_dict, n_seeds):\n",
    "    \"\"\" Combine convergence analysis results of simulation seeds\n",
    "    in res_dict. \"\"\"\n",
    "    combi_gaps, combi_specifs, combi_varis = [], [], []\n",
    "    for i in range(n_seeds):\n",
    "        gaps, specifs, hgamvari, _, _ = res_dict[i]\n",
    "        combi_gaps.append(gaps)\n",
    "        combi_specifs.append(specifs)\n",
    "        combi_varis.append(hgamvari)\n",
    "    return [np.stack(a) for a in [combi_gaps, combi_specifs, combi_varis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test version that does not really use multiprocessing\n",
    "# because it doesn't work with functions defined in the notebook\n",
    "# but the point is to develop functions for the full script in secondary_scripts/\n",
    "def main_convergence_vs_ibcm_rates(orig_seedseq, n_seeds):\n",
    "    \"\"\" Run n_seeds IBCM simulations for each combination of mu learning rate\n",
    "    and tau_theta averaging time scale, collect convergence statistics for each. \n",
    "    The same simulation seeds are tested at each combinatino of model rates, \n",
    "    to assess the convergence vs these rates while keeping the set of\n",
    "    backgrounds tested the same, for a more direct comparison. \n",
    "    \n",
    "    Since this is a main function, the rates grid and other model parameters are\n",
    "    defined within. \n",
    "    \n",
    "    Args:\n",
    "        orig_seedseq (np.random.SeedSequence): fresh SeedSequence from which\n",
    "            all other simulation seeds will be spawned. \n",
    "    \n",
    "    Returns:\n",
    "        learnrate_tautheta_grid (np.ndarray): indexed [2, mu_idx, tau_idx]\n",
    "            i.e. the first array along axis 0 contains the 2d grid of mu vals, \n",
    "            the second array contains the 2d grid of tau values, and in each grid,\n",
    "            mu varies along axis 0 (rows), tau varies along axis 1 (columns); \n",
    "            the result of np.meshgrid(murange, taurange, indexing=\"ij\"\n",
    "        all_gaps (np.ndarray): indexed [mu, tau, seed, neuron]\n",
    "        all_specifs (np.ndarray): indexed [mu, tau, seed, neuron]\n",
    "        all_varis (np.ndarray): indexed [mu, tau, seed, neuron, component]\n",
    "    \"\"\"\n",
    "    # Grid of IBCM rates mu, tau_theta in a range going to either side\n",
    "    # of the region where we get convergence for N_B = 3 odors\n",
    "    # Approximately geomspace, clustered around usual rates (0.75e-3 to 1.25e-3)\n",
    "    learnrate_range = np.asarray([5e-5, 2e-4, 5e-4, 7.5e-4, 1.25e-3, 2e-3, 5e-3, 2e-2])[:2]\n",
    "    # Also a somewhat geometric progression, clustered around good ones (800-1200-1600)\n",
    "    tautheta_range = np.asarray([100, 200, 400, 800, 1200, 1600, 2000, 3000])[:2]\n",
    "    learnrate_tautheta_grid = np.stack(\n",
    "        np.meshgrid(learnrate_range, tautheta_range, indexing=\"ij\"), axis=0)\n",
    "    # learnrate varies along axis 0 (y, rows), tautheta along axis 1 (x, columns)\n",
    "    \n",
    "    # Define simulation and model parameters\n",
    "    n_i_ibcm_sim = 24\n",
    "    n_dims_sim = 25\n",
    "    n_comp_sim = 3\n",
    "    dimensions_sim = [n_comp_sim, n_dims_sim, n_i_ibcm_sim]\n",
    "    \n",
    "    # Default IBCM model rates\n",
    "    learnrate_ibcm_sim = 0.00125  # will vary\n",
    "    tau_avg_ibcm_sim = 1200  # will vary\n",
    "    coupling_eta_ibcm_sim = 0.6/n_i_ibcm_sim\n",
    "    ssat_ibcm_sim = 50.0\n",
    "    k_c2bar_avg_sim = 0.1\n",
    "    decay_relative_ibcm_sim = 0.005\n",
    "    lambd_ibcm_sim = 1.0\n",
    "    ibcm_rates_sim = [\n",
    "        learnrate_ibcm_sim, \n",
    "        tau_avg_ibcm_sim, \n",
    "        coupling_eta_ibcm_sim, \n",
    "        lambd_ibcm_sim,\n",
    "        ssat_ibcm_sim, \n",
    "        k_c2bar_avg_sim,\n",
    "        decay_relative_ibcm_sim\n",
    "    ]\n",
    "    ibcm_options_sim = {\n",
    "        \"activ_fct\": \"identity\",\n",
    "        \"saturation\": \"tanh\", \n",
    "        \"variant\": \"law\",   # maybe we will want to test \"intrator\" later?\n",
    "        \"decay\": True\n",
    "    }\n",
    "    # default turbulent background parameters\n",
    "    back_rates_sim = default_background_params(n_comp_sim)\n",
    "    # Default alpha, beta\n",
    "    inhib_rates_sim = [0.0001, 0.00002]  # alpha, beta\n",
    "    \n",
    "    # Time parameters\n",
    "    duration_sim = 36000.0\n",
    "    deltat_sim = 1.0\n",
    "    skp_sim = 20\n",
    "    \n",
    "    # Containers for alignment gaps, specificities, and hgammas variances\n",
    "    # that will be stacked arrays indexed [mu, tau, seed, ...]\n",
    "    all_gaps, all_specifs, all_varis = [], [], []\n",
    "    \n",
    "    # Spawn simulation seeds, reused at each combination on the IBCM rates grid\n",
    "    simul_seeds = orig_seedseq.spawn(n_seeds)\n",
    "    n_workers = min(count_parallel_cpu(), n_seeds)\n",
    "    n_threads = count_threads_per_process(n_workers)\n",
    "    pool = multiprocessing.Pool(n_workers)\n",
    "\n",
    "    # Treat one rate combination at a time\n",
    "    for i in range(learnrate_range.shape[0]):\n",
    "        mu = learnrate_range[i]\n",
    "        i_gaps, i_specifs, i_varis = [], [], []\n",
    "        for j in range(tautheta_range.shape[0]):\n",
    "            tau = tautheta_range[j]\n",
    "            ibcm_rates_sim[0] = mu\n",
    "            ibcm_rates_sim[1] = tau\n",
    "            # Launch multiple seeds for the current (mu, tau) combination\n",
    "            all_procs_mutau = {}\n",
    "            res_seeds_mutau = {}\n",
    "            for k in range(n_seeds):\n",
    "                apply_args = (run_analyze_ibcm_one_back_seed, n_threads, \n",
    "                              ibcm_rates_sim, back_rates_sim, inhib_rates_sim,  \n",
    "                              ibcm_options_sim,dimensions_sim, simul_seeds[k])\n",
    "                apply_kwds = dict(duration_loc=duration_sim, dt_loc=deltat_sim, \n",
    "                                  skp_loc=skp_sim, full_returns=False)\n",
    "                #all_procs_mutau[k] = pool.apply_async(func_wrapper_threadpool, \n",
    "                #                 args=apply_args, kwds=apply_kwds)\n",
    "                res_seeds_mutau[k] = func_wrapper_threadpool(*apply_args, **apply_kwds)\n",
    "                \n",
    "\n",
    "            # Collect convergence analysis results for this mu, tau\n",
    "            #res_seeds_mutau = {k:all_procs_mutau[k].get() for k in all_procs_mutau.keys()}\n",
    "            # Stack them over seeds\n",
    "            combined_seed_res = combine_seed_results(res_seeds_mutau, n_seeds)\n",
    "            i_gaps.append(combined_seed_res[0])\n",
    "            i_specifs.append(combined_seed_res[1])\n",
    "            i_varis.append(combined_seed_res[2])\n",
    "            print(\"Finished mu i = {}, tau j = {}\".format(i, j))\n",
    "        \n",
    "        # Stack arrays over j (tau_theta) for the current i value (mu)\n",
    "        all_gaps.append(np.stack(i_gaps))\n",
    "        all_specifs.append(np.stack(i_specifs))\n",
    "        all_varis.append(np.stack(i_varis))\n",
    "    \n",
    "    # Stack arrays over i (mu)\n",
    "    all_gaps = np.stack(all_gaps)\n",
    "    all_specifs = np.stack(all_specifs)\n",
    "    all_varis = np.stack(all_varis)\n",
    "    \n",
    "    # Reuse pool for each (mu, tau) but close them at the end\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return learnrate_tautheta_grid, all_gaps, all_specifs, all_varis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutau_grid, align_gaps, specifs, varis = main_convergence_vs_ibcm_rates(\n",
    "#    np.random.SeedSequence(0xf44f62d0818452d631061e695b75c517), 2)\n",
    "\n",
    "# Load complete simulations run on the cluster\n",
    "conv_results = np.load(pj(outputs_folder, \"convergence_vs_ibcm_rates_results.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutau_grid = conv_results[\"mutau_grid\"]\n",
    "align_gaps = conv_results[\"align_gaps\"]\n",
    "gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "hgamma_varis = conv_results[\"hgamma_varis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "tau_range = mutau_grid[1, 0, :]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_tau)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    gap_mean_line_mu = np.mean(align_gaps[i, :, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_mu = np.std(np.mean(align_gaps[i, :, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(tau_range, gap_mean_line_mu, color=colors[i], label=mutau_grid[0, i, 0])\n",
    "    ax.fill_between(tau_range, gap_mean_line_mu-gap_std_line_mu, \n",
    "                   gap_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"Averaging time scale $\\tau_{\\Theta}$\", \n",
    "       ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\mu$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_gaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage of odors: fraction of seeds where all 3 odors are covered?\n",
    "fig, ax = plt.subplots()\n",
    "# extent: left, right, bottom, top\n",
    "#im = ax.imshow(np.mean(align_gaps, axis=(2, 3)).T, \n",
    "#          extent=(mu_range.min(), mu_range.max(), tau_range.min()*0.01, tau_range.max()*0.01), \n",
    "#         cmap=\"viridis\", aspect=\"auto\")\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], np.mean(align_gaps, axis=(2, 3)), cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "fig.colorbar(im, label=\"Alignment gap\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "stdev_specifs = np.zeros(hgamma_varis.shape[:4])\n",
    "n_i = gamma_specifs.shape[3]\n",
    "for i in range(n_mu):\n",
    "    for j in range(n_tau):\n",
    "        for k in range(hgamma_varis.shape[2]):\n",
    "            stdev_specifs[i, j, k] = hgamma_varis[i, j, k][np.arange(n_i), gamma_specifs[i, j, k]]\n",
    "        \n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Standard dev. of $h_{\\gamma,sp}$\" \"\\n(smaller is better)\", \n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", ncol=2, frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "n_i = gamma_specifs.shape[3]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    std_mean_line_mu = np.mean(stdev_specifs[i, :, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    std_std_line_mu = np.std(stdev_specifs[i, :, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(tau_range, std_mean_line_mu, color=colors[i], label=mutau_grid[0, i, 0])\n",
    "    #ax.fill_between(tau_range, std_mean_line_mu-std_std_line_mu, \n",
    "    #               std_mean_line_mu+std_std_line_mu, color=colors[i], alpha=0.15)\n",
    "ax.set(xlabel=r\"Averaging time $\\tau_\\Theta$\", \n",
    "       ylabel=r\"Standard dev. of $h_{\\gamma,sp}$\" \"\\n(smaller is better)\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\mu$\", frameon=False, bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ascertain plot vs tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage of odors: fraction of seeds where all 3 odors are covered?\n",
    "odor_coverage = np.zeros(gamma_specifs.shape[:3])\n",
    "for i in range(n_mu):\n",
    "    for j in range(n_tau):\n",
    "        for k in range(gamma_specifs.shape[2]):\n",
    "            odor_coverage[i, j, k] = np.unique(gamma_specifs[i, j, k]).size\n",
    "odor_coverage_mean = np.mean(odor_coverage, axis=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# extent: left, right, bottom, top\n",
    "#im = ax.imshow(odor_coverage_mean.T, \n",
    "#          extent=(mu_range.min(), mu_range.max(), tau_range.min()*0.01, tau_range.max()*0.01), \n",
    "#         cmap=\"viridis\", aspect=\"auto\")\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], odor_coverage_mean, cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\", yscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "ax.set_yticks(tau_range)\n",
    "fig.colorbar(im, label=\"Average # odors covered\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_line_tau = odor_coverage_mean[:, j]\n",
    "    coverage_std_tau = np.std(odor_coverage[:, j], axis=1)  # std across seed\n",
    "    #ax.plot(mu_range, coverage_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.plot(mu_range, np.mean(odor_coverage[:, j], axis=1), color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, coverage_line_tau-coverage_std_tau, \n",
    "    #               coverage_line_tau+coverage_std_tau, color=colors[j], alpha=0.0)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Average # odors covered\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
