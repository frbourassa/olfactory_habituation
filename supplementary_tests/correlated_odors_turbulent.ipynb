{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlating turbulent background processes\n",
    "\n",
    "We first simulate independent concentration variables $\\nu(t)$ and consider their zero-meaned version, $\\tilde{\\nu}(t) = \\nu(t) - \\langle \\nu \\rangle$. These variables have mean zero, a diagonal covariance matrix $\\sigma^2 \\mathbb{1}$, and third moment $m_3$ (while mixed third-order moments of different $\\tilde{\\nu}_\\mu$ are zero due to their zero average). \n",
    "\n",
    "Then, we can transform these variables to have some desired covariance matrix $C$ with Cholesky decomposition $\\sigma^2 RR^T = \\Sigma$ (i.e. $R$ is the Cholesky decomposition of $\\Sigma / \\sigma^2$ and thus has scale $1/\\sigma$). Indeed, we take concentrations $c = R \\tilde{\\nu} + \\langle \\nu \\rangle$. These have the same mean as the original variables, $\\langle c \\rangle = \\langle \\nu \\rangle$, and the desired covariance matrix, \n",
    "\n",
    "$$ C = \\langle (c - \\langle c \\rangle) (c - \\langle c \\rangle)^T \\rangle = \\langle R \\tilde{\\nu} \\tilde{\\nu}^T R^T \\rangle = \\sigma^2 R R^T = \\Sigma $$\n",
    "\n",
    "The third moments are altered by this transformation, but they are not generally zero, so we can still numerically get the IBCM model to converge. \n",
    "\n",
    "We consider what happens as a pair of odors progressively gets more correlated. We take the last two odors for this. This corresponds to a covariance matrix\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & 0 & \\ldots & 0 & 0 \\\\\n",
    "    0 & 1 & \\ldots & 0 & 0 \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    0 & 0 & \\ldots & 1 & \\rho  \\\\\n",
    "    0 & 0 & \\ldots & \\rho & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "where $-1 < \\rho < 1$ is the Pearson correlation coefficient. Then $R$ is the Cholesky decomposition of the matrix on the right, which has a simple expression because only a $2x2$ block is not diagonal:\n",
    "\n",
    "$$ R = \\sigma \\begin{pmatrix}\n",
    "    1 & 0 & \\ldots & 0 & 0 \\\\\n",
    "    0 & 1 & \\ldots & 0 & 0 \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    0 & 0 & \\ldots & 1 & 0 \\\\\n",
    "    0 & 0 & \\ldots & \\rho & \\sqrt{1 - \\rho^2}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We also test the limiting case $\\rho = 0$, in which case the Cholesky decomposition does not formally exist, but we can still mix odors with a matrix\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 &  \\ldots & 0 & 0 \\\\\n",
    "    \\ldots  & \\ldots & \\ldots \\\\\n",
    "    0 & \\ldots & 1 & 0  \\\\\n",
    "    0 & \\ldots & 1 & 0\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "## In this notebook\n",
    "We run simulations for a few different $\\rho$s with a given background seed, to illustrate how each model (IBCM, BioPCA) behaves as $\\rho$ increases. \n",
    "A systematic check of how background correlations affect new odor recognition is done in ``secondary_scripts/run_performance_correlation.py`` and ``secondary_scripts/analyze_correlation_results.py``. The present notebook is a companion to these. \n",
    "\n",
    "### Remark\n",
    "\n",
    "If the background has very strong correlation between odors, it may not be effectively $N_\\mathrm{B}$-dimensional anymore, so the representation in terms of the dot products with original odors, $h_{\\gamma}$, may not be appropriate. There could be a new basis picked up by neurons; some decomposition of the mixture $\\sum_\\gamma \\xi_\\gamma \\mathbf{y}_\\gamma$ where the $\\xi_{\\gamma}$ are not odor concentrations but other independent components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json, sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "from os.path import join as pj\n",
    "    \n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_hgammas_hbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_mixed_concs,\n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw,\n",
    "    sample_ss_mixed_concs_powerlaw,\n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_cholesky(correl, n_comp):\n",
    "    target_covmat_scaled = np.zeros((n_comp, n_comp))\n",
    "    target_covmat_scaled[[-1, -2], [-2, -1]] = correl\n",
    "    target_covmat_scaled[np.diag_indices(n_comp)] = 1.0\n",
    "    if abs(correl) < 1.0:\n",
    "        target_cholesky = np.linalg.cholesky(target_covmat_scaled)\n",
    "    else:\n",
    "        target_cholesky = np.zeros((n_comp, n_comp))\n",
    "        target_cholesky[np.diag_indices(n_comp)] = 1.0\n",
    "        target_cholesky[-1, -1] = 0.0\n",
    "        target_cholesky[-1, -2] = correl  # Replace odor 1 by odor 0 for rho = +-1\n",
    "    return target_cholesky\n",
    "\n",
    "def mix_concs(nuser, means, chol):\n",
    "    mixed_concs_ser = np.einsum(\"ij,kj->ki\", chol, nuser - means) + means\n",
    "    return mixed_concs_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_outputs = False\n",
    "do_save_plots = False\n",
    "\n",
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"optimal\": \"Optimal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"optimal\": \"xkcd:powder blue\",\n",
    "    \"ideal\": \"xkcd:light green\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Half the real number for faster simulations\n",
    "n_components = 4  # Number of background odors\n",
    "\n",
    "inhib_rates = [5e-5, 1e-5]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 1\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  # \"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_mixed_concs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "# This seed gave nicely spread out odors easier to learn 0xe329714605b83365e67b44ed7e001ec\n",
    "# Another random seed: 0xb7bf767bbad297aeeee19d0ccdc3647e\n",
    "rgen_meta = np.random.default_rng(seed=0xb7bf767bbad297aeeee19d0ccdc3647e)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "\n",
    "# Compute mean of independent underlying variables, \n",
    "# to determine the mean and target covariance of mixed variables\n",
    "tblo, tbhi, twlo, twhi = back_params[2], back_params[3], back_params[0], back_params[1]\n",
    "whiffprob = np.mean(1.0 / (1.0 + np.sqrt(tblo*tbhi/twlo/twhi)))\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*back_params[4:6]))\n",
    "mean_conc = whiffprob * avg_whiff_conc  # average time in whiffs vs blanks * average whiff conc\n",
    "print(\"Analytical mean conc:\", mean_conc)\n",
    "#print(\"Numerical mean conc:\", mean_conc_empirical)\n",
    "\n",
    "# Choose desired Pearson correlation between odors\n",
    "# We will vary this below\n",
    "# Up to 0.5 there is some convergence, beyond, issue. \n",
    "# Note that for rho = 1, there would effectively be only one odor. \n",
    "correl_rho = 0.7\n",
    "\n",
    "# Target covariance matrix (scaled by variance of underlying independent variables)\n",
    "target_cholesky = get_target_cholesky(correl_rho, n_components)\n",
    "target_covmat_scaled = target_cholesky.dot(target_cholesky.T)\n",
    "print(target_covmat_scaled)\n",
    "\n",
    "# Add mean conc and Cholesky mixing matrix to parameters\n",
    "back_params.append(mean_conc)\n",
    "back_params.append(target_cholesky)\n",
    "# Then add background odor vectors last to that list\n",
    "back_params.append(back_components)\n",
    "\n",
    "# Initial values of background process variables (underlying independent (t, c))\n",
    "init_concs_ind = sample_ss_conc_powerlaw(*back_params[:-3], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components), *back_params[2:4])\n",
    "tc_init = np.stack([init_times, init_concs_ind.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector \n",
    "init_concs_mix = target_cholesky.dot(tc_init[:, 1] - mean_conc) + mean_conc\n",
    "init_bkvec = init_concs_mix.dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background process example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a dense simulation to extract mixed concentrations for\n",
    "# global correl_rho chosen above (0.7)\n",
    "# Dummy initialization\n",
    "simul_seed2 = seed_from_gen(rgen_meta)\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])\n",
    "\n",
    "sim_avg_res = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed2, noisetype=\"uniform\", skp=1, **avg_options\n",
    ")\n",
    "\n",
    "_, bkser_avg, bkvecser_mixed, _, _ = sim_avg_res\n",
    "mixed_concs_sample = mix_concs(bkser_avg[:, :, 1], mean_conc, target_cholesky)\n",
    "del sim_avg_res, bkser_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mixed concentrations time series\n",
    "fig, ax = plt.subplots()\n",
    "tslice = slice(0, 1000)\n",
    "tser_dense = np.arange(0.0, duration, deltat)\n",
    "for i in range(n_components):\n",
    "    ax.plot(tser_dense[tslice]/1000, mixed_concs_sample[tslice, i], lw=0.8, label=\"Odor {}\".format(i))\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Mixed odor concentrations\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background vectors time series with mixed concentrations\n",
    "tslice = slice(0, 50000, 100)\n",
    "n_cols = 6\n",
    "n_plots = n_dimensions // 4  # Only show first 24 OSNs\n",
    "n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_cols*1.75, n_rows*1.75)\n",
    "for i in range(n_plots):\n",
    "    ax = axes.flat[i]\n",
    "    ax.scatter(bkvecser_mixed[tslice, 2*i+1], bkvecser_mixed[tslice, 2*i], \n",
    "               s=9, alpha=0.5, color=\"k\")\n",
    "    for j in range(n_components):\n",
    "        ax.plot(*zip([0.0, 0.0], 3.0*back_components[j, 2*i:2*i+2:][::-1]), lw=2.0)\n",
    "    ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "for i in range(n_plots, n_rows*n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Matrix of third moments, to see how asymmetric components become\n",
    "# thus explaining the more difficult convergence to specificity\n",
    "mean_concs_num = np.mean(mixed_concs_sample, axis=0)  # unmixed\n",
    "conc_0mean = (mixed_concs_sample - mean_concs_num)\n",
    "thirdmoments = np.mean(conc_0mean[:, :, None, None] \n",
    "                       * conc_0mean[:, None, :, None] \n",
    "                       * conc_0mean[:, None, None, :], axis=0)\n",
    "\n",
    "# Check that the covmat is approx. what we wanted\n",
    "covnum = np.mean(conc_0mean[:, :, None] \n",
    "                       * conc_0mean[:, None, :], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "absrange = np.abs(covnum).max()\n",
    "linthresh = np.abs(covnum).min()\n",
    "#cmap_norm = SymLogNorm(linthresh=linthresh, vmin=-absrange, vmax=absrange)\n",
    "#cmap_norm = Normalize(vmin=-absrange, vmax=absrange)\n",
    "cmap_choice = \"viridis\"\n",
    "cmap_norm = mpl.colors.Normalize(vmin=min(0, np.amin(covnum)), vmax=np.amax(covnum))\n",
    "im = ax.imshow(covnum, norm=cmap_norm, cmap=cmap_choice)\n",
    "ax.set(xlabel=\"j\", ylabel=\"k\", title=r\"$C_{ij}$\")\n",
    "ax.set(xticks=range(0, n_components), yticks=range(0, n_components))\n",
    "cb = fig.colorbar(im, location=\"right\", orientation=\"vertical\", \n",
    "             ax=ax, label=\"Covariance matrix\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig, axes = plt.subplots(1, n_components, constrained_layout=True)\n",
    "absrange = np.abs(thirdmoments).max()\n",
    "linthresh = np.abs(thirdmoments).min()\n",
    "#cmap_norm = SymLogNorm(linthresh=linthresh, vmin=-absrange, vmax=absrange)\n",
    "#cmap_norm = Normalize(vmin=-absrange, vmax=absrange)\n",
    "#cmap_choice = \"RdBu\"\n",
    "cmap_choice = \"viridis\"\n",
    "cmap_norm = mpl.colors.Normalize(vmin=min(0, np.amin(thirdmoments)), vmax=np.amax(thirdmoments))\n",
    "for i in range(n_components):\n",
    "    ax = axes.flat[i]\n",
    "    im = ax.imshow(thirdmoments[i], norm=cmap_norm, cmap=cmap_choice)\n",
    "    ax.set_title(\"$\" + f\"C_{i}jk\" + \"$\")\n",
    "    ax.set(xticks=range(0, n_components), yticks=range(0, n_components))\n",
    "    ax.set(xlabel=\"j\", ylabel=\"k\")\n",
    "cb = fig.colorbar(im, location=\"bottom\", orientation=\"horizontal\", \n",
    "             ax=axes.flatten(), label=\"Third-order correlation\")\n",
    "#cb.set_ticks([-1e-2, -1e-3, -1e-4, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM habituation\n",
    "## IBCM parameters and initial values, same for all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.001 #5e-5\n",
    "tau_avg_ibcm = 1200  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.5*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])*lambd_ibcm\n",
    "#init_synapses_ibcm = (0.3 * back_components[rgen_meta.choice(n_components, size=n_i_ibcm), :]\n",
    "#                      + 0.1*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions]))* lambd_ibcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM simulation functions\n",
    "They rely on global parameters that won't change as we vary $\\rho$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_correl(rho, simseed, duration_local=duration, skp_local=skp):\n",
    "    # Make a copy of global parameters, change correlation rho\n",
    "    back_params_local = list(back_params)\n",
    "\n",
    "    # Target covariance matrix (scaled by variance of underlying independent variables)\n",
    "    target_cholesky = get_target_cholesky(rho, n_components)\n",
    "\n",
    "    # Add current Cholesky matrix to list of background params\n",
    "    back_params_local[-2] = target_cholesky\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back_list, \n",
    "                ibcm_rates, inhib_rates, back_params_local, duration_local, \n",
    "                deltat, seed=simseed, noisetype=\"uniform\",  \n",
    "                skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished simulation for rho =\", rho, \"in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Mixed concentrations time series\n",
    "    nuser_ibcm = sim_results[1][:, :, 1]\n",
    "    mixed_concs_ser = mix_concs(nuser_ibcm, mean_conc, target_cholesky)\n",
    "\n",
    "    return [*sim_results, mixed_concs_ser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_ibcm_simul(results_raw, correl_rho_loc):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm, mixed_concs_ser)\n",
    "    Returns:\n",
    "        mixed_concs_ser, cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm, mixed_concs_ser) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    basis = back_components  # Combine last two vectors for rho = 0\n",
    "    if abs(correl_rho_loc - 1.0) < 1e-6:  # effectively N-1 vectors only\n",
    "        print(\"Found rho = 1\")\n",
    "        basis = np.concatenate([back_components[:2], \n",
    "            np.sum(back_components[2:4], axis=0, keepdims=True)], axis=0)\n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_hgammas_hbargammas(\n",
    "                                results_raw[3], coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = mixed_concs_ser\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    split_val = 2.0\n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                          \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    results_clean = (conc_ser, cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "                     yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_ibcm_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save cbar gamma series, that's all we really need for the figures\n",
    "    # Will run a separate short, non-skipped simulation to plot mixed concentrations\n",
    "    cbars_gamma_series = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        try: \n",
    "            float(simname)\n",
    "        except:\n",
    "            fullname = simname  # mixed conc series\n",
    "            cbars_gamma = all_results_clean[simname]\n",
    "        else:\n",
    "            (conc_ser, cbars_gamma, wser_ibcm, bkvecser_ibcm, yser_ibcm, moments_conc, \n",
    "                cgammas_bar_counts, specif_gammas, correl_c_conc) = all_results_clean[simname]\n",
    "            fullname = \"cbars_gamma_ser_\" + simname\n",
    "        cbars_gamma_series[fullname] = cbars_gamma\n",
    "    np.savez_compressed(fname, **cbars_gamma_series)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM simulations\n",
    "Run simulations for $\\rho = -0.6, \\rho = 0.2, \\rho = 0.4, \\rho=0.7, \\rho = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_range = np.asarray([-0.6, 0.2, 0.4, 0.7, 1.0])\n",
    "tser_ibcm = np.arange(0.0, duration, deltat * skp)\n",
    "all_ibcm_results_clean = {}\n",
    "\n",
    "for rho in rho_range:\n",
    "    # Run and keep all in RAM for choice of plotting below\n",
    "    print(\"Running simulation for rho = {}\".format(rho))\n",
    "    raw_res = run_ibcm_simulation_correl(rho, simul_seed)\n",
    "    all_ibcm_results_clean[str(rho)] = analyze_clean_ibcm_simul(raw_res, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM analysis\n",
    "Plot cbar_gammas series for each rho, and plot the sample correlated concentrations time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    cbars_gamma = all_ibcm_results_clean[str(rho)][1]\n",
    "    fig , ax, _ = plot_hbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=5, transient=320000 // skp)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    if do_save_plots:\n",
    "        fig.savefig(pj(\"figures\", \"correlation\", \n",
    "            \"cbargammas_series_turbulent_correlation_{}.pdf\".format(str(rho).replace(\".\", \"-\"))), \n",
    "            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y series norm for each of these cases\n",
    "# Not very interesting. Rho = 0.7 is only slightly worse. \n",
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    yser_ibcm = all_ibcm_results_clean[str(rho)][4]\n",
    "    bkvecser_ibcm = all_ibcm_results_clean[str(rho)][3]\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 100000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    if do_save_plots:\n",
    "        fig.savefig(pj(\"figures\", \"correlation\", \n",
    "            \"pn_activity_norm_turbulent_correlation_ibcm_rho_{}.pdf\".format(str(rho).replace(\".\", \"-\"))),  \n",
    "            transparent=True, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu and cbarser, to see if some neurons are specific to odors\n",
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    cbar_nu_correl = all_ibcm_results_clean[str(rho)][8]\n",
    "    cbars_gamma_series = all_ibcm_results_clean[str(rho)][1]\n",
    "    specif_gammas = all_ibcm_results_clean[str(rho)][7]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(cbar_nu_correl.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    if do_save_plots:\n",
    "        fig.savefig(pj(\"figures\", \"correlation\", \n",
    "            \"specificities_turbulent_correlation_{}.pdf\".format(rho.replace(\".\", \"-\"))), \n",
    "            transparent=True, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    split_val = 3.5\n",
    "    n_comp_expected = n_components-1 if abs(rho - 1.0) < 1e-6 else n_components\n",
    "    for comp in range(n_comp_expected):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma_series[-2000:, :, comp], axis=0) > split_val)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    wser_ibcm = all_ibcm_results_clean[str(rho)][2]\n",
    "    fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPCA simulation\n",
    "\n",
    "## BioPCA parameters that don't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 8.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "rgen_pca = np.random.default_rng(seed=0x838b5119fbcfea9685dd64bd1d12d6cf)\n",
    "init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BioPCA simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation_correl(rho, simseed, duration_local=duration, skp_local=skp):\n",
    "    # Make a copy of global parameters, change correlation rho\n",
    "    back_params_local = list(back_params)\n",
    "\n",
    "    # Target covariance matrix (scaled by variance of underlying independent variables)\n",
    "    target_cholesky = get_target_cholesky(rho, n_components)\n",
    "\n",
    "    # Add current Cholesky matrix to list of background params\n",
    "    back_params_local[-2] = target_cholesky\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params_local, duration_local, deltat, \n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished simulation for rho =\", rho, \"in {:.2f} s\".format(tend - tstart))\n",
    "        \n",
    "    # Mixed concentrations time series\n",
    "    nuser_pca = sim_results[1]\n",
    "    mixed_concs_ser = (np.einsum(\"ij,kj->ki\", target_cholesky, \n",
    "                                nuser_pca[:, :, 1] - mean_conc) + mean_conc)\n",
    "\n",
    "    return [*sim_results, mixed_concs_ser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw, correl_rho_loc):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, cbarser_pca, wser_pca, yser_pca, mixed_concs_ser)\n",
    "    Returns:\n",
    "        mixed_concs_ser, bkvecser_pca, yser_pca, wser_pca,\n",
    "            true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, mser_pca, lser_pca, xser_pca, \n",
    "         cbarser_pca, wser_pca, yser_pca, mixed_concs_ser) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (mixed_concs_ser, bkvecser_pca, yser_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_biopca_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save true and learnt PCA, that's all we really need\n",
    "    true_learnt_pcas = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        (mixed_concs_ser, bkvecser_pca, yser_pca, wser_pca, true_pca, \n",
    "         learnt_pca, off_diag_l_avg_abs, align_error_ser) = all_results_clean[simname]\n",
    "        fullname = \"true_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = true_pca[0]\n",
    "        fullname = \"learnt_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = learnt_pca[0]\n",
    "        fullname = \"pca_align_error_\" + simname\n",
    "        true_learnt_pcas[fullname] = align_error_ser\n",
    "        print(learnt_pca[1].shape)\n",
    "    np.savez_compressed(fname, **true_learnt_pcas)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tser_biopca = np.arange(0.0, duration, deltat * skp)\n",
    "all_biopca_results_clean = {}\n",
    "\n",
    "for rho in rho_range:\n",
    "    # Run and keep all in RAM for choice of plotting below\n",
    "    print(\"Running simulation for rho = {}\".format(rho))\n",
    "    raw_res = run_biopca_simulation_correl(rho, simul_seed)\n",
    "    all_biopca_results_clean[str(rho)] = analyze_clean_biopca_simul(raw_res, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    # (mixed_concs_ser, bkvecser_pca, yser_pca, wser_pca\n",
    "    #   true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    true_pca, learnt_pca = all_biopca_results_clean[str(rho)][4:6]\n",
    "    align_error_ser = all_biopca_results_clean[str(rho)][7]\n",
    "    off_diag_l = all_biopca_results_clean[str(rho)][6]\n",
    "    fig, axes = plot_pca_results(tser_biopca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l)\n",
    "    axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 3*2.5)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot y series norm for each of these cases\n",
    "# Not very interesting, not consistent with level of convergence to true PCA. \n",
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    yser_biopca = all_biopca_results_clean[str(rho)][2]\n",
    "    bkvecser_biopca = all_biopca_results_clean[str(rho)][1]\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_biopca, bkvecser_biopca, yser_biopca, skp=1)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 100000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    if do_save_plots:\n",
    "        fig.savefig(pj(\"figures\", \"correlation\", \n",
    "            \"pn_activity_norm_turbulent_correlation_biopca_rho_{}.pdf\".format(str(rho).replace(\".\", \"-\"))),  \n",
    "            transparent=True, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rho in rho_range:\n",
    "    print(\"rho = {}\".format(rho))\n",
    "    wser_biopca = all_biopca_results_clean[str(rho)][3]\n",
    "    fig, axes = plot_w_matrix(tser_biopca, wser_biopca, skp=100)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_outputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_outputs:\n",
    "    save_folder = pj(\"..\", \"results\", \"for_plots\", \"correlation\")\n",
    "    save_ibcm_simuls_to_disk(pj(save_folder, \"ibcm_examples_turbulent_correl.npz\"), \n",
    "                             mixed_concs=mixed_concs_sample, **all_ibcm_results_clean)\n",
    "    save_biopca_simuls_to_disk(pj(save_folder, \"biopca_examples_turbulent_correl.npz\"), \n",
    "                               **all_biopca_results_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
