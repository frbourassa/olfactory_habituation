{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPCA habituation to turbulent backgrounds as a function of model rates\n",
    "\n",
    "For BioPCA, measure L2 distance between learned and true log-eigenvalues? Or alignment subspace error as usual? \n",
    "\n",
    "Things to check:\n",
    " - Convergence of BioPCA as as function of $\\mu$ and $\\mu_L$. Leave $\\Lambda$ matrix as it is, not the major driver of convergence (although it can help a little). \n",
    " - Convergence as a function of the number of odors, the strength of turbulence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse, special\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw, \n",
    "    generate_odorant\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"convergence\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"convergence\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background generation and initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combi(concs, backs):\n",
    "    \"\"\" concs: shaped [..., n_odors]\n",
    "        backs: 2D array, shaped [n_odors, n_osn]\n",
    "    \"\"\"\n",
    "    return concs.dot(backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global choice of background and odor mixing functions\n",
    "update_fct = update_powerlaw_times_concs\n",
    "combine_fct = linear_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will later explore the effect of varying these parameters on the convergence, \n",
    "# but put the default ones in a function\n",
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background initialization, given parameters and a seeded random generator\n",
    "def initialize_given_background(back_pms, rgen, n_comp, n_dim):\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-1], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    back_comps = back_pms[-1]\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps)\n",
    "    # background random variables are first in the list of initial values\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPCA simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of analyses to do on BioPCA results\n",
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    We do not need to save odor vectors (back_components), \n",
    "    since the IBCM simulation will provide them for both models. \n",
    "    \n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, hbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "            learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, mser_pca, lser_pca, xser_pca, \n",
    "         hbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    \n",
    "    # Sum of squared differences between learnt and true eigenvalues in log scale\n",
    "    transient = int(3 * tser_pca.shape[0] // 4)\n",
    "    avg_learnt = learnt_pca[0][transient:].mean(axis=0)\n",
    "    n_comp = nuser_pca.shape[1]\n",
    "    eigenvals_diff_log = (np.log10(true_pca[0][:n_comp]) - np.log10(avg_learnt[:n_comp]))**2\n",
    "    \n",
    "    eigenvals_vari = np.var(learnt_pca[0][transient:, :n_comp], axis=0, ddof=1)\n",
    "    \n",
    "    results_clean = (true_pca, learnt_pca, off_diag_l_avg_abs, \n",
    "                     align_error_ser, eigenvals_diff_log, eigenvals_vari)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analyze_biopca_one_back_seed(\n",
    "        biopca_rates_loc, back_rates, inhib_rates_loc, \n",
    "        options_loc, dimensions, seedseq, \n",
    "        duration_loc=360000.0, dt_loc=1.0, skp_loc=20, full_returns=False\n",
    "    ):\n",
    "    \"\"\" Given BioPCA model rates and background parameters except\n",
    "    background odors (but incl. number odors and c0), and a main seed sequence, \n",
    "    run and analyze convergence of BioPCA on the background generated from that seed. \n",
    "    The seedseq should itself have been spawned from a root seed to have a distinct\n",
    "    one per run; this still makes seeds reproducible yet distinct for different runs. \n",
    "    The seedseq here is spawned again for a background gen. seed and a simul. seed. \n",
    "    \n",
    "    Args:\n",
    "        dimensions: gives [n_components, n_dimensions, n_i_ibcm]\n",
    "    \n",
    "    Returns:\n",
    "        iff full_return:\n",
    "            gaps, specifs, hgamvari, hgammas_ser, sim_results\n",
    "        else:\n",
    "            gaps, specifs, hgamvari, None, None\n",
    "        alignment_gaps: indexed [neuron]\n",
    "        specif_gammas: indexed [neuron]\n",
    "        gamma_vari: indexed [neuron, component]\n",
    "    \"\"\"\n",
    "    #print(\"Initializing IBCM simulation...\")\n",
    "    # Get dimensions\n",
    "    n_comp, n_dim, n_i_pca = dimensions\n",
    "    \n",
    "    # Spawn back. generation seed and simul seed\n",
    "    initseed, simseed = seedseq.spawn(2)\n",
    "    \n",
    "    # Duplicate back params before appending locally-generated odor vectors to them\n",
    "    back_pms_loc = list(back_rates)\n",
    "    \n",
    "    # Create background\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    back_comps_loc = generate_odorant((n_comp, n_dim), rgen_init)\n",
    "    back_comps_loc = back_comps_loc / l2_norm(back_comps_loc, axis=1)[:, None]\n",
    "\n",
    "    # Add odors to the list of background parameters\n",
    "    back_pms_loc.append(back_comps_loc)\n",
    "\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    init_back = initialize_given_background(back_pms_loc, rgen_init, n_comp, n_dim)\n",
    "\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_pca = rgen_init.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen_init.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the BioPCA simulation\n",
    "    print(\"Running BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back, \n",
    "                biopca_rates_loc, inhib_rates_loc, back_pms_loc,\n",
    "                duration_loc, dt_loc, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_loc, **options_loc\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Now analyze BioPCA simul for convergence\n",
    "    print(\"Starting to analyze BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    results_clean = analyze_clean_biopca_simul(sim_results)\n",
    "    # TODO: compute additional convergence metrics\n",
    "    true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser, pc_diff, pc_vari = results_clean\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished analyzing BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Doesn't return full series, only the summary statistics of convergence\n",
    "    analysis_results_ret = (true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser, pc_diff, pc_vari)\n",
    "    if full_returns:\n",
    "        sim_results_ret = sim_results\n",
    "    else:\n",
    "        sim_results_ret = None\n",
    "    \n",
    "    return analysis_results_ret, sim_results_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy a simplification for once: we do not need to consider other models like average subtraction, optimal $P$, orthogonal projection since all we care about in this notebook is the convergence of the two biologically plausible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_biopca_results(res_biopca_clean, res_biopca_raw, skp_loc=10):\n",
    "    # Extract individual arrays from the lists of results\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, mser_pca, lser_pca, xser_pca, \n",
    "         hbarser_pca, wser_pca, yser_pca) = res_biopca_raw\n",
    "    (true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser, pca_diff, pca_vari) = res_biopca_clean\n",
    "    \n",
    "    print(\"log-scale squared distance in principal values learned vs true:\", pca_diff)\n",
    "\n",
    "    # Plot learnt vs true PCA\n",
    "    tser_scaled = tser_common * dtscale\n",
    "    fig, axes = plot_pca_results(tser_scaled, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "    axes[-1].set_xlabel(\"Time (min)\")\n",
    "    axes[0].get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 2.5*plt.rcParams[\"figure.figsize\"][1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot level of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_scaled*1000, bkvecser_pca, yser_pca, skp=2)\n",
    "    ax.set_xlabel(\"Time (min)\")\n",
    "    \n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp_loc\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPCA simulation with proper rates \n",
    "To illustrate convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for all simulations\n",
    "# Dimensions: 25 is enough?\n",
    "n_dimensions = 25\n",
    "n_components = 3  # try with 3 for simplicity by default\n",
    "\n",
    "# Inhibition W learning and decay rates\n",
    "inhib_rates_default = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration and integration time step\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Saving every skp simulation point, 50 is enough for plots, \n",
    "# here use 20 to get convergence time accurately\n",
    "skp_default = 20 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp_default)\n",
    "dtscale = 10.0 / 1000.0 / 60.0  # to convert time steps units to minutes\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 2.5e-5  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.3\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 3.0\n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = 1e-4  #learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates_default = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "meta_seedseq = np.random.SeedSequence(0x6225b86e0b4a826a8845d573f634c7d9)\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "back_rates_default = default_background_params(n_components)\n",
    "# Try changing background rates here if desired\n",
    "# Concentration scale c0: multiply up-down to change convergence dynamics\n",
    "# just as well as the learning rate, although with a different scaling. \n",
    "back_rates_default[4][:] = 0.6\n",
    "back_rates_default[1][:] = 500.0  # whiff duration\n",
    "back_rates_default[3][:] = 800.0  # blank duration\n",
    "dimensions_biopca = [n_components, n_dimensions, n_i_pca]\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "lysis_res, sim_res = run_analyze_biopca_one_back_seed(biopca_rates_default, back_rates_default, \n",
    "                        inhib_rates_default, pca_options, dimensions_biopca, meta_seedseq,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "print(\"True eigenvalues:\", lysis_res[0][0][:3])\n",
    "print(\"L_ii standard dev., scaled by true eigenvalues (so, CV):\", \n",
    "      np.sqrt(lysis_res[5]) / lysis_res[0][0][:3])\n",
    "plot_biopca_results(lysis_res, sim_res, skp_loc=skp_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_i_pca)\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1]*3)\n",
    "tser_scaled = tser_common * dtscale\n",
    "mser_biopca = sim_res[3]\n",
    "lser_biopca = sim_res[4]\n",
    "\n",
    "tsl = slice(tser_scaled.shape[0]//3, None, 4)\n",
    "for i in range(n_i_pca):\n",
    "    for j in range(n_dimensions):\n",
    "        axes[i].plot(tser_scaled[tsl], mser_biopca[tsl, i, j])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "Recall the BioPCA equations, written in my notation:\n",
    "\n",
    "$$ \\frac{\\mathrm{d} M}{\\mathrm{d} t} = \\mu (\\mathbf{h} \\mathbf{s}^T - M) $$\n",
    "\n",
    "$$ \\frac{\\mathrm{d} L'}{\\mathrm{d} t} = \\mu_L \\mu (\\mathbf{h} \\mathbf{h}^T - \\underline{\\underline{\\Lambda}} L' \\underline{\\underline{\\Lambda}} ) $$\n",
    "\n",
    "$$ \\mathbf{h} = LM \\mathbf{s} \\quad, \\quad L' = L^{-1} \\quad, \\quad \\mu_L \\approx 2$$\n",
    "\n",
    "My usual initial conditions were $L_{ii} = 1$: much too large compared to typical principal values in the backgrounds I consider. Then, if $L$ is too large, $L'$ is too small: $\\mathbf{h}$ amplitude is suppressed, so we essentially have exponential decay, $ \\frac{\\mathrm{d} L'}{\\mathrm{d} t} \\sim - \\underline{\\underline{\\Lambda}} L' \\underline{\\underline{\\Lambda}}$, of the eigenvalues $L'$ (increase of $L$). So that's where the linear behavior in log scale comes from. The decay rate is $\\mu \\Lambda_i^2$: each eigenvalue gets a different decay rate, so they reach different eigenvalues at different times and break fixed point degeneracy. Fastest decaying (largest $\\Lambda_{ii}$ = first entry of the matrix) stops to decay at largest $L'$, and so on.  \n",
    "\n",
    "For $M$, due to small $h$, we initially get exponential decay. Then, once $L'$ is small enough ($L$ large enough, $h$ larger), we have an increase of $M$ until stabilization to the fixed point\n",
    "\n",
    "Note that by increasing the scale of $\\Lambda$, we increase the $L'$ decay rate: we should probably compensate by decreasing $\\mu_L$ if $L'$ entries are found to shoot past the true eigenvalues. We want to keep $\\Lambda^2 \\mu_L > 2$ to ensure convergence.\n",
    "\n",
    "Note that if $M$ has too much time to decay, $h$ stays too small and $L'$ keeps decaying and overshoots the eigenvalue. So this is also why we want $\\mu_L$ relatively large. It's a good idea, in fact, to keep $\\mu_L = 2$ even if the $\\Lambda$ scale is increased. \n",
    "\n",
    "\n",
    "For $L'_{ii}$ much too small initially ($L$ large), we would have $h$ very large, so $L'$ increses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
