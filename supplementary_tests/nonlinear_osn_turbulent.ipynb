{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbulent backgrounds with nonlinear OSN model\n",
    "\n",
    "We use the model from Kadakia and Emonet, because OSN adaptation is easy to include into it afterwards. \n",
    "\n",
    "The olfactory receptors (OR) have Orco co-receptors. Each OR-Orco complex has an active and an inactive state, with affinities $K^*_{i \\mu}$, $K_{i \\mu}$ respectively for odor $\\mu$, receptor index $i$. Given odor concentrations $c_\\mu$, the quasi-static OSN firing rate in response is\n",
    "\n",
    "$$ A_i = \\left[1 + \\mathrm{exp} \\left( \\epsilon_i(t) + \\ln \\left( \\frac{1 + \\sum_\\mu K_{i \\mu} c_\\mu }{1 + \\sum_\\mu K^*_{i \\mu} c_\\mu }  \\right) \\right) \\right]^{-1} $$\n",
    "\n",
    "where $\\epsilon_i(t)$ is the free energy difference between the unbound states in the inactive and active conformations for OR type $i$, in units of $k_\\mathrm{B} T$. This free energy difference changes with feedback from OSN activity, with an adaptation time scale of $250$ ms, but here, we will keep $\\epsilon_i$ fixed; the point of the model is to have a nonlinear receptor activation with easy inclusion of adaptation later. \n",
    "\n",
    "The affinities $K^*_{i \\mu}$ and $K_{i \\mu}$ are sampled i.i.d. from a power-law with exponent $\\alpha = 0.35$ based on Si et al., 2019. We then ensure that $K^* > K$ (no antagonists) and select a few OSNs per odor to be strongly activated (breaking the power law). \n",
    "\n",
    "Note that the power law distribution is over a finite range, \n",
    "\n",
    "$$ f_K(k) = \\alpha k^{-1-\\alpha} \\,\\, \\mathrm{if} k \\in [k_\\mathrm{low}, k_\\mathrm{hi}], \\, 0 \\, \\mathrm{else}$$\n",
    "\n",
    "and $\\alpha > 0$ implies its normalization would blow up at zero, hence there needs to be a cutoff $k_\\mathrm{low} > 0$. \n",
    "\n",
    "The free energy default value is $5.0$ in Kadakia and Emonet's code, without variance between odors based on the parameters coded in the Github repo. \n",
    "\n",
    "### Linear regime for small concentrations\n",
    "If we assume concentrations are small, we can Taylor expand $A_i$ near $c=0$. We find that\n",
    "\n",
    "$$ A_i \\approx \\frac{1}{1 + e^{\\epsilon_i(t)}} + \\frac{e^{\\epsilon_i(t)}}{(1 + e^{\\epsilon_i(t)})^2} \\sum_{\\mu} (K_{i \\mu}^* - K_{i \\mu}) \\, c_{\\mu}(t) \\,\\, .  $$\n",
    "\n",
    "So, with a large free energy difference to suppress the constant term, this expansion basically reduces to our linear mixture model with odor vectors\n",
    "\n",
    "$$ \\mathbf{s}_{\\gamma} = \\frac{e^{\\epsilon_i(t)}}{(1 + e^{\\epsilon_i(t)})^2} \\left( \\mathbf{K}^*_{\\gamma} - \\mathbf{K}_{\\gamma} \\right) \\propto \\mathbf{K}^*_{\\gamma} - \\mathbf{K}_{\\gamma} $$\n",
    "\n",
    "Hence, we are looking at a nonlinear manifold with underlying linear odor vectors whose elements are drawn from a power-law. \n",
    "\n",
    "\n",
    "### Possible simplifications\n",
    "\n",
    "Possibly, we could use $\\mathbf{K}^*$ drawn from our usual exponential distribution and very small $\\mathbf{K}_{i \\mu} = 0.01$, so this whole model would reduce to a mildly nonlinear version of our usual manifolds? \n",
    "\n",
    "Or we could simplify even further to keep things conceptual, and use a mild Michaelis-Menten saturation function for OSNs, with a MM constant that adapts like the free energy difference of Kadakia and Emonet? That would result in\n",
    "\n",
    "$$ A_i = \\frac{\\sum_{\\gamma} c_{\\gamma} s_{i \\gamma}}{e^{\\epsilon_i(t)} + \\sum_{\\gamma} c_{\\gamma} s_{i \\gamma}} $$\n",
    "\n",
    "Notice that we can recover exactly this form in the regime where $c$ is not too small, $K \\ll K^*$, so $\\mathbf{K}^*_i \\cdot \\mathbf{c} \\gg 1$ but $\\mathbf{K}_i \\cdot \\mathbf{c} \\ll 1$, and we have\n",
    "\n",
    "$$ A_i \\approx \\frac{1}{1 + e^{\\epsilon_i(t)} / \\mathbf{K^*}_i \\cdot \\mathbf{c}} \n",
    "    = \\frac{\\sum_\\mu  \\mathbf{K}^*_{i \\mu} \\cdot \\mathbf{c}}{e^{\\epsilon_i(t)} + \\mathbf{K^*}_i \\cdot \\mathbf{c}}  $$\n",
    "\n",
    "which is the Michaelis-Menten for that we wanted with odor vector $\\mathbf{K}^*_\\mu$ for each odor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Looking at the code from Kadakia and Emonet, \n",
    "https://github.com/elifesciences-publications/ORN-WL-gain-control/blob/master/src/four_state_receptor_CS.py\n",
    "the $K$ parameters are all equal to $1/10^2 = 0.01$ exactly, while the $1/K^*$ power law ranges from $10^{-4}$ to $10^{-3}$, such that $K^*$ ranges from $1000$ to $10\\, 000$\n",
    "Also, they state in the text that $\\alpha = 0.35$, but they actually use $0.38$ in the code. Meanwhile, the original paper reports of fit of $\\alpha = 0.42$. I will use $0.38$ since it is in between. \n",
    "\n",
    "Then, a few OSNs are randomly selected for each odor to have a larger $K^*$? I will first try without that, it is a bit complicated in Kadakia, they look for OSNs which are weak responders to all odors (average response to all odors at standard conc. and energy difference below $0.03$ in the panel of odors, and make them very responsive to one of the odors, by randomly setting their inverse $K^*$ log-uniform between $10^{-4.5}$ and $10^{-3}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor, \n",
    "    compute_optimal_matrices\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    sample_ss_conc_powerlaw,\n",
    "    update_tc_odor,\n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform, \n",
    "    power_range_inverse_transform\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main new function: background update with OSN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_odor_aff_power(n_rec, rgen, k1val=0.01, \n",
    "            k2inv_bounds=(1e-4, 1e-3), alpha=0.35, unit_scale=0.05):\n",
    "    \"\"\" Generate vectors eta and kappa^-1 for an odorant, with antagonism parameter rho.\n",
    "    Following Kadakia and Emonet, eLife, 2019. \n",
    "    \n",
    "    Our unit concentrations are normalized, so we need to normalize affinities\n",
    "    as well (the values used by Kadakia and Emonet are EC50 units). \n",
    "    To stay in a nearly linear regime where OSNs do not saturate\n",
    "    we use a factor of 0.01. \n",
    "    \n",
    "    Args:\n",
    "        n_rec (int): number of receptor types, length of vectors\n",
    "        rgen (np.random.Generator): random generator (numpy >= 1.17)\n",
    "        k1val (float): value of all K affinities\n",
    "        k2inv_bounds (tuple of 2 floats): lowest and highest 1/K* possible\n",
    "        alpha (float): power law has CDF with exponent alpha, \n",
    "            PDF with exponent alpha-1\n",
    "    \n",
    "    Returns:\n",
    "        k1vec (np.ndarray): 1d vector of inactive complex binding affinities\n",
    "        k2vec (np.ndarray): 1d vector of active complex binding affinities\n",
    "    \"\"\"\n",
    "    # Generate inverse K: all 1/K equal to 100\n",
    "    k1vec = np.full(n_rec, 0.01 * unit_scale)\n",
    "    \n",
    "    # and inverse K*: 1/K* follows power law between k2inv_bounds. \n",
    "    # First sample power law elements x in [0, 1], f_X(x) = ax^{a-1}\n",
    "    # Scipy transforms 0 < x < 1 power-law as 1/K = k_lo + k_hi*x \n",
    "    # this seems to be how scipy shifts the powerlaw and what is done in Kadakia and Emonet\n",
    "    #x = rgen.power(a=alpha, size=n_rec)\n",
    "    #k2inv = x * k2inv_bounds[1] + k2inv_bounds[0]\n",
    "    # Same result would be obtained from scipy\n",
    "    #k2inv = powerlaw.rvs(alpha, loc=k2inv_bounds[0], scale=k2inv_bounds[1], \n",
    "    #                     size=n_rec, random_state=rgen)\n",
    "    \n",
    "    # The above are within [k_lo, k_hi] indeed, but 1/K doesn't itself follow a power-law \n",
    "    # like A*(1/K)^{a-1}, so we implement our own proper power-law with a cutoff:\n",
    "    r = rgen.random(size=n_rec)\n",
    "    k2inv = power_range_inverse_transform(r, *k2inv_bounds, alpha)\n",
    "    \n",
    "    k2vec = unit_scale / k2inv\n",
    "    return k1vec, k2vec\n",
    "\n",
    "\n",
    "# To generate several odors and format parameters the right way\n",
    "def generate_background_aff_power(n_comp, n_rec, rgen, \n",
    "        k1val=0.01, k2inv_bounds=(1e-4, 1e-3), alpha=0.38):\n",
    "    k1mat, k2mat = [], []\n",
    "    for i in range(n_comp):\n",
    "        k1, k2 = generate_odor_aff_power(n_dimensions, rgen_meta, \n",
    "                    k1val=k1val, k2inv_bounds=k2inv_bounds, alpha=alpha)\n",
    "        k1mat.append(k1)\n",
    "        k2mat.append(k2)\n",
    "    k1mat = np.stack(k1mat, axis=0)\n",
    "    k2mat = np.stack(k2mat, axis=0)\n",
    "    kbothmat = np.stack([k1mat, k2mat], axis=0)\n",
    "    return kbothmat\n",
    "\n",
    "\n",
    "def inverse_transform_tanhcdf(r, logb, alpha):\n",
    "    return (10.0**logb * np.arctanh(r))**(-1.0 / alpha)\n",
    "\n",
    "\n",
    "def generate_odor_true_si2019(n_rec, rgen, k1val=0.01, \n",
    "            alpha=0.49, logb=-1.99, unit_scale=1e-3):\n",
    "    k1vec = np.full(n_rec, k1val * unit_scale)\n",
    "    \n",
    "    r = rgen.random(size=n_rec)\n",
    "    k2vec = inverse_transform_tanhcdf(r, logb, alpha) * unit_scale\n",
    "    k2vec = np.clip(k2vec, 0.0, 1e3)\n",
    "    \n",
    "    return k1vec, k2vec\n",
    "\n",
    "def combine_odors_adapt(concs, k1mat, k2mat, epsils, fmax=1.0):\n",
    "    \"\"\" Combine odors coming in with concentrations conc and defined\n",
    "    by active and inactive binding affinities kappa1, kappa2. \n",
    "    OSN types have free energy differences epsils. \n",
    "\n",
    "    Args:\n",
    "        concs (np.ndarray): 1d array of odor concentrations, indexed [n_odors]\n",
    "        kappa1 (np.ndarray): shape [n_odors, n_osns], affinity of active complexes\n",
    "        kappa2 (np.ndarray):  shape [n_odors, n_osns], affinity of inactive complexes\n",
    "        epsils (np.ndarray): shape [n_osns], free energy difference of each OSN type\n",
    "        fmax (float): maximum amplitude, default 1\n",
    "\n",
    "    Returns:\n",
    "        activ (np.ndarray): 1d array of ORN activation, indexed [n_receptors]\n",
    "    \"\"\"\n",
    "    # Dot products over odors\n",
    "    kc1 = concs.dot(k1mat)\n",
    "    kc2 = concs.dot(k2mat)\n",
    "    logterm = (1.0 + kc1) / (1.0 + kc2)\n",
    "    activs = fmax / (1.0 + np.exp(epsils) * logterm)\n",
    "    return activs\n",
    "\n",
    "def combine_odors_compet(concs, kmat, epsils, fmax=1.0):\n",
    "    r\"\"\" Combine odors coming in with concentrations conc and defined\n",
    "    by binding affinities kmat. OSN types have free energy differences epsils. \n",
    "    Approximation of the model by Kadakia and Emonet when K^* >> K, reduces\n",
    "    to a Michaelis-Menten function of $\\sum_\\mu K^*_{i \\mu} c_\\mu$\n",
    "    with potentially adaptable Michaelis-Menten constant $e^{\\epsilon_i(t)}$\n",
    "\n",
    "    Args:\n",
    "        concs (np.ndarray): 1d array of odor concentrations, indexed [n_odors]\n",
    "        kmat (np.ndarray):  shape [n_odors, n_osns], affinity of active complexes\n",
    "        epsils (np.ndarray): shape [n_osns], free energy difference of each OSN type\n",
    "        fmax (float): maximum amplitude, default 1\n",
    "\n",
    "    Returns:\n",
    "        activ (np.ndarray): 1d array of ORN activation, indexed [n_receptors]\n",
    "    \"\"\"\n",
    "    # Dot products over odors\n",
    "    kc = concs.dot(kmat)\n",
    "    activs = fmax * kc / (np.exp(epsils) + kc)\n",
    "    return activs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the background by combining odorant at the current concentrations with the OSN model\n",
    "def update_powerlaw_times_concs_aff(tc_bk, params_bk, noises, dt):\n",
    "    \"\"\"\n",
    "    Simulate turbulent odors by pulling wait times until the end of a whiff\n",
    "        or until the next blank, and a concentration of the whiff.\n",
    "        For each odor, check whether the time left until switch is <= zero;\n",
    "        if so, pull either\n",
    "            - another wait time t_w if current c=0, and pull the new c > 0\n",
    "              (we were in a blank and are starting a whiff)\n",
    "            - another wait time t_b if current c > 0, and set c = 0\n",
    "              (we were in a whiff and are starting a blank)\n",
    "        Otherwise, decrement t by dt and don't change c.\n",
    "\n",
    "    Args:\n",
    "        tc_bk (np.ndarray): array of t, c for each odor in the background,\n",
    "            where t = time left until next change, c = current concentration\n",
    "            of the odor. Shaped [n_odors, 2]\n",
    "        params_bk (list): contains the following elements (a lot needed!):\n",
    "            whiff_tmins (np.ndarray): lower cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            whiff_tmaxs (np.ndarray): upper cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            blank_tmins (np.ndarray): same as whiff_tmins but for blanks\n",
    "            blank_tmaxs (np.ndarray): same as whiff_tmaxs but for blanks\n",
    "            c0s (np.ndarray): c0 concentration scale for each odor\n",
    "            alphas (np.ndarray): alpha*c0 is the lower cutoff of p_c\n",
    "            vecs (np.ndarray): 3d array where axis 0 has length 2, \n",
    "                the first sub-array giving the K vectors of each odor, \n",
    "                and the second, giving K* vectors. \n",
    "            epsils (np.ndarray): free energy difference of each OSN type, \n",
    "                shaped [n_osn]. \n",
    "            fmax (float): maximum OSN activation amplitude (normalization)\n",
    "        noises (np.ndarray): fresh U(0, 1) samples, shaped [n_odors, 2],\n",
    "            in case we need to pull a new t and/or c.\n",
    "        dt (float): time step duration, in simulation units\n",
    "    \"\"\"\n",
    "    # Update one odor's t and c at a time, if necessary\n",
    "    tc_bk_new = np.zeros(tc_bk.shape)\n",
    "    for i in range(tc_bk.shape[0]):\n",
    "        tc_bk_new[i] = update_tc_odor(tc_bk[i], dt, noises[i],\n",
    "                                *[p[i] for p in params_bk[:-3]])\n",
    "\n",
    "    # Compute backgound vector (even if it didn't change)\n",
    "    k1vecs, k2vecs = params_bk[-3]  # k, k*\n",
    "    epsils = params_bk[-2]\n",
    "    fmax = params_bk[-1]\n",
    "    new_bk_vec = combine_odors_adapt(tc_bk_new[:, 1], k1vecs, k2vecs, epsils, fmax=fmax)\n",
    "    return new_bk_vec, tc_bk_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)\n",
    "do_save_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Half the real number for faster simulations\n",
    "n_components = 3  # Number of background odors\n",
    "\n",
    "inhib_rates = [0.0001, 0.00002]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 10\n",
    "skp = 20 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x220369e90599ffa80a743d99ac942f28)\n",
    "#rgen_meta = np.random.default_rng(seed=0x207839aa985b66f211ebb049ecc23647)\n",
    "#rgen_meta = np.random.default_rng(seed=0x8fec8e034ed20f055ae9f5e0321aaee)\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_times_concs_aff\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "\n",
    "# Background odors: parameters (K, K^*) and epsils\n",
    "# K array in back_components is ndexed [kappa_or_eta, n_odors, n_dimensions]\n",
    "#back_components = generate_background_aff_power(n_components, n_dimensions, rgen_meta)\n",
    "#back_components = [generate_odorant((n_components, n_dimensions), rgen_meta)*0.01, \n",
    "#                  generate_odorant((n_components, n_dimensions), rgen_meta)*50.0]\n",
    "back_components = generate_odor_true_si2019((n_components, n_dimensions), rgen_meta)\n",
    "epsils_vec = np.full(n_dimensions, 5.0)\n",
    "back_params.append(back_components)\n",
    "back_params.append(epsils_vec)\n",
    "\n",
    "# To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "max_osn_ampli = 3.0 / np.sqrt(n_dimensions)\n",
    "back_params.append(max_osn_ampli)\n",
    "\n",
    "# In the small conc. approx, the odor vectors are (K^* - K)/2\n",
    "s_gamma_vecs = back_components[1] - back_components[0]\n",
    "s_gamma_vecs = s_gamma_vecs / l2_norm(s_gamma_vecs, axis=1)[:, None]\n",
    "\n",
    "# Initial values of background process variables (t, c for each variable)\n",
    "init_concs = sample_ss_conc_powerlaw(*back_params[:-3], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components), *back_params[2:4])\n",
    "tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector: combine odors with the tc_init concentrations\n",
    "init_bkvec = combine_odors_adapt(tc_init[:, 1], \n",
    "                back_components[0], back_components[1], epsils_vec, fmax=max_osn_ampli)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check OSN activation distribution is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = n_dimensions * int(1e5)\n",
    "bunch_values1, bunch_values2 = generate_odor_aff_power(n_samples, rgen_meta)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hallem = pd.read_excel(os.path.join(\"..\", \"data\", \"hallem2006_TableS1_source.xlsx\"))\n",
    "hallem = hallem.set_index(\"Odorant\")\n",
    "fig, ax = plt.subplots()\n",
    "hist, edges = np.histogram(hallem.values.flatten(), bins=50, density=True)\n",
    "ax.bar(edges[:-1], hist, align=\"edge\", width=np.diff(edges))\n",
    "ax.set(yscale=\"log\", xlabel=\"OR-Orco active affinity\", ylabel=\"Prob. density\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "kd2 = bunch_values2\n",
    "counts, binseps = np.histogram(np.log10(kd2), bins=\"doane\")\n",
    "binwidths = np.diff(10**binseps)\n",
    "# Center of bins on a log scale, but given in linear coordinates\n",
    "bin_centers_forlog = 10**((binseps[1:] + binseps[:-1])/2)\n",
    "pdf = counts / binwidths / kd2.size\n",
    "ax.bar(x=10**binseps[:-1], align=\"edge\", height=pdf, width=binwidths)\n",
    "ax.set(yscale=\"log\", xscale=\"log\", xlabel=r\"OR-Orco active affinity $K^*$\", ylabel=\"Prob. density\")\n",
    "ax.set(yscale=\"log\", xscale=\"log\", xlabel=r\"OR-Orco active affinity $K^*$\", ylabel=\"1-CDF\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odor_vecs = (bunch_values2 - bunch_values1).reshape(-1, n_dimensions)\n",
    "averages = []\n",
    "# Average dot product between them?\n",
    "chunksize = int(2e3)\n",
    "nchunks = odor_vecs.shape[0] // chunksize\n",
    "for i in range(nchunks):\n",
    "    odors_loc = odor_vecs[i*chunksize:(i+1)*chunksize]\n",
    "    norms_loc = l2_norm(odors_loc, axis=1)\n",
    "    dots = odors_loc.dot(odors_loc.T) / np.outer(norms_loc, norms_loc)\n",
    "    dots[np.diag_indices(dots.shape[0])] = np.nan\n",
    "    averages.append(np.nanmean(dots))  # ignore diagonal\n",
    "averages = np.asarray(averages)\n",
    "print(\"Average cosine similarity:\", np.mean(averages))\n",
    "print(\"With bootstrap std:\", np.std(averages, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a heatmap of affinity matrices\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(4, 1)\n",
    "ax = fig.add_subplot(gs[:3])\n",
    "cax = fig.add_subplot(gs[3:])\n",
    "im = ax.imshow(np.log10(back_components[1]), origin=\"upper\", aspect=\"auto\")\n",
    "ax.set_xlabel(\"OR type\")\n",
    "ax.set_ylabel(\"Odor\")\n",
    "fig.colorbar(im, cax=cax, orientation=\"horizontal\", label=\"OR affinity\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation\n",
    "\n",
    "The good question is: what are IBCM neurons going to learn? The input space is $\\vec{x}$, but it's not a linear combination of background odors, which are rather defined by $\\vec{\\eta}$ and $\\vec{\\kappa}$. So it's unclear even what exactly will be the input process distribution, what components will be learnt, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 12  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.001  #5e-5\n",
    "tau_avg_ibcm = 1200  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.3*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])*lambd_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "# Perform successive shorter runs/restarts for memory efficiency\n",
    "tser_ibcm = []\n",
    "nuser_ibcm = []\n",
    "bkvecser_ibcm = []\n",
    "mser_ibcm = []\n",
    "cbarser_ibcm = []\n",
    "wser_ibcm = []\n",
    "yser_ibcm = []\n",
    "thetaser_ibcm = []\n",
    "if n_chunks > 1:\n",
    "    seed_spawns = np.random.SeedSequence(simul_seed).spawn(10)\n",
    "else:\n",
    "    seed_spawns = [simul_seed]\n",
    "for i in range(n_chunks):\n",
    "    tstart = perf_counter()\n",
    "    if i == 0:\n",
    "        init_vari = init_synapses_ibcm\n",
    "        init_back = init_back_list\n",
    "    else:\n",
    "        init_vari = [mser_ibcm[i-1][-1], thetaser_ibcm[i-1][-1], wser_ibcm[i-1][-1]]\n",
    "        init_back = [nuser_ibcm[i-1][-1], bkvecser_ibcm[i-1][-1]]\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_vari, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params, duration/n_chunks, \n",
    "                deltat, seed=seed_spawns[i], noisetype=\"uniform\",  \n",
    "                skp=skp, **ibcm_options\n",
    "    )\n",
    "    tser_ibcm.append(sim_results[0] + i/n_chunks*duration)\n",
    "    nuser_ibcm.append(sim_results[1])\n",
    "    bkvecser_ibcm.append(sim_results[2])\n",
    "    mser_ibcm.append(sim_results[3]) \n",
    "    cbarser_ibcm.append(sim_results[4]) \n",
    "    thetaser_ibcm.append(sim_results[5])\n",
    "    wser_ibcm.append(sim_results[6])\n",
    "    yser_ibcm.append(sim_results[7])\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished chunk\", i, \"in {:.2f} s\".format(tend - tstart))\n",
    "\n",
    "# Concatenate\n",
    "tser_ibcm = np.concatenate(tser_ibcm, axis=0)\n",
    "nuser_ibcm = np.concatenate(nuser_ibcm)\n",
    "bkvecser_ibcm = np.concatenate(bkvecser_ibcm)\n",
    "mser_ibcm = np.concatenate(mser_ibcm)\n",
    "cbarser_ibcm = np.concatenate(cbarser_ibcm)\n",
    "thetaser_ibcm = np.concatenate(thetaser_ibcm)\n",
    "wser_ibcm = np.concatenate(wser_ibcm)\n",
    "yser_ibcm = np.concatenate(yser_ibcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background process plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_dimensions <= 2:\n",
    "    fig, ax = plt.subplots()\n",
    "    # Use a different color for 3 cases: either odor absent, both odors\n",
    "    where_12 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    where_both = np.logical_and(where_12[:, 0], where_12[:, 1])\n",
    "    where_1 = np.logical_and(where_12[:, 0], ~where_12[:, 1])\n",
    "    where_2 = np.logical_and(~where_12[:, 0], where_12[:, 1])\n",
    "    ax.scatter(bkvecser_ibcm[where_both, 0], bkvecser_ibcm[where_both, 1], label=\"Both odors\")\n",
    "    ax.scatter(bkvecser_ibcm[where_1, 0], bkvecser_ibcm[where_1, 1], label=\"Odor 0 only\")\n",
    "    ax.scatter(bkvecser_ibcm[where_2, 0], bkvecser_ibcm[where_2, 1], label=\"Odor 1 only\")\n",
    "    vecs = np.zeros(s_gamma_vecs.shape)\n",
    "    scale = 0.5\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = s_gamma_vecs[i] / np.sqrt(np.sum(s_gamma_vecs[i]**2)) * scale\n",
    "        ax.annotate(\"\", xytext=(0, 0), xy=s_gamma_vecs[i], \n",
    "                    arrowprops=dict(width=2.0, color=\"k\"))\n",
    "    figname = \"background_two_odors_2d_affinity_model.pdf\"\n",
    "    zlbl = None\n",
    "elif n_components >= 3:\n",
    "    dims = (1, 2, 4)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    fig.set_size_inches(6.0, 3.0)\n",
    "    where_123 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    locations = {\n",
    "        \"All odors\": np.all(where_123, axis=1),  #all\n",
    "        \"Odors 0&1\": (where_123[:, 0] & where_123[:, 1] & ~where_123[:, 2]),  # 12\n",
    "        \"Odors 0&2\": (where_123[:, 0] & ~where_123[:, 1] & where_123[:, 2]),  # 13\n",
    "        \"Odors 1&2\": (~where_123[:, 0] & where_123[:, 1] & where_123[:, 2]),  # 23\n",
    "        \"Odor 0\": (where_123[:, 0] & ~where_123[:, 1] & ~where_123[:, 2]),  #1\n",
    "        \"Odor 1\": (~where_123[:, 0] & where_123[:, 1] & ~where_123[:, 2]), \n",
    "        \"Odor 2\": (~where_123[:, 0] & ~where_123[:, 1] & where_123[:, 2])\n",
    "    }\n",
    "    all_colors = {\n",
    "        \"All odors\": \"xkcd:grey\",\n",
    "        \"Odors 0&1\": \"xkcd:orange\",\n",
    "        \"Odors 0&2\": \"xkcd:purple\",\n",
    "        \"Odors 1&2\": \"xkcd:green\",\n",
    "        \"Odor 0\": \"xkcd:red\",\n",
    "        \"Odor 1\": \"xkcd:yellow\",\n",
    "        \"Odor 2\": \"xkcd:blue\"\n",
    "        \n",
    "    }\n",
    "    for lbl, slc in locations.items():\n",
    "        ax.scatter(bkvecser_ibcm[slc, dims[0]], bkvecser_ibcm[slc, dims[1]], \n",
    "                   bkvecser_ibcm[slc, dims[2]], label=lbl, color=all_colors[lbl])\n",
    "    vecs = np.zeros(s_gamma_vecs.shape)\n",
    "    scale = 1.0\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = s_gamma_vecs[i] / np.sqrt(np.sum(s_gamma_vecs[i]**2)) * scale\n",
    "    ax.quiver(*orig, *(vecs[:, dims].T), color=\"k\", lw=2.0, arrow_length_ratio=0.2)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=100)\n",
    "    ax.view_init(azim=120, elev=35)\n",
    "    figname = \"background_manifold_three_odors_affinity_model.pdf\"\n",
    "    ax.set(xlabel=\"OSN {}\".format(dims[0]), ylabel=\"OSN {}\".format(dims[1]))\n",
    "    zlbl = ax.set_zlabel(\"OSN {}\".format(dims[2]))\n",
    "elif n_components == 2:\n",
    "    dims = (0, 1, 4)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    fig.set_size_inches(6.0, 3.0)\n",
    "    where_12 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    locations = {\n",
    "        \"Odor 0\": (where_12[:, 0] & ~where_12[:, 1]),  # 1\n",
    "        \"Odor 1\": (~where_12[:, 0] & where_12[:, 1]),  # 2\n",
    "        \"Both odors\": np.all(where_12, axis=1)  # all\n",
    "    }\n",
    "    all_colors = {\n",
    "        \"Odor 0\": \"tab:blue\",\n",
    "        \"Odor 1\": \"tab:orange\",\n",
    "        \"Both odors\": \"tab:green\",\n",
    "        \n",
    "    }\n",
    "    for i, lbl in enumerate(locations.keys()):\n",
    "        slc = locations[lbl]\n",
    "        ax.scatter(bkvecser_ibcm[slc, dims[0]], bkvecser_ibcm[slc, dims[1]], \n",
    "                   bkvecser_ibcm[slc, dims[2]], label=lbl, color=all_colors[lbl], zorder=3-i)\n",
    "    vecs = np.zeros(s_gamma_vecs.shape)\n",
    "    scale = 0.5\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = s_gamma_vecs[i] / np.sqrt(np.sum(s_gamma_vecs[i]**2)) * scale\n",
    "    ax.quiver(*orig, *(s_gamma_vecs[:, dims].T), color=\"k\", lw=2.0)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=100)\n",
    "    ax.view_init(elev=22)\n",
    "    figname = \"background_two_odors_3d_affinity_model.pdf\"\n",
    "    ax.set(xlabel=\"OSN {}\".format(dims[0]), ylabel=\"OSN {}\".format(dims[1]))\n",
    "    zlbl = ax.set_zlabel(\"OSN {}\".format(dims[2]))\n",
    "leg = ax.legend(loc=\"upper right\", bbox_to_anchor=(0.0, 1.0))\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(\"..\", \"figures\", \"adaptation\", figname), \n",
    "                transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(zlbl, leg))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background vectors time series with mixed concentrations\n",
    "tslice = slice(0, 50000, 200)\n",
    "n_cols = 6\n",
    "n_plots = 24 // 2  # Only show first 24 OSNs\n",
    "n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_cols*1.25, n_rows*1.25)\n",
    "for i in range(n_plots):\n",
    "    ax = axes.flat[i]\n",
    "    ax.scatter(bkvecser_ibcm[tslice, 2*i+1], bkvecser_ibcm[tslice, 2*i], \n",
    "               s=9, alpha=0.5, color=\"k\")\n",
    "    for j in range(n_components):\n",
    "        ax.plot(*zip([0.0, 0.0], s_gamma_vecs[j, 2*i:2*i+2:][::-1]), lw=2.0)\n",
    "    ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "for i in range(n_plots, n_rows*n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "fig.tight_layout()\n",
    "#if do_save_plots:\n",
    "#    fig.savefig(pj(\"..\", \"figures\", \"correlation\", \"osn_background_vectors.pdf\"), \n",
    "#               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the time course of the different neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = int(5/6*duration / deltat) // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, s_gamma_vecs)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. Easiest to compute numerically. \n",
    "conc_ser = nuser_ibcm[:, :, 1]\n",
    "# Odors are all iid so we can average over all odors\n",
    "mean_conc = np.mean(conc_ser)\n",
    "sigma2_conc = np.var(conc_ser)\n",
    "thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "moments_conc = [mean_conc, sigma2_conc, thirdmom_conc]\n",
    "\n",
    "# Analytical prediction\n",
    "res = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1)\n",
    "c_specif, c_nonspecif = res[:2]\n",
    "cs_cn = res[:2]\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 2.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, s_gamma_vecs, cs_cn, specif_gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(tser_ibcm[:300], nuser_ibcm[:300, :, 1])\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm/1000, thetaser_ibcm[:, i], lw=0.5, color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"$\\bar{\\Theta} = \\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=320000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "#ax.set_xlim([350, 360])\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "fig.tight_layout()\n",
    "leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "\n",
    "#fig.savefig(\"figures/powerlaw/cbargammas_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = (nuser_ibcm[:, :, 1] \n",
    "                     - np.mean(nuser_ibcm[transient:, :, 1], axis=0))\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/specificities_turbulent_background_example.pdf\", \n",
    "#           transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/pn_activity_norm_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/w_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of average fixed points\n",
    "Check the eigenvalues of the jacobian for one neuron, for every possible specificity. There are $2^{n_B}$ possibilities: choosing specific or not for each odor\n",
    "\n",
    "That calculation does not really work here, since the background process is not at all the linear superposition used analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_eigenvalues = ibcm_all_largest_eigenvalues(\n",
    "    moments_conc, ibcm_rates, s_gamma_vecs, m3=1.0, cut=1e-16, options=ibcm_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ibcm_specif_keys = list(all_max_eigenvalues.keys())\n",
    "ibcm_eig_values = np.asarray([all_max_eigenvalues[a] for a in ibcm_specif_keys])\n",
    "reals, imags = np.real(ibcm_eig_values), np.imag(ibcm_eig_values)\n",
    "ibcm_eig_values_specif1 = np.asarray([len(s) == 1 for s in ibcm_specif_keys], dtype=bool)\n",
    "highlights = ibcm_eig_values_specif1\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "scaleup = 1e3\n",
    "ax.plot(reals[highlights]*scaleup, imags[highlights]*scaleup, marker=\"*\", mfc=\"b\", mec=\"b\", \n",
    "        ls=\"none\", label=\"One odor\", ms=8)\n",
    "ax.plot(reals[~highlights]*scaleup, imags[~highlights]*scaleup, marker=\"o\", mfc=\"k\", mec=\"k\", \n",
    "       ls=\"none\", label=\"0 or 2+ odors\", ms=6)\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.legend(title=\"Specificity\")\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{max}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{max}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 3e-5  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 8.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "# We selected a seed (out of 40+ tested) giving initial conditions leading to correct PCA\n",
    "# The model has trouble converging on this background, we're giving as many chances as possible here. \n",
    "rgen_pca = np.random.default_rng(seed=0xa135db8c88ef9466dcf2320262a588be)\n",
    "init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " yser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.statistics import principal_component_analysis\n",
    "from modelfcts.checktools import compute_pca_meankept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
