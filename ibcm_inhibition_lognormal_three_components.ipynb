{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation with IBCM neurons gating inhibitory neurons\n",
    "The details of the model are described in other Jupyter notebooks (e.g., ibcm_inhibition_three_components.ipynb). The goal here is to include this model in the full olfactory network down to Kenyon cells, apply it to increasingly realistic olfactory backgrounds and estimate its performance at 1) inhibiting the fluctuating background, and 2) still recognizing new odors. \n",
    "\n",
    "Here, in particular, we focus on log-normal concentration fluctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network, \n",
    "    integrate_inhib_ibcm_network_tanh,\n",
    "    relu_inplace, \n",
    "    compute_mbars_cgammas_cbargammas\n",
    ")\n",
    "# Functions to update the fluctuating background variable\n",
    "from modelfcts.backgrounds import (\n",
    "    update_logou_kinputs, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    generate_odorant, \n",
    "    logof10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simulation\n",
    "Hopefully, the inhibitory neurons can be combined to perfectly inhibit the input. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 3  # Half the real number for faster simulations\n",
    "# The larger the dimension, the more likely the odors are orthogonal. \n",
    "n_components = 3  # no need to look at super complicated odors for now; keep effective space 3D\n",
    "# Can actually look at this latent space by Gram-Schmidt to find orthogonal axes spanning the input odors. \n",
    "n_neurons = 16  # Start small\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.001\n",
    "tau_avg = 200\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=92387)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Choose random exponential LI vectors\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=1.0)\n",
    "print(back_components)\n",
    "\n",
    "\n",
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# m_init, update_bk, bk_init, inhib_params, bk_params, tmax, dt, learnrate, seed=14345124, noisetype=\"normal\", tavg=10, coupling=0.1\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_alternating_inputs, init_back_altern, inhib_rates,\n",
    "                    back_params_altern, duration, deltat, learnrate=learnrate, seed=509811537, \n",
    "                    noisetype=\"uniform\", tavg=tau_avg, coupling=coupling_eta)\n",
    "tser, mser, nuser, cser, cbarser, _, wser, bkvecser = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 4  # Half the real number for faster simulations\n",
    "# The larger the dimension, the more likely the odors are orthogonal. \n",
    "n_components = 3  # no need to look at super complicated odors for now; keep effective space 3D\n",
    "# Can actually look at this latent space by Gram-Schmidt to find orthogonal axes spanning the input odors. \n",
    "n_neurons = 12  # Start small\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.0015\n",
    "tau_avg = 200\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "saturation_ampli = 50.0\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "ibcm_rates = [learnrate, tau_avg, coupling_eta, saturation_ampli]\n",
    "\n",
    "# Symmetric components to begin with\n",
    "back_components = 0.1*np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8\n",
    "    else:  # If there are more components than there are dimensions (ORNs)\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "print(back_components)\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "# Issue: the neurons do not seem to distribute equally to the 3 fixed points with this choice.\n",
    "# Maybe because of lack of symmetry there are not 3 stable fixed points anymore? Tricky. \n",
    "# Try other initial conditions\n",
    "rgen_meta = np.random.default_rng(seed=0x959905bd65b43006c10a3b72fb9ab60f)\n",
    "#init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "init_synapses = 0.5*rgen_meta.random(size=[n_neurons, n_components]).dot(back_components)\n",
    "\n",
    "# Try forcing a third of the neurons to each fixed point\n",
    "# By initializing them orthogonal to two of three background components. \n",
    "force_init = False\n",
    "if force_init:\n",
    "    init_synapses = np.zeros([n_neurons, n_components])\n",
    "    orthogonal1 = np.cross(back_components[0], back_components[1])\n",
    "    orthogonal2 = np.cross(back_components[1], back_components[2])\n",
    "    orthogonal3 = np.cross(back_components[2], back_components[0])\n",
    "    init_synapses[:n_neurons // 3] = orthogonal1[np.newaxis, :]\n",
    "    init_synapses[n_neurons // 3:2*n_neurons//3] = orthogonal2[np.newaxis, :]\n",
    "    init_synapses[2*n_neurons//3:] = orthogonal3[np.newaxis, :]\n",
    "    init_synapses += (rgen_meta.random(size=[n_neurons, n_components]) - 0.5) * 0.25\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "# Log-normal concentrations, nus are the logs of concentrations\n",
    "averages_nu = -0.5*np.ones(n_components)  # Average of log(c); for c < 1, these averages are < 0\n",
    "init_nu = averages_nu.copy()\n",
    "init_bkvec = np.exp(averages_nu*logof10).dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Mean and variance of the concentrations themselves\n",
    "# Using moments of log-normal: https://en.wikipedia.org/wiki/Log-normal_distribution\n",
    "mean_nu_lnbase = averages_nu.mean() * logof10\n",
    "vari_nu_lnbase = sigma2 * logof10**2\n",
    "lognorm_mean = np.exp(mean_nu_lnbase + vari_nu_lnbase/2.0)\n",
    "lognorm_vari = (np.exp(vari_nu_lnbase) - 1.0)*np.exp(2*mean_nu_lnbase + vari_nu_lnbase)\n",
    "# Third centered moment: rom skewness, multiply by its variance**3\n",
    "lognorm_skewness = (np.exp(vari_nu_lnbase) + 2)*np.sqrt(np.exp(vari_nu_lnbase) - 1)\n",
    "lognorm_thirdmom = lognorm_skewness * lognorm_vari**1.5\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_synapses, update_ou_kinputs, init_back_list, ibcm_rates, \n",
    "# inhib_rates, back_params, duration, deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\"\n",
    "#init_synapses = mser[-1]\n",
    "sim_results = integrate_inhib_ibcm_network_tanh(init_synapses, update_logou_kinputs, init_back_list, \n",
    "                    ibcm_rates, inhib_rates, back_params, duration, deltat, \n",
    "                    seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser, nuser, bkvecser, mser, cbarser, _, wser, sser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background statistics\n",
    "Useful figure for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All odors have the same statistics, flatten before taking histogram\n",
    "odor_concs_ser = np.exp((nuser+averages_nu[None, :])*logof10)\n",
    "odor_concs_histo, odor_concs_binseps = np.histogram(odor_concs_ser, bins=100, density=True)\n",
    "\n",
    "# Plot histogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar((odor_concs_binseps[1:]+odor_concs_binseps[:-1])/2.0, odor_concs_histo, \n",
    "       width=np.diff(odor_concs_binseps), color=\"grey\")\n",
    "ax.set(xlabel=\"Odor concentration\", ylabel=\"Probability density\", yscale=\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the synaptic weights against fixed points\n",
    "The analytical prediction neglecting correlations between $\\vec{m}$ and $\\nu$ is verified, provided that the time scales $\\tau_{\\nu}$ and $\\frac{1}{\\mu}$ are different enough. Computing corrections to account for incompletely separated time scales would be very hard, since the equation for $\\vec{m}$ is a multivariate, non-linear stochastic differential equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm_analytics import fixedpoint_thirdmoment_perturbtheory, fixedpoint_thirdmoment_exact\n",
    "from simulfcts.plotting import plot_cbars_gammas_sums, plot_cbars_gamma_series, plot_3d_series, plot_w_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 120000\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(mser, coupling_eta, back_components)\n",
    "# Compute analytical prediction for sum of cgammas and (cgamma squared)s. \n",
    "# Uses perturbation theory even though the third moment isn't exactly small\n",
    "res = fixedpoint_thirdmoment_perturbtheory(lognorm_mean, lognorm_vari, lognorm_thirdmom, \n",
    "                                           1, n_components-1, m3=1.0, order=1)\n",
    "\n",
    "pred_cbars_gamma = res[:2]\n",
    "pred_sums_cbars = list(res[2:])\n",
    "# The function returns c_d, which is sum of c_gammas times average concentration\n",
    "pred_sums_cbars[0] = pred_sums_cbars[0] / lognorm_mean\n",
    "pred_sums_cbars = tuple(pred_sums_cbars)\n",
    "\n",
    "# Compare to the exact solution\n",
    "res = fixedpoint_thirdmoment_exact([lognorm_mean, lognorm_vari, lognorm_thirdmom], 1, n_components-1)\n",
    "\n",
    "pred_cbars_gamma_exact = res[:2]\n",
    "pred_sums_cbars_exact = list(res[2:])\n",
    "# The function returns c_d, which is sum of c_gammas times average concentration\n",
    "pred_sums_cbars_exact[0] = pred_sums_cbars_exact[0] / lognorm_mean\n",
    "pred_sums_cbars_exact = tuple(pred_sums_cbars_exact)\n",
    "\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Constaint 1: sum of c_gammas for each neuron, equal to 1 plus correction\n",
    "print(\"Comparison to analytical fixed points\")\n",
    "print(\"This should be approximately zeros:\", np.mean(sums_cbars_gamma[transient:], axis=0) / pred_sums_cbars[0] - 1.0)\n",
    "print(\"This should be all zeros, exact analytical solution:\", np.mean(sums_cbars_gamma[transient:], axis=0) / pred_sums_cbars_exact[0] - 1.0)\n",
    "# Constraint 2: sum of c_gammas^2 for each neuron, compared to 1/sigma^2 + correction\n",
    "print(\"This should be all approximately zeros:\", np.mean(sums_cbars_gamma2[transient:], axis=0) / pred_sums_cbars[1] - 1.0)\n",
    "print(\"This should be all zeros, exact analytical solution:\", np.mean(sums_cbars_gamma2[transient:], axis=0) / pred_sums_cbars_exact[1] - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_cbars_gammas_sums(tser, sums_cbars_gamma, sums_cbars_gamma2, skp=200, skp_lbl=1)\n",
    "axes[0].axhline(pred_sums_cbars[0], ls=\"--\", color=\"k\", label=r\"Perturb. $1 / \\langle \\nu \\rangle$\")\n",
    "axes[1].axhline(pred_sums_cbars[1], ls=\"--\", color=\"k\", label=r'Perturb. $1 / \\sigma^2$')\n",
    "axes[0].axhline(pred_sums_cbars_exact[0], ls=\"-.\", color=\"grey\", label=r\"Exact $1 / \\langle \\nu \\rangle$\")\n",
    "axes[1].axhline(pred_sums_cbars_exact[1], ls=\"-.\", color=\"grey\", label=r'Exact $1 / \\sigma^2$')\n",
    "for ax in axes:\n",
    "    ax.get_legend().set_visible(False)\n",
    "axes[0].legend(*[a[-2:] for a in axes[0].get_legend_handles_labels()])\n",
    "axes[1].legend(*[a[-2:] for a in axes[1].get_legend_handles_labels()])\n",
    "# fig.savefig(\"figures/three_odors/sum_cgammas_squared_lognormal_background.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprisingly good!\n",
    "fig, ax, _ = plot_cbars_gamma_series(tser, cbars_gamma, skp=100, transient=50000)\n",
    "ax.get_legend().set_visible(False)\n",
    "\n",
    "# Annotate with analytical prediction. Might fail because third moment is high. \n",
    "ax.axhline(pred_cbars_gamma[0], ls=\"--\", color=\"k\", label=r\"Perturbative $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")  # higher value\n",
    "ax.axhline(pred_cbars_gamma[1], ls=\":\", color=\"k\", label=r\"Perturbative $\\bar{c}_{\\gamma=\\mathrm{non}}$\")  # lower value\n",
    "# Exact solution should align well nevertheless\n",
    "ax.axhline(pred_cbars_gamma_exact[0], ls=\"--\", color=\"grey\", label=r\"Exact $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")  # higher value\n",
    "ax.axhline(pred_cbars_gamma_exact[1], ls=\":\", color=\"grey\", label=r\"Exact $\\bar{c}_{\\gamma=\\mathrm{non}}$\")  # lower value\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_series(mbarser, dim_idx=[0, 1, 2], transient=10000, skp=1000)\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=45, elev=30)\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "# fig.savefig(\"figures/three_odors/points_fixes_ibcm_3_odeurs_lognormal.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the inhibitory neurons' weights $\\vec{w}_i$\n",
    "Analytically, I find that, on average, $\\vec{w}_i$ converges to $\\vec{x}(\\pm \\sigma)$, i.e. to either input vector one standard deviation away from the mean input. So, here, I compare the numerical results for $\\vec{w}$ to the possible fixed points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the time course of the dot products -- not interesting with gaussian degeneracy\n",
    "# Unclear what it shows. \n",
    "fig, axes = plot_w_matrix(tser, wser, skp=500, lw=1.5)\n",
    "        \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background before and after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_background_norm_inhibition, plot_background_neurons_inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(tser, bkvecser, sser)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bknorm_ser[transient:])\n",
    "avg_snorm = np.mean(snorm_ser[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser[transient:])\n",
    "std_snorm = np.std(snorm_ser[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/three_odors/inhibition_lognormal_background_norm_3odors.pdf\", \n",
    "            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser, bkvecser, sser)\n",
    "axes[-1].legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.6), fontsize=8, handlelength=1.5)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/three_odors/inhibition_lognormal_background_neurons_3odors.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3D plot of the original and inhibited odors, sampled sparsely in time\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Raw background\n",
    "skp = 1000\n",
    "tslice = slice(transient, None, skp)\n",
    "ax.scatter(bkvecser[tslice, 0], bkvecser[tslice, 1], bkvecser[tslice, 2], color=\"r\", label=\"Background\")\n",
    "# Compare to inhibition of the mean\n",
    "mean_inhibition = bkvecser - np.mean(bkvecser[transient:], axis=0)*inhib_rates[0]/sum(inhib_rates)\n",
    "ax.scatter(mean_inhibition[tslice, 0], mean_inhibition[tslice, 1], mean_inhibition[tslice, 2], \n",
    "           color=\"xkcd:light blue\", label=\"Average subtraction\")\n",
    "ax.scatter(sser[tslice, 0], sser[tslice, 1], sser[tslice, 2], \n",
    "           color=\"b\", label=\"IBCM inhibition\")\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=200, alpha=1)\n",
    "ax.view_init(azim=160, elev=20)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.85))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some results for re-plotting\n",
    "Use as a sample simulation of a log-normal background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results/for_plots/\"\n",
    "np.savez_compressed(results_dir + \"sample_lognormal_simulation.npz\", nuser=nuser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding correlation between odors\n",
    "Not sure what happens then. Analytical predictions fail for non-gaussian distributions. For gaussian, this amounts to a re-definition of $\\vec{x}_{\\gamma}$s, but extra third moment terms appear otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgen_meta_cr = np.random.default_rng(seed=0x91117e6a405e752c66db7f05127e03a6)\n",
    "#init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "init_synapses_cr = 0.5*rgen_meta_cr.random(size=[n_neurons, n_components]).dot(back_components)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2_cr = 0.09\n",
    "correl_rho_cr = 0.5\n",
    "steady_covmat_cr = correl_rho_cr * sigma2_cr * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat_cr[np.eye(n_components, dtype=bool)] = sigma2_cr  # diagonal: ones\n",
    "\n",
    "# Mean and variance of the concentrations themselves\n",
    "# Using moments of log-normal: https://en.wikipedia.org/wiki/Log-normal_distribution\n",
    "# Not sure what they are when correlated; TODO\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat_cr = np.linalg.cholesky(steady_covmat_cr)\n",
    "update_mat_B_cr = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat_cr\n",
    "\n",
    "back_params_cr = [update_mat_A, update_mat_B_cr, back_components, averages_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_synapses, update_ou_kinputs, init_back_list, ibcm_rates, \n",
    "# inhib_rates, back_params, duration, deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\"\n",
    "#init_synapses = mser[-1]\n",
    "sim_results = integrate_inhib_ibcm_network_tanh(init_synapses_cr, update_logou_kinputs, init_back_list, \n",
    "                    ibcm_rates, inhib_rates, back_params_cr, duration, deltat, \n",
    "                    seed=seed_from_gen(rgen_meta_cr), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser_cr, nuser_cr, bkvecser_cr, mser_cr, cbarser_cr, _, wser_cr, sser_cr = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations between odors\n",
    "# All odors have the same statistics, flatten before taking histogram\n",
    "odor_concs_ser_cr = np.exp((nuser_cr+averages_nu[None, :])*logof10)\n",
    "\n",
    "# Plot a time slice\n",
    "tslice = slice(0, 200, 1)\n",
    "fig, ax = plt.subplots()\n",
    "odor_colors = sns.color_palette(\"Greys\", n_colors=n_components)\n",
    "for i in range(n_components):\n",
    "    ax.plot(tser_cr[tslice], odor_concs_ser_cr[tslice, i], label=r\"$\\gamma = {}$\".format(i), \n",
    "            color=odor_colors[i])\n",
    "ax.set(xlabel=\"Time (steps)\", ylabel=\"Odor concentration\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 120000\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser_cr, c_gammas_cr, cbars_gamma_cr = compute_mbars_cgammas_cbargammas(mser_cr, coupling_eta, back_components)\n",
    "\n",
    "# Analytical predictions already computed, not assuming correlations \n",
    "# change anything -- to see how wrong it is to suppose that\n",
    "sums_cbars_gamma_cr = np.sum(cbars_gamma_cr, axis=2)\n",
    "sums_cbars_gamma2_cr = np.sum(cbars_gamma_cr**2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The analytical prediction is quantitatively off, but the principle holds\n",
    "# Each neuron becomes specific to one $\\vec{x}_{\\gamma}$, non-specific to others. \n",
    "fig, ax, _ = plot_cbars_gamma_series(tser_cr, cbars_gamma_cr, skp=100, transient=50000)\n",
    "ax.get_legend().set_visible(False)\n",
    "\n",
    "# Annotate with analytical prediction. Might fail because third moment is high. \n",
    "ax.axhline(pred_cbars_gamma[0], ls=\"--\", color=\"k\", label=r\"Analytic $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")  # higher value\n",
    "ax.axhline(pred_cbars_gamma[1], ls=\":\", color=\"grey\", label=r\"Analytic $\\bar{c}_{\\gamma=\\mathrm{non-specif}}$\")  # lower value\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding why neurons fail to distribute to different fixed points\n",
    "# THIS IS NOW OUTDATED, THE NETWORK WORKS\n",
    "Because of optimally-derived $W$ weights, all is fine. \n",
    "\n",
    "Still, IBCM neurons may not distribute optimally, should double-check TODO\n",
    "\n",
    "One hypothesis: with the log-normal noise, basins of attractor favour, in high-dimensional space ($K\\times N$), steady-states where the neurons go to the same fixed point. Not sure why that would be and hard to understand. \n",
    " - Consequence of that hypothesis: the neurons would go to the same fixed point if they are initialized at the same places initially, no matter the realization of the process. \n",
    "\n",
    "\n",
    "Another hypothesis (Gautam Reddy): a large fluctuation along one of the components early in the simulation pushes all neurons in the basin of attraction of the same fixed point. \n",
    " - Consequence of that hypothesis: even initialized at the same initial positions, neurons would converge to different fixed points from one realization to the next. \n",
    " \n",
    "Note: to test both hypotheses, we need to use one seeded random generator for initial positions, and another generator for the time simulations. We keep the initial position generator at the same seed, such that we can keep the same initial conditions, but we try different seeds for the time evolution process, to obtain different dynamical realizations.\n",
    "\n",
    "If the majority fixed point changes from one realization to the next, starting from the same initial conditions, the second hypothesis is more likely. I should then try to identify the fluctuation that causes neurons to change basin of attraction. \n",
    "\n",
    "However, if the final fixed point is always the same, it means the cause is not in a realization-dependent fluctuation, but really in the structure of the model, averaged over realizations. \n",
    "\n",
    "## Code to achieve this test\n",
    "\n",
    "Anyways, the first thing I need is a fixed point detector, to detect automatically which fixed point neurons converged to. I coded such a function below; it is based on the analytical fixed points. A simple clustering algorithm could work too when I start working on cases for which I have absolutely no analytical answer. \n",
    "\n",
    "When I start working with more than three components, I will be able to check. But here, I am sticking to three components, where obviously the dot product with two components will be one value, and with the third, another. \n",
    " - Note that it could be that the dot product is the same for all components, or that two components have the high positive dot product, the odd one being at the negative dot product. But that isn't the case. Numerically, all neurons have two dot products equal to the low value, and one equal to the high value. There must be some analytical proof of that... \n",
    " \n",
    " ## Analytical calculations to come\n",
    " - Stability of fixed points\n",
    " - Multiscale analysis to understand where the time scale of convergence of inhibitory neurons come from. Even get an expression for it in terms of the model rates. See Misbah and Bender-Orszag. Or slow-fast analysis? See papers from Prof. Khadra's course. \n",
    " - WORK HARD!!! Stop feeling that I don't master the system well enough and hence shying away from calculations. This is a vicious circle that I need to break by trying as hard as I can to do those calculations. Set long time slots aside to do that work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few simulations with different seeds\n",
    "# Record the late time series of synaptic weights \\bar{m}, discarding transient time steps. \n",
    "late_time_series = []\n",
    "noise_reductions = []\n",
    "n_simuls = 10\n",
    "last_steps = 10000\n",
    "for i in range(n_simuls):\n",
    "    sim_results = integrate_inhib_ibcm_network(init_synapses, update_logou_kinputs, init_back_list, inhib_rates,\n",
    "                    back_params, duration-20000, deltat, learnrate=learnrate, seed=50981*(i+1), \n",
    "                    noisetype=\"normal\", tavg=tau_avg, coupling=coupling_eta)\n",
    "    late_time_series.append(sim_results[1][-last_steps:])\n",
    "    # Also compute the noise reduction\n",
    "    inhib_projser = apply_background_inhibition(sim_results[4], sim_results[6], sim_results[7], n_neurons)\n",
    "    stdev_inhib_comps = np.std(inhib_projser[-last_steps:], axis=0)\n",
    "    stdev_bk_comps = np.std(sim_results[7][-last_steps:], axis=0)\n",
    "    noise_reductions.append((stdev_inhib_comps/stdev_bk_comps)*100)\n",
    "    print(\"Finished simulation {}; noise reduction by {} %\".format(i, noise_reductions[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_fixed_point(neuron_ser, fixed_pts_dict={}, distance_metric=\"euclidean\"):\n",
    "    \"\"\" Given the steady-state time series of a neuron, determine which point it\n",
    "    is closest to in the dictionary of possible fixed points, fixed_pts_dict. \n",
    "    Even if the analytical fixed point predictions are not too great, as long\n",
    "    as they are closest to the real fixed points than to one another, this should\n",
    "    be good enough to distinguish neurons that converged to different fixed points. \n",
    "    \n",
    "    Args:\n",
    "        neuron_ser (np.ndarray): shape (n_time_steps, n_dimensions). \n",
    "        fixed_pts_dict (dict): each value is a potential fixed point, \n",
    "            a 1d array of shape (n_dimensions,), and the key of that value\n",
    "            is the \"annotation\" of that fixed point.\n",
    "        distance_metric (str): currently, only \"euclidean\" is supported. \n",
    "            Could use sklearn's metrics to allow more. Not implemented for now. \n",
    "            \n",
    "    Returns:\n",
    "        annotation (object): the key of the fixed point in fixed_pts_dict that\n",
    "            is closest to the average steady-state of the neuron. \n",
    "        distance (float): the distance to the closest fixed point. \n",
    "    \"\"\"\n",
    "    if distance_metric != \"euclidean\":\n",
    "        raise NotImplementedError(\"Currently, only 'euclidean' distance metric is supported.\")\n",
    "    avg_neuron = np.mean(neuron_ser, axis=0)\n",
    "    distances = {}\n",
    "    for tag in fixed_pts_dict.keys():\n",
    "        distances[tag] = np.sqrt(np.sum((avg_neuron - fixed_pts_dict[tag])**2))\n",
    "    annotate = max(distances, key=distances.get)\n",
    "    return annotate, distances[annotate]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_fixed_points(backvecs, avg_nus):\n",
    "    \"\"\" As a very crude approximation of the fixed points, instead of going through\n",
    "    the perturbative solution, use vectors orthogonal to all but one of the \n",
    "    background components. Using Gram-Schmidt, then give it a norm of 1/<nu>. \n",
    "    Args:\n",
    "        backvecs (np.ndarray): array of background components, \n",
    "            indexed (component, dimension)\n",
    "        avg_nus (np.ndarray): array of average nu of each background vector. \n",
    "    \"\"\"\n",
    "    # Minimize backvecs.dot(B) - identity, solving for B, the new basis\n",
    "    # Identity should have as many rows as there are vectors, so first dim. of backvecs. \n",
    "    vecs, _, _, _ = np.linalg.lstsq(backvecs, np.identity(backvecs.shape[0]), rcond=None)\n",
    "    # Normalize columns, then multiply by 1/nu\n",
    "    vecs = vecs / np.sqrt(np.sum(vecs**2, axis=0))\n",
    "    vecs = vecs / avg_nus\n",
    "    # Return transposed, so each row contains a vector\n",
    "    return vecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fixed point to which each neuron converged. \n",
    "# Determine approximate fixed points as a guideline. \n",
    "# The averages are 10^<nu>\n",
    "orthogonal_fixed_points = approx_fixed_points(back_components, np.exp(logof10*averages_nu))\n",
    "print(orthogonal_fixed_points)\n",
    "dict_fixed_points = {i:orthogonal_fixed_points[i] for i in range(n_components)}\n",
    "\n",
    "df_fixed_pts = pd.DataFrame(np.zeros([n_simuls*n_neurons, 2], dtype=np.int64), \n",
    "                            index=pd.MultiIndex.from_product([list(range(n_simuls)), \n",
    "                                        list(range(n_neurons))], names=[\"Simulation\", \"Neuron\"]), \n",
    "                            columns=pd.Index([\"Annotation\", \"Distance\"], name=\"Fixed point\"))\n",
    "df_fixed_pts = df_fixed_pts.astype({'Distance': 'float64'})\n",
    "\n",
    "for i in range(n_simuls):\n",
    "    for j in range(n_neurons):\n",
    "        annot, dista = annotate_fixed_point(late_time_series[i][:, j], fixed_pts_dict=dict_fixed_points)\n",
    "        df_fixed_pts.loc[(i, j), \"Annotation\"] = annot\n",
    "        df_fixed_pts.loc[(i, j), \"Distance\"] = dista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "late_time_series[0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fractions = df_fixed_pts[\"Annotation\"].reset_index().groupby([\"Simulation\", \"Annotation\"]).count().unstack(\"Annotation\") / n_neurons * 100\n",
    "df_fractions.columns = df_fractions.columns.droplevel(0)\n",
    "df_fractions.columns.name = \"Fixed point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph showing this?\n",
    "fig, axes = plt.subplots(5, 2)\n",
    "fig.set_size_inches(2*1.5, 5*1.3)\n",
    "for i in df_fractions.index:\n",
    "    axes.flat[i].pie(df_fractions.loc[i].values, autopct='%d%%', shadow=False)\n",
    "fig.tight_layout(w_pad=0.3, h_pad=0.3)\n",
    "#fig.savefig(\"figures/three_odors/pie_charts_neuron_distributions_different_runs.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
