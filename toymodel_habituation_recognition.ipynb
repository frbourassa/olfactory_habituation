{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to a toy model of odor background\n",
    "Look at new odor recognition on top of a two-odor, one fluctuating proportion background:\n",
    "\n",
    "$$ \\vec{x}(t) = \\left(\\frac12 + \\nu \\right) \\vec{x}_a + \\left(\\frac12 - \\nu \\right) \\vec{x}_b $$\n",
    "\n",
    "The odor vectors are generated with i.i.d. exponential elements with scale (mean) mu=0.1, then normalized to have an L2 norm of 1. I take $\\nu(t)$ following a Ornstein-Uhlenbeck process, typically with standard deviation $\\sigma = 0.3$ and average $\\langle \\nu \\rangle = 0$. \n",
    "\n",
    "Here, I optionally clip elements of $\\vec{x}(t)$ to be non-negative, although this does not make a significant difference since samples where $|\\nu| > 0.5$ are rare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoints_m_2vectors, \n",
    "    fixedpoints_barm_2vectors, \n",
    "    fixedpoints_w_2vectors, \n",
    "    fixedpoint_s_2vectors_instant, \n",
    "    fixedpoint_s_2vectors_mean, \n",
    "    fixedpoint_s_2vectors_norm2\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_ifpsp_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import(\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    compute_pca_meankept, \n",
    "    compute_projector_series, \n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_ou_2inputs_clip, \n",
    "    update_ou_2inputs,\n",
    "    logof10, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    "    tags_list_to_csr_matrix\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 25\n",
    "n_components = 2  # Two odors but only one fluctuating proportion\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Simulation duration\n",
    "duration = 80000.0\n",
    "deltat = 1.0\n",
    "skp = 1  # We can save all time steps, small enough network\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_ou_2inputs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x4146e7d68791560a406669280c37e5bb)\n",
    "\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "average_nu = np.zeros(1)\n",
    "init_nu = np.zeros(1)\n",
    "init_bkvec = 0.5*back_components[0] + 0.5*back_components[1]\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "sigma2_nu = 0.09\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_coefs_mean = np.exp(-deltat/tau_nu)\n",
    "update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*deltat/tau_nu)))\n",
    "\n",
    "back_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 2  # Two neurons is enough, one per fixed point x_{\\pm}\n",
    "\n",
    "learnrate_ibcm = 0.0025\n",
    "tau_avg_ibcm = 300\n",
    "coupling_eta_ibcm = 0.2\n",
    "decay_relative_ibcm = 0.005\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    None,   # Saturation, none\n",
    "    None,   # For the Law and Cooper variant\n",
    "    decay_relative_ibcm\n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"linear\", \n",
    "    \"variant\": \"intrator\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.1*rgen_meta.random(size=[n_i_ibcm, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back_list, \n",
    "                ibcm_rates, inhib_rates, back_params, duration, \n",
    "                deltat, seed=simul_seed, noisetype=\"normal\",  \n",
    "                skp=skp, **ibcm_options\n",
    ")\n",
    "\n",
    "(tser_ibcm, \n",
    " nuser_ibcm, \n",
    " bkvecser_ibcm, \n",
    " mser_ibcm, \n",
    " cbarser_ibcm, \n",
    " wser_ibcm, \n",
    " sser_ibcm) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_synapses_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 50000 // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, back_components)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction of fixed points\n",
    "analytical_barm, _ = fixedpoints_barm_2vectors(back_components, np.sqrt(sigma2_nu), \n",
    "                                               coupling_eta_ibcm, n_r=n_dimensions)\n",
    "# Plot dot products with x_a and x_b, in terms of reduced m vectors. \n",
    "# Take any of the fixed points for one neuron, its two dot products are the only two possible dot product values. \n",
    "c_specif = np.dot(analytical_barm[2, 0], back_components[0])\n",
    "c_nonspecif = np.dot(analytical_barm[2, 0], back_components[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cbar2_avg term throughout\n",
    "cbar2_avg_ser = moving_average(cbarser_ibcm*cbarser_ibcm, kernelsize=tau_avg_ibcm)\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm[:-tau_avg_ibcm], cbar2_avg_ser[:-tau_avg_ibcm, i], \n",
    "            color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000)\", ylabel=r\"$\\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=50000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"-.\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = np.stack([nuser_ibcm[:, 0], -nuser_ibcm[:, 0]], axis=1)\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "split_val = 0.0\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = 1  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 0.0005  # Learning rate of M\n",
    "rel_lrate_pca = 2.0  # Learning rate of L, relative to learnrate\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "lambda_range_pca = 0.5\n",
    "lambda_mat_diag = build_lambda_matrix(lambda_range_pca, n_i_pca)\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_range_pca]\n",
    "if pca_options[\"remove_mean\"]:\n",
    "    biopca_rates.append(xavg_rate_pca)\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " sser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis\n",
    "\n",
    "#### Analytical predictions for one PCA neuron\n",
    "$L$ matrix: 1x1, a scalar, equal to the inverse of the eigenvalue: $L = 1/L' = \\frac{1}{\\sigma^2 \\| \\vec{x}_s \\|^2}$\n",
    "\n",
    "$M$ matrix: 1x$n_R$, parallel to the fluctuating part of the background, $\\vec{m} = \\sigma^2 \\| \\vec{x}_s \\| \\vec{x}_s$.\n",
    "\n",
    "$W$ matrix: $n_Rx1$, a vector also parallel to $\\vec{x}_s$, we find $\\vec{w} = \\frac{\\sigma^2 \\| \\vec{x}_s \\|}{\\sigma^2 \\| \\vec{x}_s \\|^2 + \\beta/\\alpha} \\vec{x}_s$. \n",
    "\n",
    "Then the instantaneous PN activity should be $\\vec{s}(t) = \\vec{x}(t) - \\langle \\vec{x} \\rangle - WLM(\\vec{x}(t) - \\langle \\vec{x} \\rangle) = \\frac{\\beta/\\alpha}{\\beta/\\alpha + \\sigma^2 \\| \\vec{x}_s \\|^2} \\nu(t) \\vec{x}_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.pca_analytics import (\n",
    "    fixedpoints_pca_2vectors, \n",
    "    pca_fixedpoint_s_2vectors_instant, \n",
    "    pca_fixedpoint_s_2vectors_variance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = (back_components[0] - back_components[1])\n",
    "x_d = (back_components[0] + back_components[1])/2\n",
    "m_pca_pred, l_pca_pred, w_pca_pred = fixedpoints_pca_2vectors(back_components, sigma2_nu, inhib_rates)\n",
    "sser_pca_pred = pca_fixedpoint_s_2vectors_instant(back_components, sigma2_nu, inhib_rates, bkvecser_pca)\n",
    "svari_pca_pred = pca_fixedpoint_s_2vectors_variance(back_components, sigma2_nu, inhib_rates)\n",
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m_pca_pred)\n",
    "print(l_pca_pred)\n",
    "print(w_pca_pred)\n",
    "print(sser_pca_pred[-1])\n",
    "xvari_pca = np.var(l2_norm(bkvecser_pca, axis=1))\n",
    "xvari_pca = np.mean(np.sum((bkvecser_pca - x_d)**2, axis=1), axis=0)\n",
    "print(np.sqrt(svari_pca_pred / xvari_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.statistics import principal_component_analysis\n",
    "from modelfcts.checktools import compute_pca_meankept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, sser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "sser_ideal = bkvecser_ibcm * ideal_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snorm_series = {\n",
    "    \"ibcm\": l2_norm(sser_ibcm), \n",
    "    \"biopca\": l2_norm(sser_pca), \n",
    "    \"avgsub\": l2_norm(sser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(sser_ideal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(snorm_series[a], **std_options)) for a in snorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(snorm_series[a], **mean_options) for a in snorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "snorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + snorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + snorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)\n",
    "\n",
    "sser_dict = {\n",
    "    \"ibcm\": sser_ibcm, \n",
    "    \"biopca\": sser_pca, \n",
    "    \"avgsub\": sser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": sser_ideal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 100\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "avg_whiff_conc = 0.5\n",
    "print(\"Average whiff concentration: {:.4f}\".format(avg_whiff_conc))\n",
    "new_test_concs *= avg_whiff_conc\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "# Steady-state distribution of nu: normal distribution, mean 0, variance sigma2\n",
    "conc_samples = rgen_meta.normal(loc=0.0, scale=np.sqrt(sigma2_nu), size=n_test_times*(n_back_samples-1))\n",
    "# Clip between -0.5 and 0.5\n",
    "conc_samples = np.clip(conc_samples, a_min=-0.5, a_max=0.5).reshape(-1, 1)\n",
    "back_samples = (0.5+conc_samples)*back_components[0:1] + (0.5-conc_samples)*back_components[1:2]\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_svecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in sser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in sser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in sser_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_svecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_svecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_svecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_svecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_svecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, mixtures, new_test_concs[k], \n",
    "                *inhib_rates, **avg_options\n",
    "            )\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_svecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_svecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_svecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_svecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_one_odor_habituation_{}.pdf\".format(activ_function),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_svecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_snorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
