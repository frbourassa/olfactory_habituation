{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM model for fluctuating inputs\n",
    "\n",
    "## Model to consider\n",
    "I consider a feedforward network of IBCM  neurons. Each IBCM neuron has two input connections, represented by the connectivity vector $\\vec{m}_i = (m^1_i, m^2_i)$. Its activation upon stimulation is given by $c = \\vec{m}_i \\cdot \\vec{x}$, where $\\vec{x}$ is a two-dimensional input vector. Its inhibited activity is \n",
    "\n",
    "$$ \\bar{c}_i = c_i - \\eta \\sum_{j \\neq i} c_j \\quad \\mathrm{where} \\, \\, c_i(t) = \\vec{m}_i(t) \\cdot \\vec{x}(t)  \\, \\,$$\n",
    "\n",
    "The update equation of each IBCM neuron's weights uses this inhibited activity:\n",
    "\n",
    "$$ \\dot{m}_i = \\mu \\left[ \\bar{c}_i(\\bar{c}_i - \\bar{\\Theta}_{m, i}) \\vec{x} - \\eta \\sum_{j \\neq i} \\bar{c}_j(\\bar{c}_j - \\bar{\\Theta}_{m, j}) \\vec{x} \\right]  \\quad \\mathrm{where} \\, \\, \\bar{\\Theta}_m = \\mathbb{E}[\\bar{c}_i^2] $$\n",
    "\n",
    "The parameter $\\eta$ is the coupling strength. \n",
    "\n",
    "The expectation value is taken over a time window $T$, which should be much larger than the fluctuation time scale of the input $\\tau$, but also much smaller than the $\\vec{m}$ evolution time scale $\\frac{1}{\\mu}$. To begin with, I just average computationally, but eventually, for realism of the model, I will compute $\\Theta$ with an auxiliary variable that performs a time-averaging by having a slow decay rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.statistics import estimate_empirical_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_ibcm_network(m_init, update_bk, bk_init, bk_params, tmax, dt, \n",
    "                   learnrate, seed=14345124, noisetype=\"normal\", tavg=100, coupling=0.1):\n",
    "    \"\"\" Integrate the IBCM equation when the input updated by the derivative\n",
    "    function update_bk, which takes pre-generated noise and bk_params. \n",
    "    The intended usage here is for an input that is a linear \n",
    "    combination of two LI vectors, and the proportion of each \n",
    "    component in the input fluctuates around 1/2 following a \n",
    "    Ornstein-Uhlenbeck process with rate specified in bk_params. \n",
    "    \n",
    "    Args:\n",
    "        m_init (np.ndarray): 2d array, shape (number neurons, number dimensions)\n",
    "        update_bk (callable): function that updates the background variables and \n",
    "            the background vector\n",
    "        bk_init (list of two 1d np.ndarrays): [bk_vari_init, bk_vec_init]\n",
    "            bk_vari_init (np.ndarray): 1d array of background random variables\n",
    "            bk_vec_init (np.ndarray): initial background vector, must have size n_dim\n",
    "        bk_params (list): list of parameters passed to update_bk (3rd argument)\n",
    "        tmax (float): max time\n",
    "        dt (float): time step\n",
    "        learnrate (float): kinetic learning rate \\mu in the IBCM equation\n",
    "        seed (int): seed for the random number generator\n",
    "        noisetype (str): either \"normal\" or \"uniform\"\n",
    "        tavg (float): default: 10\n",
    "        coupling (float): eta, between 0 and 1\n",
    "   \n",
    "    Returns:\n",
    "        tseries, m_series, bk_series, c_series, cbar_series, w_series, bkvec_series\n",
    "    \"\"\"\n",
    "    n_neu = m_init.shape[0]  # Number of neurons\n",
    "    n_dim = m_init.shape[1]\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    n_bkvari = bk_vari_init.shape[0]\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    \n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "    tavg = int(round(tavg / dt))  # Number of time steps over which to average\n",
    "    \n",
    "    # Containers for the solution over time\n",
    "    bk_series = np.zeros([tseries.shape[0], n_bkvari])\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])\n",
    "    c_series = np.zeros([tseries.shape[0], n_neu])\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "    \n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects. \n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities\n",
    "    cbar2_avg = np.zeros(n_neu)  # Average inhibited neuron activities squared, initialized at zero\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    m = m_init.copy()\n",
    "    \n",
    "    # Initialize neuron activity with m and background at time zero\n",
    "    c = m.dot(bkvec)\n",
    "    cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "    cbar2_avg = cbar**2\n",
    "    \n",
    "    # Store back some initial values in containers\n",
    "    c_series[0] = c\n",
    "    cbar_series[0] = cbar\n",
    "    bk_series[0] = bk_vari\n",
    "    m_series[0] = m_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    \n",
    "    # Generate N(0, 1) noise samples in advance\n",
    "    if (tseries.shape[0]-1)*n_dim > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1, n_bkvari))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, n_bkvari))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    \n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        t += dt  \n",
    "        ### IBCM neurons\n",
    "        # Update first their synaptic weights m to time k+1 before updating c, cbar, \n",
    "        # because dm/dt depends on c, cbar, cbar2_avg at time k\n",
    "        # Phi function for each neuron\n",
    "        phiterms_vec = cbar * (cbar - cbar2_avg)\n",
    "        # Now, careful with broadcast: for each neuron (dimension 0 of m and cbar), we need a scalar element\n",
    "        # of phiterms_vec times the whole bkvec, for dimension 1 of m. \n",
    "        # This can be done vectorially with a dot product (n_neu, 1)x(1, n_components)\n",
    "        rhs_scalar = phiterms_vec - coupling*(np.sum(phiterms_vec) - phiterms_vec)\n",
    "        # Euler integrator and learning rate\n",
    "        m = m + learnrate*dt*rhs_scalar[:, np.newaxis].dot(bkvec[np.newaxis, :])\n",
    "        \n",
    "        # Store the updated synaptic weights\n",
    "        m_series[k+1] = m\n",
    "        \n",
    "        # Now, update to time k+1 the threshold (cbar2_avg) using cbar at time k\n",
    "        # to be used to update m in the next time step\n",
    "        cbar2_avg = cbar2_avg + (cbar*cbar - cbar2_avg)/tavg * dt\n",
    "        # This Euler scheme could cause numerical stability problems if dt is too large, \n",
    "        # or tavg too small, or the average vector too large. \n",
    "        \n",
    "        # Update background to time k+1, to be used in next time step\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        bk_series[k+1] = bk_vari\n",
    "        bkvec_series[k+1] = bkvec\n",
    "        \n",
    "        # Lastly, compute activity of IBCM neurons at next time step, k+1, \n",
    "        # with the updated background and synaptic weight vector m\n",
    "        # Compute un-inhibited activity of each neuron with current input (at time k)\n",
    "        c = m.dot(bkvec)\n",
    "        c_series[k+1] = c\n",
    "        cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "        # np.sum(c) is a scalar and c a vector, so it broadcasts properly. \n",
    "        cbar_series[k+1] = cbar  # Save activity of neurons at time k+1\n",
    "        \n",
    "        # IF we hit some nan\n",
    "        if np.any(np.isnan(cbar)):\n",
    "            print(\"Last two m values, neuron 0:\", m_series[k, 0], m_series[k+1, 0])\n",
    "            print(\"Last threshold value:\", cbar2_avg)\n",
    "            print(\"Last two cbar values:\", cbar_series[k], cbar_series[k+1])\n",
    "            print(\"Last two c values:\", c_series[k], c_series[k+1])\n",
    "            raise ValueError(\"Hit infinite values for some reason at iteration \" + str(k))\n",
    "        \n",
    "    return tseries, m_series, bk_series, c_series, cbar_series, bkvec_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluctuating mixture\n",
    "To have multiple input components fluctuating realistically, we write the input as\n",
    "$$ \\vec{x}(t) = \\sum_{\\alpha=1}^K \\nu_\\alpha \\vec{x}_\\alpha $$\n",
    "where the $\\nu_\\alpha$ are random variables with some correlation, to mimick the turbulent flow that carries all odorants together. Ideally, they would follow the distributions derived in (Celani, Villermaux and Vergassola 2014), but those are a bit tricky to simulate. A good approximation would be a multivariate log-normal distribution with a strong correlation between components; that's easy to simulate with a multivariate linear Fokker-Planck equation, converted to a Langevin equation for numerical purposes. \n",
    "\n",
    "### General case of the Ornstein-Uhlenbeck process\n",
    "The multivariate Langevin equation for the Ornstein-Uhlenbeck process is:\n",
    "\n",
    "$$ d\\vec{x} = -A \\vec{x}(t) dt + B dW(t) $$\n",
    "\n",
    "where $\\frac{dW}{dt} = \\vec{\\eta}(t)$, a vector of gaussian white noise (independent components), $A$ and $B$ are matrices. Assume the matrix $A$ is normal and can be diagonalized as $A = U D u^\\dagger$, $D = \\mathrm{diag}(\\lambda^1, ..., \\lambda^n)$. For a deterministic initial condition $\\vec{x}(t_0) = \\vec{x}_0$, the general solution is that $\\vec{x}(t)$ follows a multivariate normal distribution, with mean and variance given by\n",
    "\n",
    "$$ \\langle \\vec{x}(t) \\rangle = U\\mathrm{e}^{-D(t-t_0)}U^{\\dagger} \\vec{x}_0 $$\n",
    "$$ \\langle \\vec{x}(t) \\vec{x}(t)^T \\rangle = U J(t, t_0) U^\\dagger $$\n",
    "\n",
    "where the components of $J$ are \n",
    "\n",
    "$$ J^{ij}(t, t_0) = \\left(\\frac{U^\\dagger B B^T U}{\\lambda^i + \\lambda^j} \\right)^{ij} \\left(1 - e^{-(\\lambda^i + \\lambda^j)(t - t_0)}  \\right) $$\n",
    "\n",
    "\n",
    "The stationary distribution of $\\vec{x}$ is\n",
    "\n",
    "$$ \\vec{x}^* \\sim \\mathcal{N} \\left(\\vec{0}, U^{ik} \\left(\\frac{B B^T}{\\lambda^k + \\lambda^l} \\right)^{kl} (U^\\dagger)^{lj} \\right) \\,\\, .$$\n",
    "\n",
    "### Exact numerical simulation, general case\n",
    "To simulate a realization of this process exactly, we use a trick suggested by Gillespie in the univariate case (which only works for the Ornstein-Uhlenbeck process because it's linear and gaussian). We iteratively take $\\vec{x}(t)$ as the initial condition of the evolution up to $\\vec{x}(t + \\Delta t)$, the distribution of which is\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) \\sim \\mathcal{N}\\left( U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) , U J(t + \\Delta t, t) U^\\dagger \\right) $$\n",
    "\n",
    "which can be rewritten using the following property of multivariate normal distributions: if $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$, then $\\vec{x} = \\vec{\\mu} + \\Psi \\vec{n} \\sim \\mathcal{N}(\\vec{\\mu}, \\Psi \\Psi^T)$ ($\\Psi$ is the Cholesky decomposition of the desired covariance matrix). This property is easily demonstrated by computing $\\langle \\vec{x} \\rangle$ and $\\langle \\vec{x} \\vec{x}^T \\rangle$ and using the linearity of multivariate normal distributions. For our update rule, this gives\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) + \\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right] \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n}$ is a vector of standard normal(0, 1) samples. The matrices $U e^{-D \\Delta t}U^\\dagger$ and $\\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right]$ can be computed only once and applied repeatedly to the $\\vec{x}(t)$ obtained in sequence and the $\\vec{n}$ drawn at each iteration. The Cholesky decomposition of $UJU^\\dagger$ is not obviously expressed in terms of $B$, because the possibly different $\\lambda^i$ values mix up components. \n",
    "\n",
    "\n",
    "### Simple case and exact simulation of it\n",
    "If $A$ is diagonal, the $U$ matrices are just identity matrices and disappear, but the Cholesky decomposition of $J(t + \\Delta t, t)$ is still not obvious. More explicit expressions can be obtained in the simplifying case where $A$ is proportional to the identity matrix, i.e., all components of $\\vec{x}$ have the same fluctuation time scale. \n",
    "\n",
    "Let's say that $A =  \\frac{1}{\\tau} \\mathbb{1}$, where $\\tau$ is the fluctuation time scale ($\\lambda^i = \\tau \\,\\, \\forall i$). Then, the matrix $J$ simplifies to \n",
    "\n",
    "$$J(t, t_0) = \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  $$\n",
    "\n",
    "and its Cholesky decomposition is simply $\\sqrt{\\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right) } B$. Hence, the distribution of $\\vec{x}(t)$ at any time since $t_0$ (deterministic initial condition $\\vec{x}_0$) is\n",
    "\n",
    "$$ \\vec{x}(t) \\sim \\mathcal{N} \\left(e^{-(t-t_0)/\\tau} \\vec{x}_0, \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  \\right) $$\n",
    "\n",
    "The stationary distribution is simply the above with the exponential factors set to 0. The update rule from $\\vec{x}(t)$ to $\\vec{x}(t + \\Delta t)$ to simulate a realization of the process is nicer as well:\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = e^{-\\Delta t / \\tau} \\vec{x}(t) + \\sqrt{\\frac{\\tau}{2} \\left(1 - e^{-2\\Delta t/\\tau}  \\right)} B \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$ is a vector of independent standard normal samples.\n",
    "\n",
    "As before, we can compute once the (scalar) factor $e^{-\\Delta t / \\tau}$ and the . This is exact for any $\\Delta t$, there is no increase in accuracy by decreasing $\\Delta t$. You just choose the $\\Delta t$ resolution at which you want to sample the realization of the process. \n",
    "\n",
    "### Symmetric choices for correlations\n",
    "We want all pairs of $\\nu_\\alpha$ to have the same correlation. More specifically, we want to force a Pearson correlation coefficient of $0 < \\rho < 1$ between any pair of $\\nu$s. We suppose all background components have the same individual variance $\\sigma^2$. The corresponding covariance matrix we want for the steady-state distribution is\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & \\rho & \\ldots & \\rho \\\\\n",
    "    \\rho & 1 & \\ldots & \\rho \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    \\rho & \\rho & \\ldots & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we apply Cholesky decomposition to get $\\Sigma = \\Psi \\Psi^T$, then $\\sqrt{\\tau/2} B = \\Psi$, since the steady-state covariance of the Ornstein-Uhlenbeck process is, in this simplified case, $\\frac{\\tau}{2} BB^T$. The $M_B$ coefficient in the update rule is then\n",
    "\n",
    "$$ M_B = \\sqrt{\\tau/2(1 - e^{-2 \\Delta t/\\tau})}B = \\sqrt{(1 - e^{-2 \\Delta t/\\tau})} \\Psi $$\n",
    "\n",
    "The other coefficient is just\n",
    "\n",
    "$$ M_A = e^{-\\Delta t / \\tau} \\mathbb{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the fluctuating background variable\n",
    "def update_ou_kinputs(nu_bk, params_bk, noises, dt):\n",
    "    \"\"\" \n",
    "    Update a background made of L odorants with concentrations combined linearly:\n",
    "        x(t) = \\sum_{\\alpha} \\nu_{\\alpha} x_{\\alpha}\n",
    "    The concentrations nu_alpha fluctuate according to a multivariate\n",
    "    Ornstein-Uhlenbeck process with some correlation between them. The general update rule is of the form\n",
    "        \\vec{\\nu}(t + dt) = M_A \\vec{\\nu}(t) + M_B \\vec{n}\n",
    "    where M_A and M_B are related to the matrix coefficients A and B in the Langevin equation\n",
    "    as indicated above (exp factors and Cholesky decomposition), and \\vec{n} is a vector of \n",
    "    independent normal(0, 1) samples. \n",
    "    This function is agnostic to A and B and just requires the overall matrix coefficients \n",
    "    in front of of x(t) and \\vec{n} in the update rule. \n",
    "    \n",
    "    Args:\n",
    "        nu_bk (np.ndarray): array of length K, containing concentrations nu_i \n",
    "            of odorants in the background\n",
    "        params_bk (list of arrays): mateA, matJB, vecs_nu, means_nu: matrices of shape (K, K)\n",
    "            involved in the update rule and matrix of background component vectors x_{\\alpha}, shape (K, d)\n",
    "            where d is the number of dimensions. Each row is a different component. \n",
    "            Also a 1d array of the mean values of the nu variables. They are simulated with zero mean\n",
    "            and their average is added before the background is computed\n",
    "        noises: 1d array of pre-generated normal(0, 1) samples, one per component in nu_bk\n",
    "    \"\"\"\n",
    "    mateA, matJB, vecs_nu, means_nu = params_bk\n",
    "    nu_bk_new = np.dot(mateA, nu_bk) + np.dot(matJB, noises)\n",
    "    # Update background vector, putting back the mean value of the nu's\n",
    "    bkvec = np.squeeze(np.dot((nu_bk+means_nu)[np.newaxis, :], vecs_nu))\n",
    "    \n",
    "    return bkvec, nu_bk_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simulation\n",
    "With the parameter defined as global variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 3\n",
    "n_components = 3\n",
    "n_neurons = 32\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.0025\n",
    "tau_avg = 150\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=92347287)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant\n",
    "back_components = 0.1*np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8\n",
    "    else:\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = np.squeeze(averages_nu[np.newaxis, :].dot(back_components))\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.3\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "# The sqrt(tau/2) is already included in the Cholesky decomposition\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_res = integrate_ibcm_network(init_synapses, update_ou_kinputs, init_back_list, back_params, duration, deltat, \n",
    "                       learnrate, seed=98743877, noisetype=\"normal\", tavg=tau_avg, coupling=coupling_eta)\n",
    "\n",
    "t_ser, m_ser, bk_ser, c_ser, cbar_ser, bkvec_ser = sim_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.isfinite(c_ser), axis=0)\n",
    "np.argmin(np.isfinite(cbar_ser), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, check that the $\\nu$s have a multivariate normal distribution\n",
    "with the proper parameters (covariance matrix and zero mean â€“ remember they are simulated with zero mean). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "empirical_mean = np.mean(bk_ser, axis=0)\n",
    "print(\"Mean nu vector:\", empirical_mean)\n",
    "print(\"Expected:\", np.zeros(n_components))\n",
    "\n",
    "empirical_covmat, cov_error = estimate_empirical_covariance(bk_ser[1000:], do_variance=True)\n",
    "print(\"Empirical nu covariance matrix:\\n\", empirical_covmat)\n",
    "print(\"Expected:\\n\", steady_covmat)\n",
    "print(\"Expected error:\\n\", np.sqrt(cov_error))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(bk_ser[:, 0], bk_ser[:, 1], bk_ser[:, 2], alpha=0.1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looked fine when I tested it, no need to check anymore. \n",
    "\n",
    "### Second, check the trajectories of $\\vec{m}$ vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 15000\n",
    "skp = 500\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "ax.scatter(m_ser[trst::skp, 0, 0], m_ser[trst::skp, 0, 1], m_ser[trst::skp, 0, 2], alpha=0.5, color=\"r\")\n",
    "ax.scatter(m_ser[trst::skp, 1, 0], m_ser[trst::skp, 1, 1], m_ser[trst::skp, 1, 2], alpha=0.5, color=\"b\")\n",
    "ax.scatter(m_ser[trst::skp, 2, 0], m_ser[trst::skp, 2, 1], m_ser[trst::skp, 2, 2], alpha=0.5, color=\"k\")\n",
    "ax.view_init(azim=45, elev=30)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 150\n",
    "skp = 2000\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(m_ser[trst::skp, i, 0], m_ser[trst::skp, i, 1], m_ser[trst::skp, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_dimensions, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs.T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=45, elev=30)    \n",
    "ax.set(xlabel=\"Synapse 1 strength\", \n",
    "      ylabel=\"Synapse 2 strength\", \n",
    "      zlabel=\"Synapse 3 strength\")\n",
    "# ax.view_init(azim=45, elev=140)\n",
    "# ax.legend()\n",
    "#fig.savefig(\"figures/three_odors/neurones_ibcm_fond_gaussien_3_odeurs.png\", \n",
    "#            transparent=True, bbox_inches=\"tight\", dpi=400)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, sharex=True)\n",
    "axes = axes.flatten()\n",
    "# Plot the time course of cbar for each neuron, and the time course of each nu\n",
    "tsli = slice(13000, 13100)\n",
    "for i, clr in enumerate([\"r\", \"b\", \"k\"]):\n",
    "    axes[0].plot(t_ser[tsli], cbar_ser[tsli, i], color=clr)\n",
    "axes[0].set(xlabel=\"Time\", ylabel=r\"$\\bar{c}$\")\n",
    "\n",
    "for i, clr in enumerate([\"pink\", \"cyan\", \"grey\"]):\n",
    "    axes[1].plot(t_ser[tsli], bk_ser[tsli, i], color=clr)\n",
    "axes[1].set(xlabel=\"Time\", ylabel=r\"$\\nu$\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 10000\n",
    "skp = 100\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "bkvec_ser = np.dot(bk_ser+averages_nu[np.newaxis, :], back_components)\n",
    "ax.scatter(bkvec_ser[trst::skp, 0], bkvec_ser[trst::skp, 1], bkvec_ser[trst::skp, 2], \n",
    "           alpha=0.9, color=\"xkcd:orange\")\n",
    "ax.view_init(azim=5, elev=30)\n",
    "plt.show()\n",
    "plt.close()\n",
    "del bkvec_ser  # save some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series of each component of each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series of the projection of each neuron on each component. \n",
    "# One plot per component. \n",
    "fig, axes = plt.subplots(n_components, sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "skp = 100\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_components):\n",
    "    for j in range(n_neurons):\n",
    "        m_comp_ser = m_ser[:, j].dot(back_components[i])\n",
    "        axes[i].plot(t_ser[::skp], m_comp_ser[::skp], label=\"Neuron {}\".format(j), color=colors[j], alpha=0.8)\n",
    "    axes[i].set_ylabel(r\"$\\vec{m} \\cdot \\vec{x}_{\"+str(i)+\"}$\")\n",
    "    axes[i].axhline(0, ls=\"--\", color=\"grey\")\n",
    "axes[-1].set_xlabel(\"Time (-)\")\n",
    "#axes[0].legend(ncol=n_neurons//4, fontsize=8, loc=\"upper left\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle between average vector and neurons' m\n",
    "# Suspecting this is roughly constant: \n",
    "# m steady-states fall on a cone at one standard deviation away from the average\n",
    "transient = 40000\n",
    "steady_m = np.mean(m_ser[transient:], axis=0)\n",
    "mean_back = np.sum(back_components, axis=0)/n_components\n",
    "cosines = steady_m.dot(mean_back) / np.sqrt(np.sum(steady_m**2, axis=1) * np.sum(mean_back**2))\n",
    "print(cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of angles in the plane perpendicular to steady_m\n",
    "steady_m_perpendicular = steady_m - (steady_m.dot(mean_back) / np.sum(mean_back**2)).reshape(-1, 1) * mean_back.reshape(1, 3)\n",
    "# Use two of the three components as orthogonal basis vectors in that plane\n",
    "ref_vec = back_components[0] - back_components[0].dot(mean_back) / np.sum(mean_back**2) * mean_back\n",
    "ref_vec2 = back_components[1] - back_components[1].dot(mean_back) / np.sum(mean_back**2) * mean_back\n",
    "ref_vec2 = ref_vec2 - ref_vec.dot(ref_vec2)/np.sum(ref_vec**2) * ref_vec\n",
    "cosphi = steady_m_perpendicular.dot(ref_vec) / np.sqrt(np.sum(ref_vec**2)*np.sum(steady_m_perpendicular**2, axis=1))\n",
    "sinphi = steady_m_perpendicular.dot(ref_vec2) / np.sqrt(np.sum(ref_vec2**2)*np.sum(steady_m_perpendicular**2, axis=1))\n",
    "phi = np.arctan2(cosphi, sinphi) / (np.pi)\n",
    "plt.hist(phi)\n",
    "plt.xlabel(r\"Azimuthal angle in perpendicular plane ($\\pi$)\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical fixed point prediction for IBCM model\n",
    "Calculations predicting a $K-2$ dimensional fixed point when $K$ background components. The two constraints satisfied at the fixed points are, in the $\\rho = 0$ case, which is just a linear transformation away from the more general case (by diagonalization of the covariance matrix):\n",
    "$$ \\sum_{\\alpha} \\bar{c}_{\\alpha} = \\frac{1}{\\langle \\nu \\rangle} $$\n",
    "$$ \\sum_{\\alpha} \\bar{c}_{\\alpha}^2 = \\frac{1}{\\sigma^2} $$\n",
    "where\n",
    "$$ \\bar{c}_{i, \\alpha} = \\vec{m}_{i} \\cdot \\vec{x}_{\\alpha} - \\eta \\sum_{j \\neq i} \\vec{m}_j \\cdot \\vec{x}_{\\alpha} $$\n",
    "\n",
    "Hence, if I run a simulation with $\\rho = 0$, or transform another simulation to eigencvector coordinates, I should find that every neuron at a fixed point satisfies these two equations. Let's try that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute the update matrices for nu when rho=0\n",
    "correl_rho = 0.0\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]\n",
    "# m_init, update_bk, bk_init, inhib_params, bk_params, tmax, dt, learnrate, seed=14345124, noisetype=\"normal\", tavg=10, coupling=0.1\n",
    "\n",
    "\n",
    "sim_results = integrate_ibcm_network(init_synapses, update_ou_kinputs, init_back_list,\n",
    "                    back_params, duration, deltat, learnrate=learnrate, seed=75321653, \n",
    "                    noisetype=\"normal\", tavg=tau_avg, coupling=coupling_eta)\n",
    "tser, mser, nuser, cser, cbarser, bkvecser = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cbars in response to each component.\n",
    "transient = 80000\n",
    "mean_cs_alpha = np.mean(mser[transient:], axis=0).dot(back_components.T)\n",
    "mean_cbars_alpha = mean_cs_alpha*(1 + coupling_eta) - coupling_eta*np.sum(mean_cs_alpha, axis=0).reshape(1, -1)\n",
    "\n",
    "# Average(cbar^2): need to compute series of cbars, then square, then average. \n",
    "cs_alpha = mser[transient:].dot(back_components.T)\n",
    "cbars_alpha = cs_alpha*(1 + coupling_eta) - coupling_eta*np.sum(cs_alpha, axis=1, keepdims=True)\n",
    "mean_cbars_alpha2 = np.mean(cbars_alpha**2, axis=0)\n",
    "\n",
    "# Constaint 1: sum of c_alphas for each neuron\n",
    "print(\"This should be all close to ones:\", mean_cbars_alpha.sum(axis=1) * averages_nu.mean())\n",
    "# Constraint 2: sum of c_alphas^2 for each neuron, copmared to 1/sigma^2\n",
    "print(\"This should be all close to ones:\",  1.0/sigma2/np.sum(mean_cbars_alpha**2, axis=1))\n",
    "# Compare to the average value of squared cbars, rather than the average of cbar, squared\n",
    "# print(\"This should be all zeros:\", np.sum(mean_cbars_alpha2, axis=1) - 1.0/sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cd = np.sum(mean_cbars_alpha, axis=1) * averages_nu.mean()\n",
    "mean_cd2 = np.mean(np.sum(cbars_alpha* averages_nu.mean(), axis=2)**2, axis=0)\n",
    "print(\"This should be all ones:\",  (2*mean_cd-mean_cd2)/sigma2/np.sum(mean_cbars_alpha**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating inputs\n",
    "The case of a fluctuating mixture is surprising: there seems to be a continuum of fixed points at a constant radius and a specific angle with respect to the average vector. \n",
    "Maybe this degeneracy would be lifted if we gave different variances to the different components, or we did not dispose them symmetrically around the mean vector. Anyways, let's try these later. \n",
    "\n",
    "For now, check that I can recover the well-known case of alternating inputs: only $N$ stable fixed points should exist when there are $N$ components (here $N=3$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alternating_inputs(idx_bk, params_bk, noises, dt):\n",
    "    \"\"\" Select randomly the next background input. \n",
    "    Args:\n",
    "        nu_bk (np.ndarray): array of length k-1, containing proportions nu_i of odorants\n",
    "        params_bk (list):  Contains the following parameters       \n",
    "            cumul_probs (np.ndarray): cumulative probabilities up to the kth input vector. \n",
    "            vecs (np.ndarray): 2d array where each row is one of the possible input vectors\n",
    "        noises (np.1darray): pre-generated uniform(0, 1) samples, in an array of length 1, \n",
    "            to choose next input vector. \n",
    "        \"\"\"\n",
    "    # Index of the next input\n",
    "    cumul_probs, vecs = params_bk\n",
    "    idx = np.argmax(cumul_probs > noises[0])\n",
    "    return vecs[idx], np.asarray([idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]\n",
    "sim_res = integrate_ibcm_network(init_synapses, update_alternating_inputs, init_back_altern, \n",
    "                                    back_params_altern, duration, deltat, learnrate, seed=98743877, \n",
    "                                    noisetype=\"uniform\", tavg=tau_avg, coupling=coupling_eta)\n",
    "\n",
    "t_ser_alt, m_ser_alt, bk_ser_alt, c_ser_alt, cbar_ser_alt, bkvec_ser_alt = sim_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 15000\n",
    "skp = 500\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(m_ser_alt[trst::skp, i, 0], m_ser_alt[trst::skp, i, 1], m_ser_alt[trst::skp, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_dimensions, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs.T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=15, elev=30)\n",
    "# ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The alternating inputs case works just fine! \n",
    "It's really the continuous character of the fluctuations in my linear mixture model,  which are effectively gaussian around the mean vector, that creates a ring of fixed points around that mean vector. If the distribution was multi-modal, I'm pretty sure the model would only have discrete fixed points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark on stability of the steady-state and oscillations\n",
    "When $\\tau_{\\Theta}$ is too close to $\\frac{1}{\\mu}$, the learning time scale, the stable fixed points become unstable via a Hopf bifurcation (Udeigwe, ..., Ermentrout 2017), and oscillations around those foci appear. If $\\tau_{\\Theta}$ is further increased, the limit cycle also disappears and the solutions just escape to infinity. This result is known for the alternating input case. \n",
    "\n",
    "We also see this with our gaussian process for the input. Try the following sequence of parameter values to witness the appearance of small oscillations, then large oscillations, and finally instability. \n",
    "``` \n",
    "averages_nu = np.ones(n_components)\n",
    "learnrate = 0.0025; \n",
    "tau_avg = 100, 120, 150, 200  # 100 is stable, 120 small oscillations, 150 large, 200 unstable\n",
    "```\n",
    "The position of the bifurcations as a function of $\\tau_{\\Theta}$ also depends on the magnitude of the input fluctuations. A larger $\\tau_{\\Theta}$ value can still lead to non-oscillatory solutions if the input fluctuations are smaller, controlled by e.g. setting `averages_nu = np.ones(n_components) / np.sqrt(n_components)`. \n",
    "\n",
    "I did not explore this phenomenon much more for the moment; the lesson is that if oscillations or divergences appear in the simulations, they can be fixed by decreasing the learning rate (which makes the model convergence slower, though) or decreasing $\\tau_{\\Theta}$ (which increases the noise around the steady-state, though). Hopefully, that will never become too problematic -- although, note that we are already in a regime where the model is super slow compared to background fluctuations, which is worrying me about the realism of that model. But I don't think any model could do better without sampling the background statistics enough.  Also, note that by initializing the neurons closer to a proper inhibitory state, the convergence could be faster. It might also be faster in higher dimension, because many dimensions remain zero? Not sure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-gaussian unimodal distribution\n",
    "If there is a discrete number of fixed points when the $\\nu_{\\alpha}$ have a distribution with non-zero third moment, there should be a transition from a continuum of fixed points to this discrete case as we increase a parameter $\\epsilon = \\langle (\\nu - \\langle\\nu\\rangle)^3 \\rangle$ above zero. Will it be a first or second order phase transition? We will see. \n",
    "\n",
    "My guess is that there is no transition as long as the distribution is unimodal, and then as soon as there are multiple maxima in the pdf of $\\nu$, then only discrete fixed points exist. We'll see what happens with a unimodal distribution of non-zero central third moment. \n",
    "\n",
    "To interpolate with a parameter $\\epsilon$ from a gaussian distribution to a distribution with non-zero central third moment, we could perturb the generating functional of the Ornstein-Uhlenbeck process, however it then becomes hard to return to the stochastic differential equation, and we lose the exact update rule we had for a gaussian process (hence we run in numerical accuracy issues because of a large dt). \n",
    "\n",
    "Instead, the trick is to simulate $x$ as an Ornstein-Uhlenbeck process with zero mean, then take\n",
    "$$ \\nu = s + x + \\epsilon x^2 $$\n",
    "or, in the multivariate case, \n",
    "$$ \\vec{\\nu} = \\vec{s} + \\vec{x} + \\epsilon \\mathrm{diag}(\\vec{x}) \\vec{x} $$\n",
    "\n",
    "If there are no correlations, we can treat each component $\\nu_{\\alpha}$ as a univariate case, and we then have a third moment of order $\\epsilon$, with only lower-order corrections to the second moment and order $\\epsilon$ corrections to the desired mean $s$ (which we could correct preemptively):\n",
    "\n",
    "$$ \\langle \\nu \\rangle = s + \\epsilon \\sigma^2 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^2 \\rangle = \\sigma^2 + 2 \\epsilon^2 \\sigma^4 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^3 \\rangle = 6 \\epsilon \\sigma^4 + 8 \\epsilon^3 \\sigma^6 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the fluctuating background variable\n",
    "def update_thirdmoment_kinputs(x_bk, params_bk, noises, dt):\n",
    "    \"\"\" \n",
    "    Update a background made of L odorants with concentrations combined linearly:\n",
    "        x(t) = \\sum_{\\alpha} \\nu_{\\alpha} x_{\\alpha}\n",
    "    The concentrations nu_alpha are given by\n",
    "        \\nu_\\alpha = s_\\alpha + x_\\alpha + \\epsilon x_\\alpha\n",
    "    where the $x_\\alpha$ fluctuate according to a multivariate\n",
    "    Ornstein-Uhlenbeck process with some correlation between them. The general update rule is of the form\n",
    "        \\vec{x}(t + dt) = M_A \\vec{x}(t) + M_B \\vec{n}\n",
    "    where M_A and M_B are related to the matrix coefficients A and B in the Langevin equation\n",
    "    as indicated above (exp factors and Cholesky decomposition), and \\vec{n} is a vector of \n",
    "    independent normal(0, 1) samples. \n",
    "    This function is agnostic to A and B and just requires the overall matrix coefficients \n",
    "    in front of of x(t) and \\vec{n} in the update rule. \n",
    "    \n",
    "    Args:\n",
    "        x_bk (np.ndarray): array of length K, containing normal processes x_\\alpha underlying\n",
    "            the concentrations \\nu_\\alpha of odorants in the background\n",
    "        params_bk (list of arrays): mateA, matJB, vecs_nu, means_nu, epsil: matrices of shape (K, K)\n",
    "            involved in the update rule and matrix of background component vectors x_{\\alpha}, shape (K, d)\n",
    "            where d is the number of dimensions. Each row is a different component. \n",
    "            Also a 1d array of the mean values of the nu variables. They are simulated with zero mean\n",
    "            and their average is added before the background is computed. \n",
    "            epsil is the magnitude of the quadratic contribution to nu. \n",
    "        noises: 1d array of pre-generated normal(0, 1) samples, one per component in nu_bk\n",
    "    \"\"\"\n",
    "    # First, update gaussian x\n",
    "    mateA, matJB, vecs_nu, means_nu, epsil = params_bk\n",
    "    x_bk_new = np.dot(mateA, x_bk) + np.dot(matJB, noises)\n",
    "    # Then compute nu\n",
    "    nu_bk_new = means_nu + x_bk_new + epsil*x_bk_new*x_bk_new\n",
    "    # Update background vector, putting back the mean value of the nu's\n",
    "    bkvec = np.squeeze(np.dot(nu_bk_new[np.newaxis, :], vecs_nu))\n",
    "    \n",
    "    return bkvec, x_bk_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 3\n",
    "n_components = 3\n",
    "n_neurons = 32\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.0025\n",
    "tau_avg = 150\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=923487)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant\n",
    "back_components = 0.1*np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8\n",
    "    else:\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = np.squeeze(averages_nu[np.newaxis, :].dot(back_components))\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.3\n",
    "epsilon_nu = 0.2\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params_3 = [update_mat_A, update_mat_B, back_components, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_res = integrate_ibcm_network(init_synapses, update_thirdmoment_kinputs, init_back_list, back_params_3, \n",
    "            duration, deltat, learnrate, seed=98877742, noisetype=\"normal\", tavg=tau_avg, coupling=coupling_eta)\n",
    "\n",
    "t_ser, m_ser, bk_ser, c_ser, cbar_ser, bkvec_ser = sim_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 150\n",
    "skp = 1000\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(m_ser[trst::skp, i, 0], m_ser[trst::skp, i, 1], m_ser[trst::skp, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_dimensions, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs.T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=45, elev=30)\n",
    "ax.set(xlabel=\"Synapse 1 strength\", \n",
    "      ylabel=\"Synapse 2 strength\", \n",
    "      zlabel=\"Synapse 3 strength\")\n",
    "# ax.view_init(azim=45, elev=140)\n",
    "# ax.legend()\n",
    "#fig.savefig(\"figures/three_odors/neurones_ibcm_fond_non-gaussien_3_odeurs.png\", \n",
    "#            transparent=True, bbox_inches=\"tight\", dpi=400)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
