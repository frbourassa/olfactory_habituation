{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation with PCA neurons in the inhibitory layer\n",
    "The details of the model are described in other Jupyter notebooks (e.g., biopca_inhibition_three_components.ipynb). The ultimate goal here is to include this model in the full olfactory network down to Kenyon cells, apply it to increasingly realistic olfactory backgrounds and estimate its performance at 1) inhibiting the fluctuating background, and 2) still recognizing new odors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_ifpsp_network_skip, \n",
    "    relu_inplace, \n",
    "    biopca_respond_new_odors as response_new_odors\n",
    ")\n",
    "\n",
    "# Offline PCA for comparison\n",
    "from utils.statistics import principal_component_analysis, seed_from_gen\n",
    "\n",
    "from modelfcts.backgrounds import (update_logou_kinputs, update_thirdmoment_kinputs, \n",
    "                                   decompose_nonorthogonal_basis, logof10,  update_alternating_inputs)\n",
    "from modelfcts.tagging import project_neural_tag, create_sparse_proj_mat\n",
    "from simulfcts.plotting import (plot_3d_series, plot_w_matrix, plot_m_matrix, plot_pca_results,\n",
    "                            plot_background_norm_inhibition, plot_background_neurons_inhibition)\n",
    "from utils.metrics import frobnorm, subspace_align_error, jaccard\n",
    "from modelfcts.checktools import compute_pca_meankept, compute_projector_series, analyze_pca_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Projection neuron layer\n",
    "First, just check what happens with a new odor at the projection neuron layer. I will try the slightly non-gaussian noise case and \n",
    " - a) symmetric components, \n",
    " - b) non-symmetric (random, exponential elements) components. \n",
    "\n",
    "I will try both low dimensions and high dimensions, in the non-symmetric case. A high dimension helps finding nearly-orthogonal backgrounds and new odors. \n",
    "\n",
    "### 1.a) Symmetric odor vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 4  \n",
    "# The larger the dimension, the more likely the odors are orthogonal. \n",
    "n_components = 3  # no need to look at super complicated odors for now; keep effective space 3D\n",
    "# Can actually look at this latent space by Gram-Schmidt to find orthogonal axes spanning the input odors. \n",
    "n_neurons = 3  # Start small\n",
    "\n",
    "# Simulation time scales\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "tau_nu = 2.0  # Correlation time scale of the background nu_gammas (same for all)\n",
    "learnrate = 0.0005  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "lambda_range = 0.2\n",
    "lambda_max = 5.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate = 2.0  #  / lambda_max**2 \n",
    "lambda_mat_diag = np.asarray([1.0 - lambda_range*k / (n_neurons - 1) for k in range(n_neurons)])\n",
    "lambda_mat_diag *= lambda_max\n",
    "biopca_rates = [learnrate, rel_lrate, lambda_max, lambda_range, learnrate]\n",
    "inhib_rates = [1e-4, 2e-5]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: as advised in Minden et al., 2018 \n",
    "rgen_meta = np.random.default_rng(seed=0x718e19927b12b8df4daaa66a2f0e6b76)\n",
    "init_mmat = rgen_meta.standard_normal(size=[n_neurons, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat = np.eye(n_neurons, n_neurons)  # Supposed to be near-identity, start as identity\n",
    "ml_inits = [init_mmat, init_lmat]\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant: [0.8, 0.2, 0.2], [0.2, 0.8, 0.2], etc.\n",
    "back_components = 0.2 * np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8\n",
    "    else:  # If there are more components than there are dimensions (ORNs)\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "epsilon_nu = 0.2\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "# The sqrt(tau/2) is already included in the Cholesky of the covariance matrix; if is if we\n",
    "# wanted the B matrix of the O-U equation itself that we should divide the Cholesky by that factor. \n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params_3 = [update_mat_A, update_mat_B, back_components, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(ml_inits, update_thirdmoment_kinputs, \n",
    "                        init_back_list, biopca_rates, inhib_rates, back_params_3, duration, \n",
    "                        deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser3, nuser3, bkvecser3, mser3, lser3, cbarser3, wser3, sser3 = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the time course of the different neurons"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check whether W is a bit like the pseudo-inverse of mbar\n",
    "mbarser3 = mser3 - coupling_eta * (np.sum(mser3, axis=1, keepdims=True) - mser3)\n",
    "transient = 100000\n",
    "mbar3_mean = np.mean(mbarser3[transient:], axis=0)\n",
    "w3_mean = np.mean(wser3[transient:], axis=0)\n",
    "pinv_mbar3_mean = np.linalg.pinv(mbar3_mean)\n",
    "\n",
    "print(w3_mean.dot(mbar3_mean.dot(bkvecser3[-1])))\n",
    "print(pinv_mbar3_mean.dot(mbar3_mean.dot(bkvecser3[-1])))\n",
    "print(bkvecser3[-1])\n",
    "# The Wc from my network is actually closer to the original background vector than the pseudo-inverse! \n",
    "# This works incredibly well. Too well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp = 100\n",
    "tini = 0\n",
    "tmx = int(duration)\n",
    "tslice = slice(tini, tmx, skp)\n",
    "fig, ax = plt.subplots()\n",
    "w1_palette = sns.color_palette(\"Blues\", n_colors=n_neurons)\n",
    "w2_palette = sns.color_palette(\"Purples\", n_colors=n_neurons)\n",
    "w3_palette = sns.color_palette(\"Greens\", n_colors=n_neurons)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 0], color=\"pink\", alpha=0.5)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 1], color=\"blue\", alpha=0.5)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 2], color=\"red\", alpha=0.8)\n",
    "#ax.plot(tser3[tslice], thetaser3[tslice, -1], color=\"orange\", alpha=0.8)\n",
    "for i in range(n_neurons-1):\n",
    "    ax.plot(tser3[tslice], mser3[tslice, i, 0], color=w1_palette[i], alpha=0.8)\n",
    "    ax.plot(tser3[tslice], mser3[tslice, i, 1], color=w2_palette[i], alpha=0.8)\n",
    "    ax.plot(tser3[tslice], mser3[tslice, i, 2], color=w3_palette[i], alpha=0.8)\n",
    "ax.plot(tser3[tslice], mser3[tslice, -1, 0], color=w1_palette[-1], label=\"Neuron Component 0\", alpha=0.8)\n",
    "ax.plot(tser3[tslice], mser3[tslice, -1, 1], color=w2_palette[-1], label=\"Neuron Component 1\", alpha=0.8)\n",
    "ax.plot(tser3[tslice], mser3[tslice, -1, 2], color=w3_palette[-1], label=\"Neuron Component 2\", alpha=0.8)\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Inhibition neurons components\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to offline PCA\n",
    "Check that the model is doing the best it can for non-gaussian inputs. \n",
    "\n",
    "Let $X$ be a matrix with each input sample in a column. According to Lemma 3 from Minden et al., 2018:\n",
    " - $L$ is diagonal with the $K$ first principal values, that is, the $K$ first eigenvalues of $XX^T$, on its diagonal\n",
    " - $\\hat{U}_K = \\Lambda^{-1} L^{-1} M$ is the learnt projector on the $K$ subspace: rows of $\\hat{U}_K$ are the first $K$ principal components. \n",
    " - Note that in the ifPSP algorithm, the Taylor series approximation $L^{-1} = L_d^{-1} - L_d^{-1} L_o L_d^{-1}$ is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser3, mser3, lser3, lambda_mat_diag)\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser3, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the inhibitory neurons' weights $\\vec{w}_i$\n",
    "Analytically, I find that, on average, $\\vec{w}_i$ converges to $\\vec{x}(\\pm \\sigma)$, i.e. to either input vector one standard deviation away from the mean input. So, here, I compare the numerical results for $\\vec{w}$ to the possible fixed points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the time course of the dot products -- not interesting with gaussian degeneracy\n",
    "# Unclear what it shows. \n",
    "fig, axes = plot_w_matrix(tser3, wser3, skp=20, lw=1.5)\n",
    "        \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(tser3, bkvecser3, sser3, skp=10)\n",
    "skp_sym = 1\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bknorm_ser[transient:])\n",
    "avg_snorm = np.mean(snorm_ser[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser[transient:])\n",
    "std_snorm = np.std(snorm_ser[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser3, bkvecser3, sser3, skp=10)\n",
    "axes[-1].legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.6), fontsize=8, handlelength=1.5)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp_sym\n",
    "avg_bknorm = np.mean(bkvecser3[transient:])\n",
    "avg_snorm = np.mean(sser3[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bkvecser3[transient:])\n",
    "std_snorm = np.std(sser3[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity of a projection neuron reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of a projection neuron's activity reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a few points for each neuron\n",
    "trst = 5000\n",
    "skp = 500\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(mser3[trst::skp, i, 0], mser3[trst::skp, i, 1], mser3[trst::skp, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_dimensions, n_components])\n",
    "scale = 0.3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig[:3], *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=45, elev=30)\n",
    "# ax.view_init(azim=45, elev=140)\n",
    "# ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectorser3 = compute_projector_series(mser3, lser3)\n",
    "projectorser3 = np.einsum(\"...ij,...jk\", wser3, projectorser3)\n",
    "projectionser3 = np.einsum(\"...ij,...j\", projectorser3, bkvecser3)\n",
    "subtracted_series = bkvecser3 - projectionser3\n",
    "\n",
    "fig, axes = plt.subplots(2, n_dimensions // 2 + min(1, n_dimensions % 2), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_dimensions):\n",
    "    axes[i].plot(tser3/1000, subtracted_series[:, i], color=\"k\")\n",
    "    axes[i].set(xlabel=\"Time (x1000 steps)\", \n",
    "                ylabel=r\"$\\vec{x} - LM\\vec{x}$, \" + \"ORN {:d}\".format(i))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response to a new odor\n",
    "This part of the code only runs if the simulation above had ``n_dimensions > n_components``. \n",
    "\n",
    "The goal is to see whether a new odor, not linearly dependent of the ones in the background, also gets repressed close to zero, or produces an inhibited output noticeably different from the inhibited background, and still similar to the new odor vector, at least its component perpendicular to the background subspace. \n",
    "\n",
    "Need to test for many samples from the background odor distribution. Keep the new odor at a constant concentration, typical of the concentration at which we actually want the system to pick up the new odor. \n",
    "\n",
    "I realize that it's fine if the disentanglement of odors isn't perfect at the PN layer: besides the question of habituation, the sparse tag network proposed by Dasgupta does not address too well how multiple odors are disentangled from a complicated mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_new_odors(odors, typical_l, typical_m, typical_w):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        odors (np.ndarray): indexed [..., n_orn] \n",
    "            so can take dot product properly with m and store many \n",
    "            odors along arbitrary other axes.\n",
    "        typical_m (np.ndarray): indexed [n_neurons, n_orn]\n",
    "        typical_w (np.ndarray): indexed [n_orn, n_neurons]\n",
    "    \"\"\"\n",
    "    # Compute L_d inverse and L_o\n",
    "    inv_l_diag = 1.0 / np.diagonal(typical_l)\n",
    "    l_off = typical_l - np.diagflat(1.0 / inv_l_diag)\n",
    "    \n",
    "    # Compute activation of neurons\n",
    "    c = inv_l_diag * (odors.dot(typical_m.T))\n",
    "    # Lateral inhibition between neurons\n",
    "    cbar = c - inv_l_diag*c.dot((typical_l - np.diagflat(1.0/inv_l_diag)).T)\n",
    "    # Compute output after inhibition\n",
    "    new_output = relu_inplace(odors - cbar.dot(typical_w.T))  # s = x - Wc\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, biopca inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import sample_background_thirdmoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of improvement of recognition\n",
    "# New odor\n",
    "new_odor = np.roll(back_components[0], shift=-1)  # Should be a new vector\n",
    "\n",
    "# Background samples, then add new odor\n",
    "mix_samples = sample_background_thirdmoment(averages_nu, steady_covmat, epsilon_nu, back_components, \n",
    "                                                  size=1000, rgen=rgen_meta)\n",
    "mix_frac = 0.5\n",
    "mix_samples += mix_frac*new_odor.reshape(1, -1)\n",
    "\n",
    "# Compare to inhibition of the average background\n",
    "avg_back = averages_nu.dot(back_components)\n",
    "a_over_2ab = inhib_rates[0] / (sum(inhib_rates) + inhib_rates[0])\n",
    "inhib_avg_samples = mix_samples - a_over_2ab * avg_back.reshape(1, -1)\n",
    "\n",
    "# Average m and w with which we will inhibit\n",
    "l3_mean = np.mean(lser3[transient:], axis=0)\n",
    "m3_mean = np.mean(mser3[transient:], axis=0)\n",
    "w3_mean = np.mean(wser3[transient:], axis=0)\n",
    "\n",
    "# Inhibition of each generated sample and statistics on performance\n",
    "inhib_biopca_samples = respond_new_odors(mix_samples, l3_mean, m3_mean, w3_mean)\n",
    "\n",
    "dist_pure_inhib_none = distance_panel_target(mix_samples, mix_frac*new_odor)\n",
    "dist_pure_inhib_avg = distance_panel_target(inhib_avg_samples, mix_frac*new_odor)\n",
    "dist_pure_inhib_biopca = distance_panel_target(inhib_biopca_samples, mix_frac*new_odor)\n",
    "\n",
    "median_distances_none = np.median(dist_pure_inhib_none, axis=0)\n",
    "median_distances_avg = np.median(dist_pure_inhib_avg, axis=0)\n",
    "median_distances_biopca = np.median(dist_pure_inhib_biopca, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_biopca = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none[i], color=clr_none, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_avg[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "    ax.axvline(median_distances_avg[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_biopca[:, i], label=\"biopca inhibition\", facecolor=clr_biopca, alpha=0.6, \n",
    "        edgecolor=clr_biopca, density=True) \n",
    "    ax.axvline(median_distances_biopca[i], color=clr_biopca, ls=\"--\", lw=1.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric to measure the quality of the inhibition\n",
    "Distance to new odor alone? Compare to un-inhibited mixture?\n",
    "\n",
    "Ultimately, will compute sparse binary neural tag and compare with Jaccard metric, but for now, I want to avoid this complication, which requires using many more dimensions than 4. I might keep this for a separate notebook (or even C code if it seems to work well). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Random odor vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic model of olfactory receptor activation patterns:\n",
    "# each component is i.i.d. exponential\n",
    "def generate_odorant(n_rec, rgen, lambda_in=0.1):\n",
    "    \"\"\" Generate vectors eta and kappa^-1 for an odorant, with antagonism parameter rho. \n",
    "    \n",
    "    Args:\n",
    "        n_rec (int): number of receptor types, length of vectors\n",
    "        rgen (np.random.Generator): random generate (numpy >= 1.17)\n",
    "        lambda_in (float): lambda parameter of the exp distribution\n",
    "            Equals the inverse of the average of each vector component\n",
    "    Returns:\n",
    "        kappa1_vec (np.ndarray): 1d vector of receptor activities\n",
    "    \"\"\"\n",
    "    return rgen.exponential(scale=1.0/lambda_in, size=n_rec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 3  # Half the real number for faster simulations\n",
    "# The larger the dimension, the more likely the odors are orthogonal. \n",
    "n_components = 3  # no need to look at super complicated odors for now; keep effective space 3D\n",
    "# Can actually look at this latent space by Gram-Schmidt to find orthogonal axes spanning the input odors. \n",
    "n_neurons = 16  # Start small\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.001\n",
    "tau_avg = 200\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=92387)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Choose random exponential LI vectors\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=1.0)\n",
    "print(back_components)\n",
    "\n",
    "\n",
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# m_init, update_bk, bk_init, inhib_params, bk_params, tmax, dt, learnrate, seed=14345124, noisetype=\"normal\", tavg=10, coupling=0.1\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_alternating_inputs, init_back_altern, inhib_rates,\n",
    "                    back_params_altern, duration, deltat, learnrate=learnrate, seed=509811537, \n",
    "                    noisetype=\"uniform\", tavg=tau_avg, coupling=coupling_eta)\n",
    "tser, mser, nuser, cser, cbarser, _, wser, bkvecser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete model: sparse Kenyon cell tags for odors\n",
    "We need to make new simulations with many more dimensions (ORN types). \n",
    "\n",
    "Consequently, to avoid running into memory issues, we only save a subset of time steps in the simulation: this is fine because we are only interested in the slowly-evolving $\\vec{w}$ and $\\vec{m}$, while we don't care too much for $\\vec{x}$'s fast fluctuations. We just want the final average $\\vec{w}$ to apply as inhibition to randomly sampled background odors, which we don't even take from simulations but just generate from the steady-state distribution. \n",
    "\n",
    "### Run a new simulation with 25 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions_tag = 25  # Half the real number for faster simulations\n",
    "n_neurons = n_components\n",
    "\n",
    "# Simulation rates and coupling stay the same (try at least)\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "tau_nu = 2.0  # Correlation time scale of the background nu_gammas (same for all)\n",
    "learnrate = 0.001  # Learning rate of M\n",
    "rel_lrate = 2.0  # Learning rate of L, relative to learnrate\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range = 0.2\n",
    "lambda_max = 5.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate = 2.0  #  / lambda_max**2 \n",
    "lambda_mat_diag = np.asarray([1.0 - lambda_range*k / (n_neurons - 1) for k in range(n_neurons)])\n",
    "lambda_mat_diag *= lambda_max\n",
    "biopca_rates = [learnrate, rel_lrate, lambda_max, lambda_range, learnrate]\n",
    "\n",
    "inhib_rates = [5e-5, 1e-5]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: as advised in Minden et al., 2018 \n",
    "rgen_meta_tag = np.random.default_rng(seed=0xb65007421a888ebe4c0e83313ef84911)\n",
    "init_mmat_tag = rgen_meta_tag.standard_normal(size=[n_neurons, n_dimensions_tag]) / np.sqrt(n_dimensions_tag)\n",
    "init_lmat_tag = np.eye(n_neurons, n_neurons)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_tag = [init_mmat_tag, init_lmat_tag]\n",
    "\n",
    "# Choose symmetric, normalized background odor components\n",
    "#back_components_tag = np.ones([n_components, n_dimensions_tag]) * 0.2\n",
    "#for i in range(n_components):\n",
    "#    back_components_tag[i, i] = 0.8\n",
    "#    back_components_tag[i] /= np.sqrt(np.sum(back_components_tag[i]**2))\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "back_components_tag = np.zeros([n_components, n_dimensions_tag])\n",
    "for i in range(n_components):\n",
    "    back_components_tag[i] = generate_odorant(n_dimensions_tag, rgen_meta_tag, lambda_in=0.1)\n",
    "print(back_components_tag)\n",
    "back_components_tag = back_components_tag / l2_norm(back_components_tag).reshape(-1, 1)\n",
    "\n",
    "# Initial nu values stay the same\n",
    "init_bkvec_tag = averages_nu.dot(back_components_tag)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list_tag = [init_nu, init_bkvec_tag]\n",
    "\n",
    "# Update matrices for nu process stay the same\n",
    "back_params_tag = [update_mat_A, update_mat_B, back_components_tag, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp_tag = 20\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(ml_inits_tag, update_thirdmoment_kinputs, \n",
    "                        init_back_list_tag, biopca_rates, inhib_rates, back_params_tag, duration, \n",
    "                        deltat, seed=seed_from_gen(rgen_meta_tag), noisetype=\"normal\", skp=skp_tag)\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser_tag, nuser_tag, bkvecser_tag, mser_tag, lser_tag, cbarser_tag, wser_tag, sser_tag = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skp = 50\n",
    "fig, ax = plt.subplots()\n",
    "w1_palette = sns.color_palette(\"Blues\", n_colors=n_neurons)\n",
    "w2_palette = sns.color_palette(\"Purples\", n_colors=n_neurons)\n",
    "w3_palette = sns.color_palette(\"Greens\", n_colors=n_neurons)\n",
    "for i in range(n_neurons-1):\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 0], color=w1_palette[i], alpha=0.8)\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 1], color=w2_palette[i], alpha=0.8)\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 2], color=w3_palette[i], alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 0], color=w1_palette[-1], label=\"Neuron Component 0\", alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 1], color=w2_palette[-1], label=\"Neuron Component 1\", alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 2], color=w3_palette[-1], label=\"Neuron Component 2\", alpha=0.8)\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Inhibition neurons components\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_tag, mser_tag, lser_tag, lambda_mat_diag)\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res\n",
    "\n",
    "fig, axes = plot_pca_results(tser_tag, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check background statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(nuser_tag, axis=0))\n",
    "print(np.mean(nuser_tag**2, axis=0))\n",
    "print(np.mean(nuser_tag**3, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectorser_tag = compute_projector_series(mser_tag, lser_tag)\n",
    "projectorser_tag = np.einsum(\"...ij,...jk\", wser_tag, projectorser_tag)\n",
    "projectionser_tag = np.einsum(\"...ij,...j\", projectorser_tag, bkvecser_tag)\n",
    "subtracted_series = bkvecser_tag - projectionser_tag\n",
    "\n",
    "n_row = 5\n",
    "n_col = n_dimensions_tag // n_row + min(1, n_dimensions_tag % n_row)\n",
    "fig, axes = plt.subplots(n_row, n_col, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_row*2.0, n_col*2.0)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_dimensions_tag):\n",
    "    axes[i].plot(tser_tag/1000, subtracted_series[:, i], color=\"k\")\n",
    "    if i // n_col == (n_dimensions_tag // n_col):\n",
    "        axes[i].set_xlabel(\"Time (x1000 steps)\")\n",
    "    if i % n_col == 0:\n",
    "        axes[i].set_ylabel(r\"$\\vec{x} - LM\\vec{x}$\")\n",
    "    axes[i].set_title(\"ORN {:d}\".format(i), y=0.85)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and compare projection tags after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New odor, mix, and inhibit\n",
    "### Repeat for many new odors (and ideally, should repeat for many backgrounds)\n",
    "### But for now, assume all simulations would give similarly good inhibition. \n",
    "n_test_new_odors = 100\n",
    "mix_frac = 0.2\n",
    "\n",
    "# Average m and w with which we will inhibit\n",
    "transient_tag = 100000 // skp_tag\n",
    "ltag_mean = np.mean(lser_tag[transient_tag:], axis=0)\n",
    "mtag_mean = np.mean(mser_tag[transient_tag:], axis=0)\n",
    "wtag_mean = np.mean(wser_tag[transient_tag:], axis=0)\n",
    "\n",
    "# Background samples, valid for all new test odors\n",
    "back_samples_tag = sample_background_thirdmoment(averages_nu, steady_covmat, epsilon_nu, back_components_tag, \n",
    "                                                  size=100, rgen=rgen_meta_tag)\n",
    "inhib_biopca_samples_tag = []\n",
    "inhib_avg_samples_tag = []\n",
    "mix_samples_tag = []\n",
    "new_odor_targets = []\n",
    "# Average background\n",
    "avg_back_tag = averages_nu.dot(back_components_tag)\n",
    "a_over_ab = inhib_rates[0] / sum(inhib_rates)\n",
    "for i in range(n_test_new_odors):\n",
    "    # New odor\n",
    "    #new_odor_tag = np.roll(back_components_tag[0], shift=-1)  # Should be a new vector\n",
    "    new_odor_tag = generate_odorant(n_dimensions_tag, rgen_meta_tag)\n",
    "    new_odor_tag = new_odor_tag / l2_norm(new_odor_tag)\n",
    "    new_odor_targets.append(new_odor_tag)\n",
    "\n",
    "    mix_samples = back_samples_tag + mix_frac * new_odor_tag.reshape(1, -1)\n",
    "    mix_samples_tag.append(mix_samples)\n",
    "    \n",
    "    # Compare to inhibition of the average background\n",
    "    inhib_avg_samples = mix_samples - a_over_ab * avg_back_tag.reshape(1, -1)\n",
    "    inhib_avg_samples_tag.append(inhib_avg_samples)\n",
    "\n",
    "    # Inhibition of each generated sample and statistics on performance\n",
    "    inhib_biopca_samples = respond_new_odors(mix_samples, ltag_mean, mtag_mean, wtag_mean)\n",
    "    inhib_biopca_samples_tag.append(inhib_biopca_samples)\n",
    "\n",
    "mix_samples_tag = np.asarray(mix_samples_tag)\n",
    "inhib_avg_samples_tag = np.asarray(inhib_avg_samples_tag)\n",
    "inhib_biopca_samples_tag = np.asarray(inhib_biopca_samples_tag)\n",
    "new_odor_targets = np.asarray(new_odor_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags. This won't be great because too few dimensions to begin with, but try anyways. \n",
    "projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=True, n_pn_per_kc=6, fix_thresh=None)\n",
    "proj_mat = create_sparse_proj_mat(n_kc=int(2000/50*n_dimensions_tag), n_rec=n_dimensions_tag, \n",
    "                        rgen=rgen_meta, fraction_filled=projtag_kwargs[\"n_pn_per_kc\"]/n_dimensions_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_none = []\n",
    "jaccards_inhib_avg = []\n",
    "jaccards_inhib_biopca = []\n",
    "for i in range(mix_samples_tag.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(mix_samples_tag.shape[1]):\n",
    "        tag_none = project_neural_tag(mix_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_avg = project_neural_tag(inhib_avg_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_biopca = project_neural_tag(inhib_biopca_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_none.append(jaccard(target_tag, tag_none))\n",
    "        jaccards_inhib_avg.append(jaccard(target_tag, tag_avg))\n",
    "        jaccards_inhib_biopca.append(jaccard(target_tag, tag_biopca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_biopca = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "\n",
    "ax.hist(jaccards_inhib_none, label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_none), color=clr_none, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_avg, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_biopca, label=\"biopca inhibition\", facecolor=clr_biopca, alpha=0.6, \n",
    "        edgecolor=clr_biopca, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_biopca), color=clr_biopca, ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "do_save = True\n",
    "if mix_frac == 0.2 and do_save:\n",
    "    fig.savefig(\"figures/detection/jaccard_similarity_biopca_average_none_f20percent.pdf\", transparent=True)\n",
    "elif mix_frac == 0.5 and do_save:\n",
    "    fig.savefig(\"figures/detection/jaccard_similarity_biopca_average_none_f50percent.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PN distances\n",
    "print(mix_samples_tag.shape)\n",
    "print(new_odor_tag.shape)\n",
    "dist_pure_inhib_none_tag = distance_panel_target(mix_samples_tag, new_odor_targets*mix_frac)\n",
    "dist_pure_inhib_avg_tag = distance_panel_target(inhib_avg_samples_tag, new_odor_tag*mix_frac)\n",
    "dist_pure_inhib_ibcm_tag = distance_panel_target(inhib_ibcm_samples_tag, new_odor_tag*mix_frac)\n",
    "\n",
    "median_distances_none_tag = np.median(dist_pure_inhib_none_tag, axis=0)\n",
    "median_distances_avg_tag = np.median(dist_pure_inhib_avg_tag, axis=0)\n",
    "median_distances_ibcm_tag = np.median(dist_pure_inhib_ibcm_tag, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none_tag[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none_tag[i], color=clr_none, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_avg_tag[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "    ax.axvline(median_distances_avg_tag[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_ibcm_tag[:, i], label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True) \n",
    "    ax.axvline(median_distances_ibcm_tag[i], color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to ideal inhibitory network\n",
    "In a linear algebra perspective, the best inhibition that could possibly be achieved of a new odor plus background mixture is that the whole component of the new odor parallel to the vector subspace spanned by the background odors is suppressed, while the component perpendicular to it is kept. Indeed, the appearance of the new odor's component in the background space cannot be distinguished from a fluctuation of the background (unless we had neurons tracking statistics of typical activations in that space, but not obvious how to get that). At any rate, this is the best we can hope our IBCM inhibition network will achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_projector(a):\n",
    "    \"\"\" Calculate projector a a^+, which projects\n",
    "    a column vector on the vector space spanned by columns of a. \n",
    "    \"\"\"\n",
    "    a_inv = np.linalg.pinv(a)\n",
    "    return a.dot(a_inv)\n",
    "    \n",
    "def find_parallel_component(x, basis, projector=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (np.ndarray): 1d array of length D containing the vector to decompose. \n",
    "        basis (np.ndarray): 2d matrix of size DxK where each column is one\n",
    "            of the linearly independent background vectors. \n",
    "        projector (np.ndarray): 2d matrix A A^+, the projector on the vector\n",
    "            space spanned by columns of basis. \n",
    "    Return:\n",
    "        x_par (np.ndarray): component of x found in the vector space of basis\n",
    "            The perpendicular component can be obtained as x - x_par. \n",
    "    \"\"\"\n",
    "    # If the projector is not provided yet\n",
    "    if projector is None:\n",
    "        # Compute Moore-Penrose pseudo-inverse and AA^+ projector\n",
    "        projector = find_projector(basis)\n",
    "    x_par = projector.dot(x)\n",
    "    return x_par\n",
    "\n",
    "def ideal_linear_inhibitor(x_n_par, x_n_ort, x_back, f, alpha, beta):\n",
    "    \"\"\" Calculate the ideal projection neuron layer, which assumes\n",
    "    perfect inhibition (down to beta/(alpha+beta)) of the component of the mixture\n",
    "    parallel to the background odors' vector space, while leaving the orthogonal\n",
    "    component of the new odor untouched. \n",
    "    \n",
    "    Args:\n",
    "        x_n_par (np.1darray): new odor, component parallel to background vector space\n",
    "        x_n_ort (np.1darray): new odor, component orthogonal to background vector space \n",
    "        x_back (np.2darray): background samples, one per row\n",
    "        f (float): mixture fraction (hard case is f=0.2)\n",
    "        alpha (float): inhibitory weights learning rate alpha\n",
    "        beta (float): inhibitory weights decaying rate beta\n",
    "    \n",
    "    Returns:\n",
    "        s (np.1darray): projection neurons after perfect linear inhibition\n",
    "    \"\"\"\n",
    "    # Allow broadcasting for multiple x_back vectors\n",
    "    factor = beta / (2*alpha + beta)\n",
    "    s = factor * f*x_n_par + f*x_n_ort\n",
    "    # I thought the following would have been even better, but turns out it is worse for small f\n",
    "    #s = f*x_n_par + f*x_n_ort\n",
    "    s = relu_inplace(s.reshape(1, -1) + factor * x_back)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse each new odor in new_odor_targets and each background in back_samples_tag\n",
    "# Compute the projector on the background odor components only once\n",
    "# Compute parallel component of each new odor\n",
    "# Mix it with all background samples at once using broadcasting capability of ideal_linear_inhibitor function\n",
    "background_projector = find_projector(back_components_tag.T)\n",
    "inhib_ideal_samples_tag = []\n",
    "for od in new_odor_targets:\n",
    "    # Decompose\n",
    "    od_par = find_parallel_component(od, basis=back_components_tag.T, projector=background_projector)\n",
    "    od_ort = od - od_par\n",
    "    # Compute the perfectly inhibited mixture with each background sample\n",
    "    inhib_ideal = ideal_linear_inhibitor(od_par, od_ort, back_samples_tag, mix_frac, *inhib_rates)\n",
    "    # Background reduced to b/(a+b), new odor intact? Perfect inhibition\n",
    "    #inhib_ideal = inhib_rates[1] / sum(inhib_rates) * (1.0 - mix_frac) * back_samples_tag + mix_frac * od.reshape(1, -1)\n",
    "    inhib_ideal_samples_tag.append(inhib_ideal)\n",
    "inhib_ideal_samples_tag = np.asarray(inhib_ideal_samples_tag)\n",
    "\n",
    "# Compute neural tags of the ideal inhibited mixtures and compare to target tags. \n",
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_ideal = []\n",
    "for i in range(new_odor_targets.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(back_samples_tag.shape[0]):\n",
    "        # Input mixture vector too\n",
    "        mixture_ideal_samples = (1.0-mix_frac)*back_samples_tag[j] + mix_frac*new_odor_targets[i]\n",
    "        tag_ideal = project_neural_tag(inhib_ideal_samples_tag[i, j], mixture_ideal_samples, proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_ideal.append(jaccard(target_tag, tag_ideal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_map = {\"none\": \"xkcd:navy blue\", \"average\": \"xkcd:orangey brown\", \n",
    "           \"ibcm\":\"xkcd:turquoise\", \"ideal\": \"xkcd:powder blue\", \"ideal2\":\"xkcd:pale rose\"}\n",
    "\n",
    "ax.hist(jaccards_inhib_ideal, label=\"Ideal inhibition\", facecolor=clr_map[\"ideal\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ideal\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ideal), color=clr_map[\"ideal\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_map[\"average\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"average\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_map[\"average\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_ibcm, label=\"IBCM inhibition\", facecolor=clr_map[\"ibcm\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ibcm\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ibcm), color=clr_map[\"ibcm\"], ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance as a function of f\n",
    "I expect to see a relatively sharp drop of median performance for IBCM a bit above $f=\\beta/(2\\alpha + \\beta)$, and at this value for the ideal inhibition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median_performances(back_samples, new_odors, f, projmat, proj_kwargs, \n",
    "                                m_mean, w_mean, eta, inhib_ab, back_components):\n",
    "    \"\"\" Compute median Jaccard similarity for the different inhibition methods we have, \n",
    "    for a given value of mixture parameter f. \"\"\"\n",
    "    all_jaccard_pairs_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}  # list of lists, one per method\n",
    "    back_proj = find_projector(back_components.T)\n",
    "    for i in range(new_odors.shape[0]):\n",
    "        # Compute target tag\n",
    "        target_tag = project_neural_tag(new_odors[i], new_odors[i], projmat, **proj_kwargs)\n",
    "        # Prepare mixtures\n",
    "        mix_samples = back_samples + new_odors[i:i+1]*f\n",
    "        \n",
    "        # Compute inhibited mixtures with the different methods\n",
    "        # No inhibition: just use mix_samples\n",
    "        # Average inhibition\n",
    "        avg_back_tag = averages_nu.dot(back_components_tag)\n",
    "        b_over_2ab = inhib_ab[1] / (sum(inhib_ab) + inhib_ab[0])\n",
    "        inhib_avg_samples = mix_samples - a_over_ab * avg_back_tag.reshape(1, -1)\n",
    "\n",
    "        # Inhibition of each generated sample and statistics on performance\n",
    "        inhib_ibcm_samples = respond_new_odors(mix_samples, m_mean, w_mean, eta)\n",
    "        \n",
    "        # Ideal inhibition\n",
    "        od_par = find_parallel_component(new_odors[i], basis=back_components.T, projector=back_proj)\n",
    "        od_ort = new_odors[i] - od_par\n",
    "        # Compute the perfectly inhibited mixture with each background sample\n",
    "        inhib_ideal_samples = ideal_linear_inhibitor(od_par, od_ort, back_samples, f, *inhib_ab)\n",
    "        # Background reduced to b/(a+b), new odor intact?\n",
    "        inhib_ideal2_samples = relu_inplace(b_over_2ab * back_samples + f * new_odors[i:i+1])\n",
    "    \n",
    "        # For each inhibited mixture, compute jaccard similarity\n",
    "        current_jaccard_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "        for j in range(back_samples.shape[0]):\n",
    "            mix_tag = project_neural_tag(mix_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"none\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Average\n",
    "            mix_tag = project_neural_tag(inhib_avg_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"average\"].append(jaccard(target_tag, mix_tag))\n",
    "        \n",
    "            # IBCM\n",
    "            mix_tag = project_neural_tag(inhib_ibcm_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ibcm\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal\n",
    "            mix_tag = project_neural_tag(inhib_ideal_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal 2\n",
    "            mix_tag = project_neural_tag(inhib_ideal2_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal2\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "        # Add those values to the total list\n",
    "        for method in all_jaccard_pairs_dict.keys():\n",
    "            all_jaccard_pairs_dict[method].append(current_jaccard_dict[method])\n",
    "    \n",
    "    # Convert to 2d array and compute median\n",
    "    # Could choose to have one median per odor or per background sample\n",
    "    all_jaccard_pairs_dict = {k:np.asarray(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    median_jaccard_pairs_dict = {k:np.median(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    return median_jaccard_pairs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previous functions for various f values\n",
    "median_jaccards = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "f_range = np.arange(0.1, 0.8, 0.1)\n",
    "for f in f_range:\n",
    "    meds = compute_median_performances(back_samples_tag, new_odor_targets, f, proj_mat, projtag_kwargs, \n",
    "                                mtag_mean, wtag_mean, coupling_eta, inhib_rates, back_components_tag)\n",
    "    for k in meds:\n",
    "        median_jaccards[k].append(meds[k])\n",
    "    print(\"Done f = {:.2f}\".format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labelmap = {\"none\":\"None\", \"average\":\"Average\", \"ibcm\":\"IBCM\", \"ideal\":r\"Ideal $\\perp$\", \"ideal2\":\"Ideal all\"}\n",
    "for k in median_jaccards:\n",
    "    ax.plot(f_range, median_jaccards[k], color=clr_map[k], label=labelmap[k], lw=3)\n",
    "ax.set(xlabel=\"Fraction $f$ of new odor\", ylabel=\"Median Jaccard similarity\")\n",
    "ax.legend(title=\"Inhibition method\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/detection/inhibition_jaccard_comparison_methods.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical prediction for the \"perfect\" inhibition model\n",
    "\n",
    "#WARNING: THIS NEVER WORKED. Code kept for reference. \n",
    "\n",
    "Perfect inhibition: only the background is reduced to $\\frac{\\beta}{\\alpha + \\beta}$ of its original amplitude, while the new odor is untouched:\n",
    "\n",
    "$$ \\vec{s} = \\frac{\\beta}{\\alpha + \\beta} (1-f) \\vec{x}^b(t) + f \\vec{x}^n $$\n",
    "\n",
    "One would think this is the best possible inhibition, but for low $f$, it fares worse than deleting the parallel component of $\\vec{x}^n$ as well, because inhibition of the parallel component lowers even further the activity of Kenyon cells specific to the background; the new odor does not reinforce them and only KCs specific to the new odor can cross the threshold. \n",
    "\n",
    "My analytical calculation is for this \"perfect\" inhibition, because it is easier to treat analytically than the . Also, I computed the mean rather than median Jaccard similarity. Anyways, let's see what it looks like and if it makes sense. \n",
    "\n",
    "UPDATE: It just does not work. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def analytical_approx_jaccard_perfect_inhibition_wrong(proj_kappa, avg_orn, avg_nu, k_back, \n",
    "                                                 kc_sparsity, f_mix, ab_rates):\n",
    "    \"\"\"\n",
    "    This is not right at all, but I tried nevertheless. The average delta is causing\n",
    "    a lot of issues. It seems one should really marginalize over deltas rather\n",
    "    than replace it by some average delta value. \n",
    "    Args:\n",
    "        proj_kappa (int): number of non-zero elements per row of the projection matrix\n",
    "        avg_orn (float): average value of a component of an odor vector\n",
    "        avg_nu (float): average value of the linear combination coefficients for t\n",
    "            the background odors. \n",
    "        k_back (int): number of odors in the background\n",
    "        kc_sparsity (float): fraction of KC cells kept in the tag (usually 0.05)\n",
    "        f_mix (float): fraction of the mixture attributed to the new odor\n",
    "        ab_rates (list of 2 floats): alpha, beta\n",
    "    \"\"\"\n",
    "    # 1. Each KC activity, y_i, in response to the new odor follows gamma(proj_kappa, 1/avg_orn)\n",
    "    # and in response to the average background, gamma(proj_kappa*k_back, 1/avg_orn/avg_nu)\n",
    "    # 2.1 Calculate the threshold of most active KC cells in response to the new odor alone. \n",
    "    tau_new = sp.special.gammaincinv(proj_kappa, 1 - kc_sparsity) * avg_orn\n",
    "    \n",
    "    # 2.2 Calculate average margin above threshold, Delta, of the activities of the most active\n",
    "    # KC in response to the new odor alone.\n",
    "    # Try an approximation first: use some fraction of tau_new itself\n",
    "    avg_delta = 0.05*tau_new\n",
    "    # Otherwise, need to calculate complicated integral numerically\n",
    "    \n",
    "    # 3. Compute the probability, on average, that one of the KCs in the\n",
    "    # new odor tag is in the tag of the background-new odor mixture\n",
    "    lamb_nu = 1.0 / (avg_orn * avg_nu)\n",
    "    factor = f_mix/(1 - f_mix) * sum(ab_rates)/ab_rates[1]\n",
    "    avg_common_i = sp.special.gammaincc(proj_kappa*k_back, lamb_nu*(tau_new - factor * avg_delta))\n",
    "    \n",
    "    # 4. The average Jaccard similarity, using the inclusion-exclusion rule, is\n",
    "    avg_jaccard = avg_common_i / (2.0 - avg_common_i)  # Ranges between 0 and 1, as it should\n",
    "    \n",
    "    return avg_jaccard\n",
    "\n",
    "def analytical_approx_jaccard_perfect_inhibition(proj_kappa, avg_orn, avg_nu, k_back, \n",
    "                                                 kc_sparsity, f_mix, ab_rates):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proj_kappa (int): number of non-zero elements per row of the projection matrix\n",
    "        avg_orn (float): average value of a component of an odor vector\n",
    "        avg_nu (float): average value of the linear combination coefficients for t\n",
    "            the background odors. \n",
    "        k_back (int): number of odors in the background\n",
    "        kc_sparsity (float): fraction of KC cells kept in the tag (usually 0.05)\n",
    "        f_mix (float): fraction of the mixture attributed to the new odor\n",
    "        ab_rates (list of 2 floats): alpha, beta\n",
    "    \"\"\"\n",
    "    # 1. Each KC activity, y_i, in response to the new odor follows gamma(proj_kappa, 1/avg_orn),\n",
    "    # and in response to the average background, gamma(proj_kappa*k_back, 1/avg_orn/avg_nu)\n",
    "    # 2. Calculate the threshold of most active KC cells in response to the new odor alone. \n",
    "    tau_new = sp.special.gammaincinv(proj_kappa, 1 - kc_sparsity) * avg_orn\n",
    "    \n",
    "    # 3. Calculate the threshold of most active KC cells \n",
    "    # in response to the average background alone\n",
    "    tau_b = sp.special.gammaincinv(proj_kappa*k_back, 1 - kc_sparsity) * (avg_orn * avg_nu)\n",
    "    \n",
    "    # The probability of a KC from the new odor's tag to be present in the mixture\n",
    "    # tag as well is given by:\n",
    "    #     avg_common_i = sp.special.gammaincc(proj_kappa*k_back, lamb_nu*(tau_new - factor * delta_i))\n",
    "    # for a given value of delta_i. We need to average this over possible delta_i values\n",
    "    # The prob distribution of delta_i is the gamma distribution, conditioned on knowing that y_i is larger\n",
    "    # than tau_i, which is the (100*(1-kc_sparsity))-th percentile \n",
    "    # (so it is a gamma pdf(tau+delta) divided by kc_sparsity)\n",
    "    loglambda_k = -proj_kappa * np.log(avg_orn)\n",
    "    factor = f_mix/(1 - f_mix) * sum(ab_rates)/ab_rates[1]\n",
    "    lamb_nu = 1.0 / (avg_orn * avg_nu)\n",
    "    loggammak = sp.special.gammaln(proj_kappa)\n",
    "    def integrand(delt):\n",
    "        pdf_delta = (proj_kappa - 1)*np.log(tau_new + delt)  - (tau_new + delt)/avg_orn + loglambda_k\n",
    "        pdf_delta = pdf_delta - loggammak\n",
    "        cumul_dist_yib = sp.special.gammaincc(proj_kappa*k_back, max(0, lamb_nu*(tau_b - factor * delt)))\n",
    "        return np.exp(pdf_delta) * cumul_dist_yib\n",
    "    \n",
    "    \n",
    "    # Integrate (we kept the conditional prob. factor 1/kc_sparsity for the end)\n",
    "    avg_common_i = sp.integrate.quad(integrand, 0, np.inf)[0]\n",
    "    avg_common_i /= kc_sparsity\n",
    "    \n",
    "    # 4. The average Jaccard similarity, using the inclusion-exclusion rule, is\n",
    "    avg_jaccard = avg_common_i / (2.0 - avg_common_i)  # Ranges between 0 and 1, as it should\n",
    "    \n",
    "    return avg_jaccard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compute the analytical prediction for each f in f_range\n",
    "predicted_perfect_jaccard = []\n",
    "for f in f_range:\n",
    "    predicted_perfect_jaccard.append(analytical_approx_jaccard_perfect_inhibition(\n",
    "        proj_kappa=projtag_kwargs[\"n_pn_per_kc\"], \n",
    "        avg_orn=np.mean(back_components_tag), \n",
    "        avg_nu=averages_nu.mean() + epsilon_nu*sigma2, \n",
    "        k_back=back_components_tag.shape[0],\n",
    "        #avg_nu=averages_nu.mean()*0.36,  # Hack to get real distrib. of KC in response to background alone\n",
    "        #k_back=back_components_tag.shape[0]/2.3,   # Hack\n",
    "        kc_sparsity=projtag_kwargs[\"kc_sparsity\"], \n",
    "        f_mix=f, ab_rates=inhib_rates)\n",
    "    )\n",
    "print(predicted_perfect_jaccard)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "labelmap = {\"none\":\"None\", \"average\":\"Average\", \"ibcm\":\"IBCM\", \"ideal\":r\"Ideal $\\perp$\", \"ideal2\":\"Ideal all\"}\n",
    "for k in [\"ideal\", \"ideal2\"]:\n",
    "    ax.plot(f_range, median_jaccards[k], color=clr_map[k], label=labelmap[k], lw=3)\n",
    "ax.plot(f_range, predicted_perfect_jaccard, color=\"k\", label=\"Predicted all\", lw=3)\n",
    "ax.set(xlabel=\"Fraction $f$ of new odor\", ylabel=\"Median Jaccard similarity\")\n",
    "ax.legend(title=\"Inhibition method\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/ideal_inhibition_jaccard_vs_predicted.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def log_gammadist(x, k, lam):\n",
    "    return (k-1)*np.log(x) - lam*x + k*np.log(lam) - sp.special.gammaln(k)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Check distribution of KC in response to background, compare to the postulated gamma\n",
    "# Dot product each back_sample_tag with the projmat, use all resulting KCs to determine distrib\n",
    "# The correct kappa, K, avg_nu, etc. do not give the right distribution\n",
    "# I can fit manually the gamma to get close to the actual distribution\n",
    "# Try that hack in the calculation of the predicted performance\n",
    "# UPDATE: does not seem to help...\n",
    "back_samples_kenyon = proj_mat.dot(back_samples_tag.T)\n",
    "kc_axis = np.linspace(back_samples_kenyon.min(), back_samples_kenyon.max(), 101)\n",
    "# Compute gamma distribution that we postulated: gamma(K*kappa, lambda/avg_nu)\n",
    "gamma_kc_axis = np.exp(log_gammadist(kc_axis, \n",
    "                            k=projtag_kwargs[\"n_pn_per_kc\"]*back_components_tag.shape[0], \n",
    "                            lam=1.0 / (np.mean(back_components_tag) * averages_nu.mean())))\n",
    "                            #k=projtag_kwargs[\"n_pn_per_kc\"]*back_components_tag.shape[0]/2.3, \n",
    "                            #lam=0.36 / (np.mean(back_components_tag) * averages_nu.mean())))\n",
    "plt.hist(back_samples_kenyon.flatten(), bins=20, density=True)\n",
    "plt.plot(kc_axis, gamma_kc_axis)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check distribution of KC in response to single new odor. Should really be a gamma distribution...\n",
    "# This seems to match pretty closely. \n",
    "target_samples_kenyon = proj_mat.dot(new_odor_targets.T)\n",
    "kc_axis = np.linspace(target_samples_kenyon.min(), target_samples_kenyon.max(), 101)\n",
    "# Compute gamma distribution that we postulated: gamma(K*kappa, lambda/avg_nu)\n",
    "gamma_kc_axis = np.exp(log_gammadist(kc_axis, \n",
    "                            k=projtag_kwargs[\"n_pn_per_kc\"], \n",
    "                            lam=1.0 / (np.mean(new_odor_targets))))\n",
    "plt.hist(target_samples_kenyon.flatten(), bins=20, density=True)\n",
    "plt.plot(kc_axis, gamma_kc_axis)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
