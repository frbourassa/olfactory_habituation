{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhibition of odors with IBCM neurons: 2D toy model\n",
    "\n",
    "A layer of IBCM neurons is used to inhibit the activity of ORNs in response to a fluctuating olfactory background. Synaptic weights from the input layer to the inhibition layer, $M = (\\vec{m}_1, \\vec{m}_2, \\ldots)$, are learnt according to the IBCM rule. Synaptic weights for inhbition, from the inhibitory neurons to the projection layer, are learnt to minimize the squared norm of the projection neuron (PN) layer. In this way, the network of IBCM neurons is like an autoencoder applying feedforward inhibition. \n",
    "\n",
    "![test](figures/feedforward_inhibitory_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network of IBCM neurons\n",
    "\n",
    "Consider a feedforward network of IBCM  neurons. Each will be paired with an inhibitory neuron, but those inhibitory neurons will not interact with other neurons except their own IBCM neuron, so we can discuss them later. Each IBCM neuron has $n_R \\geq N_I$ input synapses, represented by the connectivity vector $\\vec{m}_i = (m^1_i, \\ldots, m^{n_R}_i)$. Its activation upon stimulation, before coupling to other IBCM neurons is considered, is given by $c = \\vec{m}_i \\cdot \\vec{x}$, where $\\vec{x}$ is a two-dimensional input vector. Its activity inhibited by other IBCM neurons in the network is \n",
    "\n",
    "$$ \\bar{c}_i = c_i - \\eta \\sum_{j \\neq i} c_j \\quad \\mathrm{where} \\, \\, c_i(t) = \\vec{m}_i(t) \\cdot \\vec{x}(t)  \\, \\, .$$\n",
    "\n",
    "The update equation of each IBCM neuron's weights uses this inhibited activity:\n",
    "\n",
    "$$ \\dot{m}_i = \\mu \\left[ \\bar{c}_i(\\bar{c}_i - \\bar{\\Theta}_i) \\vec{x} - \\eta \\sum_{j \\neq i} \\bar{c}_j(\\bar{c}_j - \\bar{\\Theta}_j) \\vec{x} \\right]  \\quad \\mathrm{where} \\, \\, \\bar{\\Theta}_i = \\mathbb{E}[\\bar{c}_i^2] $$\n",
    "\n",
    "The parameter $\\eta$ is the coupling strength. The thresholds $\\bar{\\Theta}_i$ are time-dependent averages of the IBCM neuron's inhibited activity squared, $\\bar{c}_i^2$; they evolve with $\\vec{m}_i$ according to the differential equation\n",
    "\n",
    "$$ \\dot{\\bar{\\Theta}}_i = \\frac{1}{\\tau_\\Theta} (\\bar{c}_i^2 - \\bar{\\Theta}_i)  \\,\\, .$$\n",
    "\n",
    "Hence, they effectively take the average of $\\bar{c}_i^2$ over a sliding exponential window with a time scale $\\tau_\\Theta$. This time scale should be a lot longer than the fluctuation time scale of the input, $\\tau_c$, but also a lot faster than the slow time scale of $\\vec{m}_i$ learning itself. In short, we should have $\\tau_c \\ll \\tau_{\\Theta} \\ll \\frac{1}{\\mu}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward inhibitory neurons\n",
    "\n",
    "During the learning phase (i.e. exposition to the background only), we want the inhibitory neurons to silence the activity of projection neurons in response to the varying input. Define $\\vec{s}$ the vector of PN activities, with \n",
    "$$\\vec{s}= R(\\vec{x}_{in} -  W\\vec{\\bar{c}})   \\quad ,$$\n",
    "\n",
    "where $R$ is an element-wise activation function (e.g., identity or ReLU), and $W\\vec{\\bar{c}}$ is the output sent by the inhibitory neurons to projection neurons (each column of $W$ is the vector $\\vec{w}^j$ of synaptic weights leaving inhibitory vector $j$ towards the different PNs). We want the inhibitory weight matrix $W$ to minimize the cost function\n",
    "\n",
    "$$ C(W) = \\frac12 \\mathbb{E}[\\vec{s}^T \\vec{s}] + \\frac12 \\frac{\\beta}{\\alpha} \\mathbb{E}[\\vec{w}^T \\vec{w}] $$\n",
    "\n",
    "The first term ensures minimization of $\\vec{s}$'s magnitude. The second term is a regularization ensuring $\\vec{w}$ does not diverge when, for instance, $R$ is a ReLU function, such that large negative values of $\\vec{w}$ would make $\\vec{s}$ zero. In other words, the regularization ensures that the layer inhibits just enough the background and will still let through some of the new odor when it arrives. \n",
    "\n",
    "If we use gradient descent dynamics for the weights $\\vec{w}_j$, \n",
    "\n",
    "$$ \\frac{d \\vec{w}^j}{dt} = - \\alpha \\nabla_{\\vec{w}^j} C(W) $$\n",
    "\n",
    "we find, after computing the gradient, \n",
    "\n",
    "$$  \\frac{d \\vec{w}^j}{dt} = \\alpha \\bar{c}^j \\vec{s} - \\beta \\vec{w}^j $$\n",
    "\n",
    "which is a hebbian learning rule for each synaptic weight, because it is proportional to the activity of the two neurons at the endpoints ($\\bar{c}^j$ and $W_i^j$). Here, $\\alpha$ and $\\beta$ are the learning rates of the $W$ synapses; usually, one takes $\\beta < \\alpha$ to ensure that $\\vec{s}$ minimization is the dominant term of the cost function. \n",
    "\n",
    "In the toy model studied here, it is possible to calculate analytically the expectation value of $\\vec{w}^j$, under a few approximations. It is therefore a good starting point for this model more generally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.smoothing_function import moving_average\n",
    "from utils.statistics import seed_from_gen\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import integrate_inhib_ibcm_network, integrate_ibcm, relu_inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluctuating gaussian binary mixture\n",
    "### Definition of the process\n",
    "We simulate a simple two-component mixture where the proportion of the two odors change:\n",
    "\n",
    "$$ \\vec{x} = \\left(\\frac12 + \\nu\\right) \\vec{x}_a + \\left(\\frac12 - \\nu\\right) \\vec{x}_b $$\n",
    "\n",
    "where $\\nu$ follows a univariate Ornstein-Uhlenbeck process with mean $0$, time scale $\\tau_{\\nu}$, and steady-state variance $\\sigma^2$. We set $\\tau_{\\nu} = 2$ time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import update_ou_2inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical prediction of the fixed points\n",
    "\n",
    "#### Single IBCM  neuron\n",
    "Let's first consider a single IBCM neuron. \n",
    "To calculate the fixed points, it is useful to rewrite the background vector as \n",
    "$$ \\vec{x} = \\vec{x}_d + \\nu \\vec{x}_s $$\n",
    "where $\\vec{x}_d = \\frac12(\\vec{x}_a + \\vec{x}_b)$ is the \"deterministic\" part, and $\\vec{x}_s = \\vec{x}_a - \\vec{x}_b$ is the \"stochastic\" part. The two stable fixed points, $\\vec{m}_{\\pm}$, are thus found to be\n",
    "\n",
    "$$c_{d, \\pm} = \\vec{m}_{\\pm} \\cdot \\vec{x}_d = 1$$\n",
    "$$c_{s, \\pm} = \\vec{m}_{\\pm} \\cdot \\vec{x}_s = \\pm \\frac{1}{\\sigma} $$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$ c_{a, \\pm} = \\vec{m}_{\\pm} \\cdot \\vec{x}_a = 1 \\pm \\frac{1}{2 \\sigma} $$\n",
    "$$ c_{b, \\pm} = \\vec{m}_{\\pm} \\cdot \\vec{x}_b = 1 \\mp \\frac{1}{2 \\sigma} \\,\\, .$$\n",
    "\n",
    "In general, $\\vec{x}_a$ and $\\vec{x}_b$ may have different norms and a non-zero dot product $\\Omega = \\vec{x}_a \\cdot \\vec{x}_b$. In this case, $\\vec{m}_{\\pm}$ is written as a linear combination of $\\vec{x}_a$ and $\\vec{x}_b$ as\n",
    "\n",
    "$$ \\vec{m}_{\\pm} = \\frac{1}{|\\vec{x}_a|^2 |\\vec{x}_b|^2 - \\Omega^2} \\Big[(c_{a, \\pm}|\\vec{x}_b|^2 - \\Omega c_{b, \\pm})\\vec{x}_a + (c_{b, \\pm}|\\vec{x}_a|^2 - \\Omega c_{a, \\pm})\\vec{x}_b  \\Big]$$\n",
    "\n",
    "Moreover, the response to any $\\vec{x}(\\nu)$ of the IBCM neuron at a fixed point is\n",
    "\n",
    "$$ \\vec{m}_{\\pm} \\cdot \\vec{x} = 1 \\pm \\frac{\\nu}{\\sigma} \\, \\, .$$\n",
    "\n",
    "\n",
    "#### Network of IBCM  neurons\n",
    "For a network of IBCM neurons, the equations decouple when written in terms of the variables $\\vec{\\bar{m}}^i = \\vec{m}^i - \\eta \\sum_{j \\neq i} \\vec{m}^j$, which we could call the \"inhibited synaptic weights\", since $\\bar{c}$, the inhibited neuron activity, can simply be written as $\\vec{\\bar{m}} \\cdot \\vec{x}$. The fixed points for the $\\vec{\\bar{m}}$ are then the same as for a single IBCM neuron:\n",
    "\n",
    "$$ \\vec{\\bar{m}}^i_{\\pm} = \\vec{x}_d \\pm \\frac{1}{\\sigma} \\vec{x}_s $$\n",
    "\n",
    "If we have only two neurons, it is fairly easy to write the solution in terms of the \"un-inhibited\" synaptic weights $\\vec{m}^i$, although the results depend on whether both neurons are at the same fixed point ($+$ or $-$ sign) or at opposite fixed points. \n",
    "\n",
    "For neurons at the same fixed point, we find that $\\vec{m}^i$ is just rescaled by $\\frac{1}{1 - \\eta}$:\n",
    "\n",
    "$$\\vec{m}^i_{++, --} = \\frac{1}{1 - \\eta} \\vec{m}_{\\pm}$$\n",
    "\n",
    "For neurons at opposite fixed points, we find that the dot products with $\\vec{x}_a$ and $\\vec{x}_b$ are \n",
    "\n",
    "$$ c^i_a = \\vec{m}^i_{+-, -+} \\cdot \\vec{x}_a = \\frac{1}{1 - \\eta} \\pm \\frac{1}{1 + \\eta} \\frac{1}{2 \\sigma}$$\n",
    "$$ c^i_a = \\vec{m}^i_{+-, -+} \\cdot \\vec{x}_b = \\frac{1}{1 - \\eta} \\mp \\frac{1}{1 + \\eta} \\frac{1}{2 \\sigma}$$\n",
    "\n",
    "where one neuron has the upper sign, and the other has the lower sign; they both are still written as the linear combination\n",
    "\n",
    "$$ \\vec{m}^i_{+-, -+} = \\frac{1}{|\\vec{x}_a|^2 |\\vec{x}_b|^2 - \\Omega^2} \\Big[(c^i_a|\\vec{x}_b|^2 - \\Omega c^i_b)\\vec{x}_a + (c^i_b|\\vec{x}_a|^2 - \\Omega c^i_a)\\vec{x}_b  \\Big]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed points of the inhibitory synaptic weights, $\\vec{w}^i$\n",
    "\n",
    "If there are two IBCM neurons converging each to a different fixed point, i.e. we are in the case where one $\\vec{\\bar{m}}^j$ is $\\vec{\\bar{m}}_+$ and the other is $\\vec{\\bar{m}}_-$, then we find that \n",
    "\n",
    "$$ \\vec{w}^{\\pm} = \\frac{\\alpha}{2\\alpha + \\beta} \\vec{x}(\\nu = \\pm\\sigma) $$\n",
    "\n",
    "That means $\\vec{w}^{\\pm}$ is parallel to the input vector at one standard deviation away from the mean, on either side ($\\pm \\sigma$). \n",
    "\n",
    "#### Fixed points of the projection neurons, $\\vec{s} = R(\\vec{x} - W \\vec{\\bar{c}})$\n",
    "\n",
    "Using the instantaneous response of the two IBCM neurons to $\\vec{x}$, \n",
    "\n",
    "$$ \\bar{c} = \\vec{\\bar{m}} \\cdot \\vec{x}(t) = 1 \\pm \\frac{\\nu(t)}{\\sigma} $$\n",
    "\n",
    "we can calculate the average and instantaneous values of $\\vec{s} = R(\\vec{x} - W \\vec{\\bar{c}})$, considering that $W$ is at steady-state. We find:\n",
    "\n",
    "$$ \\vec{s} = R\\left(\\frac{\\beta}{2 \\alpha + \\beta} \\vec{x}\\right) $$\n",
    "\n",
    "which means that the average and squared norm of the inhibited background are\n",
    "\n",
    "$$ \\langle \\vec{s} \\rangle = R\\left(\\frac{\\beta}{2 \\alpha + \\beta} \\vec{x}_d \\right) $$\n",
    "$$ \\langle \\vec{s}^T \\vec{s} \\rangle = \\left( \\frac{\\beta}{2 \\alpha + \\beta} \\right)^2 \\langle \\vec{x}^T \\vec{x} \\rangle $$\n",
    "\n",
    "In other words, there is a reduction of the background's average *and standard deviation* down to $\\frac{\\beta}{2\\alpha + \\beta}$ of its original amplitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoints_m_2vectors, \n",
    "    fixedpoints_barm_2vectors, \n",
    "    fixedpoints_w_2vectors, \n",
    "    fixedpoint_s_2vectors_instant, \n",
    "    fixedpoint_s_2vectors_mean, \n",
    "    fixedpoint_s_2vectors_norm2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_stats(s1, s2, blocksize=1000):\n",
    "    \"\"\"Compure the (auto)correlation of random variables which have\n",
    "    a long stationary realization in s1 and s2. s1 and s2 are split in small blocks, \n",
    "    the length of which should be the max correlation time to consider, \n",
    "    the correlation is computed in each block, and the resulting\n",
    "    correlation function samples are averaged. \n",
    "    \"\"\"\n",
    "    if s1.size != s2.size:\n",
    "        raise ValueError(\"Should use arrays of same length\")\n",
    "    # Count number of blocks available; do not add one for remainder because it wouldn't have \n",
    "    # the same duration as other full blocks\n",
    "    n_blocks = max(1, s1.size // blocksize)  # At least one block\n",
    "    correl_blocks = []\n",
    "    for i in range(n_blocks):\n",
    "        sli = slice(blocksize*i, blocksize*(i+1))\n",
    "        cr = sp.signal.correlate(s1[sli], s2[sli], mode=\"same\") / blocksize\n",
    "        cr = cr[blocksize//2:]  # Keep only positive tau part\n",
    "        correl_blocks.append(cr)\n",
    "    # Sum arrays of same length and divide by number of blocks to estimate average. \n",
    "    return sum(correl_blocks) / n_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-order corrections to the $\\vec{w}$ fixed points due to correlations\n",
    "\n",
    "TODO: the correction calculations are utterly useless, unfortunately, because I changed the inhibition network. \n",
    "\n",
    "If I am really into it, redo the correction calculation for fun. Former explanation was:\n",
    "\n",
    "    Initially, my simulations did not converge to the expected fixed point (even though the analytical calculation was straigthfoward and seemed correct), so I used van Kampen's discussion of stochastic differential equations, chap. XVI, to calculate the first-order correction to the mean steady-state $\\vec{w}_{\\pm}$ due to correlations of the stochastic processes $\\nu$ and $\\vec{m}$. I'm not reporting the details here, but the resulting corrections are computed by the function ``fixedpoints_w_empirical_corrections`` below. As could be expected from the analytical study, they are utterly negligible, being of order $\\alpha dt$. But confirming this numerically helped me pinpoint the silly coding mistake that I had made. Once corrected, the numerical simulations confirmed the zeroth order solution for $\\vec{w}$ given above. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## TODO: redo the calculations for the better inhibition network chosen before. \n",
    "# Code below kept for reference. \n",
    "def fixedpoints_w_empirical_corrections(rates, sigma2, back_comp, fixed_m, msr, \n",
    "                                        eta=0.1, use_bar=True, dt=1.0, t_nu=2.0):\n",
    "    \"\"\" Given the steady-state time series of m and nu, estimate first-order correction\n",
    "    terms in the differential equation for the average w, and use it to compute a \n",
    "    corrected prediction for the fixed point value of the average w. For each neuron, \n",
    "    the corrected steady-state value of <w> is\n",
    "    \n",
    "    <w> = [ (\\alpha <\\bar{m}_d>(1 - c_{A0}) - c_{ss} - c_{dd}) \\vec{x}_d\n",
    "            + (\\alpha \\sigma^2 * <\\bar{m}_s> - c_{ds}) \\vec{x}_s\n",
    "          ] / (\\beta + \\alpha <\\bar{m}_d> - c_{dd} - c_{ss})\n",
    "        \n",
    "    where the coefficients are\n",
    "    \n",
    "    c_dd = \\alpha^2 \\int_0^\\infty d\\tau <\\bar{m}_d(t) \\bar{m}_d(t - \\tau)>\n",
    "    c_ss = \\alpha^2 \\int_0^\\infty d\\tau  \\sigma^2 e^{-\\tau/\\tau_c} <\\bar{m}_s(t) \\bar{m}_s(t - \\tau)> \n",
    "    c_{ds} = \\alpha^2 \\int_0^\\infty d\\tau  \\sigma^2 e^{-\\tau/\\tau_c} <\\bar{m}_s(t) \\bar{m}_d(t - \\tau)>\n",
    "    c_{A0} = \\alpha^2 \\int_0^\\infty d\\tau (\\tau*<\\bar{m}_d(t) \\bar{m}_d(t - \\tau)>)\n",
    "    \n",
    "    To derive this corrected expression, we neglected correlations between m and nu, \n",
    "    as they are subdominant comparing to auto-correlation of m. Hence,  all\n",
    "    correction terms linear or cubic in nu were zero, explaining why the corrected\n",
    "    <w> is relatively simple. \n",
    "    \"\"\"\n",
    "    alph, bet = rates\n",
    "    # First, get the steady-state values of \\bar{m}_d and \\bar{m}_s\n",
    "    back_comp_ds = np.stack([(back_comp[0] + back_comp[1])/2, back_comp[0]-back_comp[1]])\n",
    "    mean_m_d = fixed_m.dot(back_comp_ds[0])\n",
    "    mean_m_s = fixed_m.dot(back_comp_ds[1])\n",
    "    if use_bar:  # using \\bar{m} = m_i - eta*sum_{j \\neq i} m_j\n",
    "        mean_m_d = mean_m_d*(1.0 + eta) - eta*np.sum(mean_m_d, axis=0)  # Remove other neurons\n",
    "        mean_m_s = mean_m_s*(1.0 + eta) - eta*np.sum(mean_m_s, axis=0)\n",
    "    else:   # Using single neuron's m\n",
    "        pass\n",
    "    \n",
    "    # Next, for each neuron, compute the correlation terms c_{ss}, c_{dd}, c_{ds}, c_{A0}\n",
    "    # and use them to compute the corrected predicted fixed point for <w>\n",
    "    n_neu = msr.shape[1]\n",
    "    fixed_w = np.zeros([n_neu, msr.shape[-1]])\n",
    "    for i in range(n_neu):\n",
    "        # We need the time series of \\bar{m}_d and \\bar{m}_s\n",
    "        mbarser = (1.0 + eta)*msr[:, i] - eta*np.sum(msr, axis=1)\n",
    "        mbarser_d = mbarser.dot(back_comp_ds[0]) - mean_m_d[i]\n",
    "        mbarser_d = mbarser_d - np.mean(mbarser_d)\n",
    "        mbarser_s = mbarser.dot(back_comp_ds[1])\n",
    "        mbarser_s = mbarser_s - np.mean(mbarser_s)\n",
    "        \n",
    "        # c_{dd} and c_{A0}\n",
    "        mm_dd = correlate_stats(mbarser_d, mbarser_d, blocksize=int(2.0/alph))\n",
    "        c_dd = alph**2 * np.sum(mm_dd) * dt\n",
    "        taurange = np.arange(mm_dd.size)*dt\n",
    "        a0 = bet + alph*mean_m_d[i]\n",
    "        c_a0 = alph**2 * np.sum(mm_dd * (1-np.exp(-a0*taurange))/a0) * dt  # <m_d m_d tau>\n",
    "        \n",
    "        # c_{ss}, assuming that <nu(t) nu(t-\\tau) is exponentially decaying\n",
    "        mm_ss = correlate_stats(mbarser_s, mbarser_s, blocksize=int(2.0/alph))\n",
    "        c_ss = alph**2 * sigma2 * np.sum(mm_ss * np.exp(-taurange/t_nu)) * dt\n",
    "        \n",
    "        # c_{sd}\n",
    "        mm_sd = correlate_stats(mbarser_s, mbarser_d, blocksize=int(2.0/alph))\n",
    "        c_sd = alph**2 * sigma2 * np.sum(mm_sd * np.exp(-taurange/t_nu)) * dt\n",
    "        # Print intermediate information\n",
    "        print(\"c_dd, c_ss, c_sd, c_a0:\", c_dd, c_ss, c_sd, c_a0)\n",
    "        \n",
    "        # Compute the w steady-state average vector\n",
    "        denom = a0 - c_dd - c_ss\n",
    "        fixed_w[i] = ((alph*mean_m_d[i]*(1.0 - c_a0) - c_dd - c_ss) * back_comp_ds[0]\n",
    "                      + (alph*sigma2*mean_m_s[i]*(1.0 - c_a0) - c_sd) * back_comp_ds[1]) / denom\n",
    "    return np.asarray(fixed_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import decompose_nonorthogonal_basis\n",
    "from utils.statistics import seed_from_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simulation\n",
    "Hopefully, the inhibitory neurons can be combined to perfectly inhibit the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 2\n",
    "n_components = 2\n",
    "n_neurons = 2\n",
    "\n",
    "# Simulation times\n",
    "duration = 80000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.0025\n",
    "tau_avg = 300\n",
    "coupling_eta = 0.1\n",
    "inhib_rates = [.00025, .00005]  # alpha, beta\n",
    "#learnrate = 0.01\n",
    "#tau_avg = 50\n",
    "#inhib_rates = [0.005, 0.001]\n",
    "lambd_ibcm = 2.0\n",
    "ibcm_rates = [learnrate, tau_avg, coupling_eta, lambd_ibcm]\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=92347287)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions]) * lambd_ibcm\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant\n",
    "back_components = 0.25*np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i, i] = 1.0\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "average_nu = np.zeros(1)\n",
    "init_nu = np.zeros(1)\n",
    "init_bkvec = 0.5*back_components[0] + 0.5*back_components[1]\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "sigma2_nu = 0.09\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_coefs_mean = np.exp(-deltat/tau_nu)\n",
    "update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*deltat/tau_nu)))\n",
    "\n",
    "bk_update_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_init, update_bk, bk_init, ibcm_params,\n",
    "#    inhib_params, bk_params, tmax, dt, seed=None, noisetype=\"normal\"\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_ou_2inputs, init_back_list, ibcm_rates, \n",
    "                    inhib_rates, bk_update_params, duration, deltat, \n",
    "                    seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "tser, nuser, bkvecser, mser, cbarser, _, wser, yser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the synaptic weights against fixed points\n",
    "The analytical prediction neglecting correlations between $\\vec{m}$ and $\\nu$ is verified, provided that the time scales $\\tau_{\\nu}$ and $\\frac{1}{\\mu}$ are different enough. Computing corrections to account for incompletely separated time scales would be very hard, since the equation for $\\vec{m}$ is a multivariate, non-linear stochastic differential equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that neurons went to different fixed points (Pearson correlation of \\bar{c} = -1)\n",
    "transient = 30000\n",
    "print(\"Pearson correlation:\", np.corrcoef(cbarser[transient:].T)[0, 1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "analytical_m, _ = fixedpoints_m_2vectors(back_components, np.sqrt(sigma2_nu), coupling_eta)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# First neuron\n",
    "ax.plot(tser, mser[:, 0, 0], color=\"green\", label=\"Neuron 1 comp. 1\")\n",
    "ax.plot(tser, mser[:, 0, 1], color=\"xkcd:lime green\", label=\"Neuron 1 comp. 2\")\n",
    "ax.plot(tser, mser[:, 1, 0], color=\"purple\", label=\"Neuron 2 comp. 1\")\n",
    "ax.plot(tser, mser[:, 1, 1], color=\"violet\", label=\"Neuron 2 comp. 2\")\n",
    "ax.legend()\n",
    "ax.axhline(analytical_m[2, 0, 0], color=\"k\", ls=\"--\")\n",
    "ax.axhline(analytical_m[2, 0, 1], color=\"k\", ls=\"--\")\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"m components\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_barm, _ = fixedpoints_barm_2vectors(back_components, np.sqrt(sigma2_nu), coupling_eta, lambd=lambd_ibcm)\n",
    "# Plot dot products with x_a and x_b, in terms of reduced m vectors. \n",
    "# Take any of the fixed points for one neuron, its two dot products are the only two possible dot product values. \n",
    "analytical_mbara = np.dot(analytical_barm[2, 0], back_components[0])\n",
    "analytical_mbarb = np.dot(analytical_barm[2, 0], back_components[1])\n",
    "\n",
    "# Compute mbar for the two neurons. \n",
    "mbarser = mser*(1 + coupling_eta) - coupling_eta * np.sum(mser, axis=1, keepdims=True)\n",
    "mbarser_a = mbarser.dot(back_components[0])\n",
    "mbarser_b = mbarser.dot(back_components[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# First neuron\n",
    "skp = 10\n",
    "ax.plot(tser[::skp], mbarser_a[::skp, 0], color=\"red\", label=r\"Neuron 1, $\\overline{c}_a^-$\")\n",
    "ax.plot(tser[::skp], mbarser_b[::skp, 0], color=\"pink\", label=r\"Neuron 1, $\\overline{c}_b^-$\")\n",
    "ax.plot(tser[::skp], mbarser_a[::skp, 1], color=\"blue\", label=r\"Neuron 2, $\\overline{c}_a^+$\")\n",
    "ax.plot(tser[::skp], mbarser_b[::skp, 1], color=\"cyan\", label=r\"Neuron 2, $\\overline{c}_b^+$\")\n",
    "\n",
    "ax.axhline(analytical_mbara, color=\"k\", ls=\"--\", label=r\"$1 + 1/2 \\sigma$\")\n",
    "ax.axhline(analytical_mbarb, color=\"k\", ls=\"-.\", label=r\"$1 - 1/2 \\sigma$\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Dot products\")\n",
    "fig.set_size_inches(4.5, 3.5)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/two_odors/two_ibcm_neurons_mbar_dot_products_simulation.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the neurons' responses compared to analytical fixed points\n",
    "For a neuron at the $\\pm$ fixed point, its response should be $\\bar{c}_{\\pm} = 1 \\pm \\nu/\\sigma$ to any input vector $\\vec{x}(\\nu)$.  The plot below shows $\\bar{c}_{\\pm}$ versus each neuron's actual $\\bar{c}$ to prove it is indeed the case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "tslice = slice(int(duration/deltat)-2001, int(duration/deltat)-1901, 1)\n",
    "ax.plot(tser[tslice], cbarser[tslice, 0]/lambd_ibcm, color=\"cyan\", label=\"Response of neuron 0\", alpha=0.95)\n",
    "ax.plot(tser[tslice], cbarser[tslice, 1]/lambd_ibcm, color=\"pink\", label=\"Response of neuron 1\", alpha=0.95)\n",
    "ax.plot(tser[tslice], (1 + nuser[tslice, 0]/np.sqrt(sigma2_nu)), color=\"red\", label=\"Response at f.p. +\", alpha=0.95)\n",
    "ax.plot(tser[tslice], (1 - nuser[tslice, 0]/np.sqrt(sigma2_nu)), color=\"blue\", label=\"Response at f.p. -\", alpha=0.95)\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Neuron response\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the inhibitory neurons' weights $\\vec{w}^i$\n",
    "Compare to the analytical prediction that, on average, $\\vec{w}^i$ converges to $\\vec{x}(\\nu = \\pm \\sigma)$, i.e. to either input vector one standard deviation away from the mean input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical prediction for the w vectors, neglecting correlations between nu, m, and w\n",
    "analytical_m, _ = fixedpoints_m_2vectors(back_components, np.sqrt(sigma2_nu), coupling_eta, lambd=lambd_ibcm)\n",
    "analytical_mbar, _ = fixedpoints_barm_2vectors(back_components, np.sqrt(sigma2_nu), coupling_eta, lambd=lambd_ibcm)\n",
    "fixed_wvecs = fixedpoints_w_2vectors(inhib_rates, analytical_mbar[3], back_components, sigma2_nu, lambd=lambd_ibcm)\n",
    "\n",
    "# Analytical prediction above, plus first-order corrections from numerical estimates of autocorrelations\n",
    "#empirical_corrected_w = fixedpoints_w_empirical_corrections(inhib_rates, sigma2_nu, back_components, \n",
    "#                        analytical_m[3], mser[transient:], use_bar=True, eta=coupling_eta, dt=deltat, t_nu=tau_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "skp = 10\n",
    "#colors_choice = [\"green\", \"xkcd:lime green\", \"purple\", \"violet\"]\n",
    "ax.plot(tser[::skp], wser[::skp, 0, 0], color=\"red\", label=r\"$\\vec{w}_1$ elem. 1\", alpha=0.95)\n",
    "ax.plot(tser[::skp], wser[::skp, 0, 1], color=\"pink\", label=r\"$\\vec{w}_1$ elem. 2\", alpha=0.95)\n",
    "ax.plot(tser[::skp], wser[::skp, 1, 0], color=\"blue\", label=r\"$\\vec{w}_2$ elem. 1\", alpha=0.8)\n",
    "ax.plot(tser[::skp], wser[::skp, 1, 1], color=\"cyan\", label=r\"$\\vec{w}_2$ elem. 2\", alpha=0.8)\n",
    "\n",
    "ax.axhline(fixed_wvecs[0, 0], color=\"k\", label=r\"Analytical $\\vec{w}$\", ls=\"--\")\n",
    "ax.axhline(fixed_wvecs[0, 1], color=\"k\", ls=\"--\")\n",
    "#ax.axhline(empirical_fixed_w[0, 0], color=\"k\", label=\"Prediction from empirical means\", ls=\":\")\n",
    "#ax.axhline(empirical_fixed_w[0, 1], color=\"k\", ls=\":\")\n",
    "#ax.axhline(empirical_corrected_w[0, 0], color=\"orange\", label=\"With 1st order corrections\", ls=\"-.\")\n",
    "#ax.axhline(empirical_corrected_w[0, 1], color=\"orange\", ls=\"-.\")\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Elements of inhibitory neuron vectors\")\n",
    "ax.legend(ncol=2, labelspacing=0.5)\n",
    "fig.set_size_inches(4.5, 3.5)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/two_odors/ibcm_inhibition_2d_analytical_numerical_w_fixed_points.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition on the basis of background components\n",
    "back_components_ds = np.stack([(back_components[0] + back_components[1])/2, back_components[0]-back_components[1]])\n",
    "print(decompose_nonorthogonal_basis(np.mean(wser[transient:, 0], axis=0), back_components_ds.T))\n",
    "print(decompose_nonorthogonal_basis(fixed_wvecs[0], back_components_ds.T))\n",
    "print(decompose_nonorthogonal_basis(fixed_wvecs[1], back_components_ds.T))\n",
    "#print(decompose_nonorthogonal_basis(empirical_corrected_w[0], back_components_ds.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the vectors themselves. \n",
    "# Plot time course of m vector\n",
    "fig, ax = plt.subplots()\n",
    "aprops = dict(arrowstyle=\"<-\", color=\"k\", lw=3)\n",
    "input_vecs = back_components\n",
    "ax.annotate(r\"$\\vec{x}_a$\", xy=(0, 0), xytext=input_vecs[0], arrowprops=aprops, xycoords=\"data\", ha=\"center\")\n",
    "ax.annotate(r\"$\\vec{x}_b$\", xy=(0, 0), xytext=input_vecs[1], arrowprops=aprops, xycoords=\"data\", ha=\"center\")\n",
    "#ax.annotate(r\"$\\vec{x}_{mean}$\", xy=(0, 0), xytext=np.sum(input_vecs, axis=0)/2, arrowprops=aprops,\n",
    "#            xycoords=\"data\", ha=\"center\")\n",
    "\n",
    "# Compare to analytical fixed points\n",
    "aprops = dict(arrowstyle=\"<-\", color=\"blue\", lw=3)\n",
    "ax.annotate(\"\", xy=(0, 0), xytext=analytical_barm[2, 0], arrowprops=aprops, xycoords=\"data\", ha=\"center\", va=\"center\")\n",
    "ax.annotate(r\"$\\vec{\\overline{m}}^+$\", xy=(analytical_barm[2, 0, 0], analytical_barm[2, 0, 1]*0.8), \n",
    "            xycoords=\"data\", ha=\"center\", color=aprops[\"color\"])\n",
    "\n",
    "aprops = dict(arrowstyle=\"<-\", color=\"red\", lw=3)\n",
    "ax.annotate(\"\", xy=(0, 0), xytext=analytical_barm[2, 1], arrowprops=aprops, xycoords=\"data\", ha=\"center\", va=\"center\")\n",
    "ax.annotate(r\"$\\vec{\\overline{m}}^-$\", xy=(analytical_barm[2, 1, 0]*0.8, analytical_barm[2, 1, 1]),\n",
    "            xycoords=\"data\", ha=\"center\", color=aprops[\"color\"])\n",
    "minx = np.amin(analytical_barm[:, :, 0])*1.1\n",
    "maxx = np.amax(analytical_barm[:, :, 0])*1.1\n",
    "miny = np.amin(analytical_barm[:, :, 1])*1.1\n",
    "maxy = np.amax(analytical_barm[:, :, 1])*1.1\n",
    "\n",
    "# Also illustrate components that the model learns to inhibit? Very small\n",
    "aprops = dict(arrowstyle=\"<-\", color=\"pink\", lw=1.25)\n",
    "ax.annotate(\"\", xy=(0, 0), xytext=fixed_wvecs[0], arrowprops=aprops, xycoords=\"data\", ha=\"center\", va=\"center\")\n",
    "ax.annotate(r\"$\\vec{w}^-$\", xy=fixed_wvecs[0], \n",
    "            xycoords=\"data\", ha=\"left\", color=aprops[\"color\"])\n",
    "\n",
    "aprops = dict(arrowstyle=\"<-\", color=\"cyan\", lw=1.25)\n",
    "ax.annotate(\"\", xy=(0, 0), xytext=fixed_wvecs[1], arrowprops=aprops, xycoords=\"data\", ha=\"center\", va=\"center\")\n",
    "ax.annotate(r\"$\\vec{w}^+$\", xy=fixed_wvecs[1],\n",
    "            xycoords=\"data\", ha=\"center\", color=aprops[\"color\"])\n",
    "\n",
    "\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_ylim([miny, maxy])\n",
    "ax.set_xlim([minx, maxx])\n",
    "ax.set(xlabel=\"Dimension 1\", ylabel=\"Dimension 2\")\n",
    "fig.set_size_inches(4.5, 4.5)\n",
    "# fig.savefig(\"figures/two_odors/two_ibcm_neurons_background_mbar_w_vectors.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background before and after inhibition\n",
    "Check how well this 2D background is inhibited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_background_neurons_inhibition, plot_background_norm_inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared norm: prediction vs numerical\n",
    "def squared_norm(v, axis=-1):\n",
    "    return np.sum(v**2, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted and numerical reduction factors\n",
    "# Predictions for s\n",
    "x_determ = np.sum(back_components, axis=0) / 2.0\n",
    "reduct_factor_predict = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "s_avg_predict = relu_inplace(x_determ * reduct_factor_predict)\n",
    "\n",
    "# Predictions for s^2\n",
    "x_determ = np.sum(back_components, axis=0) / 2.0\n",
    "x_stoch = (back_components[0] - back_components[1])\n",
    "x2_expectation = squared_norm(x_determ) + sigma2_nu*squared_norm(x_stoch)\n",
    "reduct_factor_predict = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "s_norm2_predict = reduct_factor_predict**2 * x2_expectation\n",
    "\n",
    "# Numerics for s and s^2\n",
    "transient = 30000\n",
    "reduct_factor_numeric = np.mean(yser[transient:], axis=0) / np.mean(bkvecser[transient:], axis=0)\n",
    "s2_norm_numeric = np.mean(yser[transient:]*yser[transient:])\n",
    "reduct_factor_numeric2 =  s2_norm_numeric / np.mean(bkvecser*bkvecser)\n",
    "\n",
    "print(\"--- Reduction of s components ---\")\n",
    "print(\"  Predict:\", reduct_factor_predict)\n",
    "print(\"  Numeric:\", reduct_factor_numeric)\n",
    "print(\"--- Reduction of s^2 norm ---\")\n",
    "print(\"  Predict:\", reduct_factor_predict**2)\n",
    "print(\"  Numeric:\", reduct_factor_numeric2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generic plotting function\n",
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser, bkvecser, yser, skp=50)\n",
    "fig.set_size_inches(6.5, 2.5)\n",
    "# Add lines at predicted average\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axhline(s_avg_predict[i], color=\"y\", ls=\"--\", \n",
    "               label=\"Predicted avg. ({:.1f} %)\".format(reduct_factor_predict*100), lw=3.)\n",
    "    ax.axhline(np.mean(yser[transient:, i], axis=0), ls=\"--\", color=\"xkcd:pink\", label=\"Numerical avg.\", lw=1.5)\n",
    "    txt = ax.annotate(\"{:.1f} %\".format(reduct_factor_numeric[i]*100), xy=(tser[-1]/1000, s_avg_predict[i]*3), \n",
    "               ha=\"right\", va=\"bottom\", xycoords=\"data\")\n",
    "    txt.set_bbox(dict(facecolor='w', alpha=0.7, edgecolor='w'))\n",
    "axes[-1].legend(bbox_to_anchor=(1, 1), loc=\"upper left\", fontsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/two_odors/inhibition_gaussian_background_neurons_2odors.pdf\", \n",
    "#    transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generic plotting function\n",
    "fig, ax, bk_norm2, ss_norm2  = plot_background_norm_inhibition(tser, bkvecser, \n",
    "                                            yser, norm_fct=squared_norm, skp=50)\n",
    "fig.set_size_inches(5., 2.5)\n",
    "# Add lines at predicted average\n",
    "ax.axhline(s_norm2_predict, color=\"y\", ls=\"--\", \n",
    "           label=r\"Predicted norm${}^2$\" + \" ({:.1f} %)\".format(reduct_factor_predict**2*100), lw=3.)\n",
    "ax.axhline(np.mean(ss_norm2[transient:]), ls=\"--\", color=\"xkcd:pink\", \n",
    "               label=r\"Numerical norm${}^2$\", lw=1.5)\n",
    "ax.annotate(\"{:.1f} %\".format(reduct_factor_numeric2*100), xy=(tser[-1]/1000, s_norm2_predict*10), \n",
    "           ha=\"right\", va=\"bottom\", xycoords=\"data\")\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\", fontsize=9)\n",
    "ax.set_ylabel(\"Activity vector squared norm\")\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/two_odors/inhibition_gaussian_background_squarednorm_2odors.pdf\", \n",
    "#    transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "tslice = slice(transient, int(duration/deltat), 100)\n",
    "ax.scatter(bkvecser[tslice, 0], bkvecser[tslice, 1], color=\"xkcd:light red\", label=\"Un-inhibited\")\n",
    "ax.scatter(yser[tslice, 0], yser[tslice, 1], color=\"xkcd:burgundy\", label=\"Inhibited\")\n",
    "ax.plot(0, 0, \"ks\", ms=8)\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Dim. 1\", ylabel=\"Dim. 2\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/two_odors/ibcm_inhibition_2d_scatter.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary analysis of the model\n",
    "Here is collected relevant code from older notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence time scales\n",
    "See Gautam's notes for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import integrate_ibcm, integrate_ibcm_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_convergence_times_2d(init_m_sd, norms2_x_sd, mu, sigm2, alph=0.9):\n",
    "    \"\"\" Predict times for m_s and m_d to converge to fixed points. \n",
    "    Valid for small sigma^2, when we expect m_s to converge before m_d\n",
    "    If sigma^2 or the initial value of m_d are too large, the prediction\n",
    "    for ts is still good probably, but for td it will be bad, \n",
    "    because we assumed that m_s reaches its steady-state value of 1\n",
    "    to compute the time td. \n",
    "    Args:\n",
    "        init_m_sd (list of 2 floats): initial value of m.x_s and m.x_d\n",
    "        norms2_x_sd (list of 2 floats): squared norm of x_s and x_d\n",
    "        mu (float): learning rate\n",
    "        sigm2 (float): variance of nu\n",
    "    Returns:\n",
    "        ts (float): time for m_s to reach steady-state\n",
    "        td (float): time for m_d to reach steady-state, \n",
    "            assuming m_s reached steady-state much faster. \n",
    "    \"\"\"\n",
    "    ts = (1.0/init_m_sd[0] - 1.0) + np.log(alph*(1.0 - init_m_sd[0]) / (1.0 - alph) / init_m_sd[0])\n",
    "    ts /= mu * norms2_x_sd[0]\n",
    "    # Time to converge to 90 %\n",
    "    sig = np.sqrt(sigm2)\n",
    "    #td = np.log(alph*np.sqrt(1.0 - sigm2*init_m_sd[1]**2)/(sig*init_m_sd[1]*np.sqrt(1-alph**2)))\n",
    "    td = np.log(alph / np.sqrt(sigm2) / init_m_sd[1])\n",
    "    td = td / (mu * norms2_x_sd[1]*sigm2) + ts\n",
    "    return ts, td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_convergence_time(tpts, mds, mdd, sigm2, alph=0.9):\n",
    "    \"\"\" \n",
    "    tpts: time points\n",
    "    mds: time series of m \\cdot \\vec{x}_s\n",
    "    mdd: time series of m \\cdot \\vec{x}_d\n",
    "    sigm2: variance of nu\n",
    "    \"\"\"\n",
    "    # Check when mds reaches close to 1 (analytical ss value)\n",
    "    # and when mdd reaches close to \\pm 1 / sigma\n",
    "    ts = tpts[np.argmax(mds > alph)]\n",
    "    td = tpts[np.argmax(np.abs(mdd) > alph / np.sqrt(sigm2))]\n",
    "    return ts, td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_convergence_time(seed=53235417):\n",
    "    input_vecs = np.asarray([[1, 0.25], [0.25, 1]]) / np.sqrt(1**2+0.25**2)\n",
    "    back_components = input_vecs\n",
    "    tmax = 160000\n",
    "    ## Compute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "    sigma2_nu = 0.09\n",
    "    tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "    deltat = 1.0\n",
    "    average_nu = 0.0\n",
    "    learning_mu = 0.001\n",
    "    tau_theta = 200\n",
    "    update_coefs_mean = np.exp(-deltat/tau_nu)\n",
    "    update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*deltat/tau_nu)))\n",
    "    init_back_vec = np.sum(input_vecs, axis=0) / 2\n",
    "\n",
    "    bk_update_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]\n",
    "    \n",
    "    \n",
    "    # Show an example\n",
    "    m_init = np.asarray([0.05, 0.025])\n",
    "    tser, mser, nuser, cser, _, bkvecser = integrate_ibcm(m_init, update_ou_2inputs, \n",
    "                        [np.zeros(1), init_back_vec], bk_update_params, tmax, deltat, \n",
    "                        learnrate=learning_mu, seed=seed, noisetype=\"normal\", tavg=tau_theta)\n",
    "    \n",
    "    mdota_ser = np.dot(mser, input_vecs[0])\n",
    "    mdotb_ser = np.dot(mser, input_vecs[1])\n",
    "    mdots_ser = 0.5*(mdota_ser + mdotb_ser)\n",
    "    mdotd_ser = mdota_ser - mdotb_ser\n",
    "\n",
    "    initial_mdotsd = [mdots_ser[0], mdotd_ser[0]]\n",
    "    norms_vecsd = [0.25*np.sum((input_vecs[0] + input_vecs[1])**2), np.sum((input_vecs[0] - input_vecs[1])**2)]\n",
    "    predict_ts, predict_td = analytical_convergence_times_2d(initial_mdotsd, norms_vecsd, learning_mu, sigma2_nu)\n",
    "    print(predict_ts, predict_td)\n",
    "    print(find_convergence_time(tser, mdots_ser, mdotd_ser, sigma2_nu))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes = axes.flatten()\n",
    "    ax = axes[0]\n",
    "    # Plot dot products with vectors x_a and x_b\n",
    "    skp = 100\n",
    "    ax.plot(tser[::skp], mdota_ser[::skp], color=\"red\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_a$\")\n",
    "    ax.plot(tser[::skp], mdotb_ser[::skp], color=\"pink\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_b$\")\n",
    "\n",
    "    # Analytical predictions\n",
    "    analytical_mdots = [1.0 + 1.0 / (2.0 * np.sqrt(sigma2_nu)), 1.0 - 1.0 / (2.0 * np.sqrt(sigma2_nu))]\n",
    "    ax.axhline(analytical_mdots[0], label=r\"$1 + 1/(2 \\sigma)$\", ls=\"--\", color=\"k\")\n",
    "    ax.axhline(analytical_mdots[1], label=r\"$1 - 1/(2 \\sigma)$\", ls=\"-.\", color=\"k\")\n",
    "    ax.set(xlabel=\"Time\", ylabel=r\"Dot products $\\vec{m} \\cdot \\vec{x}_{a, b}$\")\n",
    "\n",
    "    # Plot dot products with x_s and x_d\n",
    "    ax = axes[1]\n",
    "    ax.plot(tser[::skp], mdotd_ser[::skp], color=\"orange\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_d$\")\n",
    "    ax.plot(tser[::skp], mdots_ser[::skp], color=\"blue\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_s$\")\n",
    "    # Analytical predictions\n",
    "    analytical_mdots = [1.0 / np.sqrt(sigma2_nu), -1.0 / np.sqrt(sigma2_nu)]\n",
    "\n",
    "    ax.axhline(analytical_mdots[0], label=r\"$\\pm 1/\\sigma$\", ls=\"-.\", color=\"xkcd:orangey brown\")\n",
    "    ax.axhline(1.0, label=r\"$\\vec{m} \\cdot \\vec{x}_s = 1$\", color=\"xkcd:marine\", ls=\"--\")\n",
    "    ax.set(xlabel=\"Time\", ylabel=r\"Dot products $\\vec{m} \\cdot \\vec{x}_{s, d}$\")\n",
    "    # Convergence times predicted\n",
    "    for ax in axes:\n",
    "        ax.axvline(predict_ts, color=\"cyan\", ls=\":\", label=r\"Conv. time $m_s$\")\n",
    "        ax.axvline(predict_td, color=\"orange\", ls=\"--\", label=r\"Conv. time $m_d$\")\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([0, 40000, 80000, 120000, 160000])\n",
    "    fig.set_size_inches(7.5, 3.)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = example_convergence_time(seed=53235417)\n",
    "# fig.savefig(\"figures/two_odors/ibcm_neuron_2d_convergence_time_example.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_convergence_time(n_tries=2, seed_sequence=None):\n",
    "    # Check how ts and td scale as a function of initial x_d and x_s\n",
    "    # n_tries x n_tries values of initial x_s and x_d are tried\n",
    "    # Plot t_d - t_s, which should depend on eps_d only once this leading\n",
    "    # order behaviour of t_s is removed. \n",
    "    \n",
    "    # Random number generation business\n",
    "    if seed_sequence is None:\n",
    "        seed_sequence = np.random.SeedSequence()\n",
    "    all_seeds = list(seed_sequence.spawn(n_tries*n_tries))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    input_vecs = np.asarray([[1, 0.25], [0.25, 1]]) / np.sqrt(1**2+0.25**2)\n",
    "    back_components = input_vecs\n",
    "    tmax = 160000\n",
    "    # Compute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "    sigma2_nu = 0.09\n",
    "    tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "    deltat = 1.0\n",
    "    average_nu = 0.0\n",
    "    learning_mu = 0.001\n",
    "    tau_theta = 200\n",
    "    update_coefs_mean = np.exp(-deltat/tau_nu)\n",
    "    update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*deltat/tau_nu)))\n",
    "    init_back_vec = np.sum(input_vecs, axis=0) / 2\n",
    "\n",
    "    bk_update_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]\n",
    "\n",
    "    # Loop over pairs of x_s, x_d values\n",
    "    epss_axis = np.geomspace(0.02, 0.2, n_tries)\n",
    "    epsd_axis = np.geomspace(0.01, 0.1, n_tries)\n",
    "    ts_grid = np.zeros([2, n_tries, n_tries])  # Should only depend on epss\n",
    "    td_grid = np.zeros([2, n_tries, n_tries])  \n",
    "    x_s, x_d = np.sum(input_vecs, axis=0)/2, input_vecs[0] - input_vecs[1]    \n",
    "    norms_vecsd = [np.sum(x_s**2), np.sum(x_d**2)]\n",
    "    \n",
    "    for i, epss in enumerate(epss_axis):\n",
    "        for j, epsd in enumerate(epsd_axis):\n",
    "            # Combine x_s and x_d to form initial m vector (x_s, x_d are orthogonal, so easy)\n",
    "            m_init = epss * x_s / norms_vecsd[0] + epsd * x_d / norms_vecsd[1]\n",
    "            tser, mser, nuser, cser, _, bkvecser = integrate_ibcm(m_init, update_ou_2inputs, \n",
    "                        [np.zeros(1), init_back_vec], bk_update_params, tmax, deltat, \n",
    "                        learnrate=learning_mu, seed=all_seeds.pop(), noisetype=\"normal\", tavg=tau_theta)\n",
    "    \n",
    "            mdota_ser = np.dot(mser, input_vecs[0])\n",
    "            mdotb_ser = np.dot(mser, input_vecs[1])\n",
    "            mdots_ser = 0.5*(mdota_ser + mdotb_ser)\n",
    "            mdotd_ser = mdota_ser - mdotb_ser\n",
    "\n",
    "            initial_mdotsd = [mdots_ser[0], mdotd_ser[0]]  # Should equal epss, epsd\n",
    "    \n",
    "            ts_grid[0, i, j], td_grid[0, i, j] = analytical_convergence_times_2d(initial_mdotsd, norms_vecsd, learning_mu, sigma2_nu)\n",
    "            ts_grid[1, i, j], td_grid[1, i, j] = find_convergence_time(tser, mdots_ser, mdotd_ser, sigma2_nu)\n",
    "        print(\"Completed {} points at eps_s = {}\".format(n_tries, epss))\n",
    "    \n",
    "    return [epss_axis, epsd_axis], [ts_grid, td_grid]\n",
    "\n",
    "def plot_time_analysis_results(eps_axes, time_grids):\n",
    "    n_tries = eps_axes[0].size\n",
    "    epss_axis, epsd_axis = eps_axes\n",
    "    ts_grid, td_grid = time_grids\n",
    "    #print(\"t_s results:\\n\", ts_grid)\n",
    "    #print(\"t_d results:\\n\", td_grid)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes = axes.flatten()\n",
    "    # ts as a function of epss; should not depend on eps_d\n",
    "    ax = axes[0]\n",
    "    colors = sns.color_palette(\"mako\", n_colors=n_tries)\n",
    "    for j in range(epsd_axis.size):\n",
    "        ax.plot(epss_axis, ts_grid[1, :, j], color=colors[j], ls=\"--\",\n",
    "                label=r\"$\\epsilon_d = {:.3f}$\".format(epsd_axis[j]), marker=\"o\")\n",
    "    # Plot theory, independent of eps_d\n",
    "    ax.plot(epss_axis, ts_grid[0, :, 0], label=\"Analytical\", color=\"k\", ls=\"-\", lw=2.0)\n",
    "    ax.set(xlabel=r\"$\\epsilon_s$ (initial $m_s = \\vec{m} \\cdot \\vec{x}_s$)\", \n",
    "           ylabel=r\"$t_s$ ($\\vec{m} \\cdot \\vec{x}_s$ convergence time)\", \n",
    "           title=\"First phase\", xscale=\"log\", yscale=\"log\")\n",
    "    ax.legend(title=\"Simulation\", fontsize=9)\n",
    "    \n",
    "    # td - ts as a function of epsd\n",
    "    ax = axes[1]\n",
    "    colors = sns.color_palette(\"flare\", n_colors=n_tries)\n",
    "    # Plot simulations\n",
    "    for i in range(epss_axis.size):\n",
    "        ax.plot(epsd_axis, td_grid[1, i] - ts_grid[1, i], color=colors[i], ls=\"--\",\n",
    "                label=r\"$\\epsilon_s = {:.3f}$\".format(epss_axis[i]), marker=\"o\")\n",
    "    # Plot theory, independent of eps_s\n",
    "    ax.plot(epsd_axis, td_grid[0, 0] - ts_grid[0, 0], label=\"Analytical\", color=\"k\", ls=\"-\", lw=2.0)\n",
    "    ax.set(xlabel=r\"$\\epsilon_d$ (initial $m_d = \\vec{m} \\cdot \\vec{x}_d$)\", \n",
    "           ylabel=r\"$t_d$ ($\\vec{m} \\cdot \\vec{x}_d$ convergence time)\", \n",
    "           title=\"Second phase\", xscale=\"log\", yscale=\"log\")\n",
    "    ax.legend(title=\"Simulation\", fontsize=9)\n",
    "    \n",
    "    fig.set_size_inches(7.5, 3.5)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_axes, time_grids = scaling_convergence_time(n_tries=4, \n",
    "                            seed_sequence=np.random.SeedSequence(seed_from_gen(rgen_meta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_time_analysis_results(eps_axes, time_grids)\n",
    "# fig.savefig(\"figures/two_odors/ibcm_neuron_2d_convergence_time_scaling.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of IBCM neurons to opposite fixed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import integrate_ibcm_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_origin_basins_2neurons(init_ampli, bk_sigma2, bk_vecs, learnrate=0.02, seed0=14345124, \n",
    "                          tavg=20, coupling=0.1, nsimuls=100):\n",
    "    \"\"\" Function to produce a 2D map of the basins of attraction of fixed points for neuron 2, \n",
    "    when the initial conditions for neuron 1 are kept constant. Ideally, start m_1 close to its \n",
    "    fixed point, so we have the basins for neuron 2 in a slice through the fixed points of neuron 1. \n",
    "    \n",
    "    seed0 is for the first simulation done, other initial points on the grid have a different seed. \n",
    "    \n",
    "    Uses the update function update_ou_2inputs, nu with mean 0.5, initial value 0.5, normal noise, \n",
    "    so we don't have to input those parameters. \n",
    "    tmax and dt are also chosen in the function, no need to input. \n",
    "    \n",
    "    Returns a 1darray of pearson correlation coefficients and a 3d array of initial vectors\n",
    "    \"\"\"\n",
    "    # Default simulation parameters\n",
    "    tmax = 7000\n",
    "    transient = 2500\n",
    "    dt = 1\n",
    "    nu_init = 0.5*np.ones(1)\n",
    "    back_init = [nu_init, (0.5+nu_init)*back_components[0] + (0.5-nu_init)*back_components[1]]\n",
    "    nu_mean = 0.5\n",
    "    tau_nu = 2\n",
    "        \n",
    "    # Compute the coefficients in the O-U update rule\n",
    "    update_coefs_mean_loc = np.exp(-dt/tau_nu)\n",
    "    update_coefs_noise_loc = np.sqrt(bk_sigma2*(1 - np.exp(-2*dt/tau_nu)))\n",
    "    # Mean of nu is zero\n",
    "    bk_update_params_loc = [0.0, update_coefs_mean_loc, update_coefs_noise_loc, bk_vecs]\n",
    "\n",
    "    # Set up the grid of initial m2 vectors: find the two fixed points and try the \n",
    "    # rectangle with a corner at (0, 0) and sides extending from 0 to 1.5/(1-eta) times the max\n",
    "    # component in each direction. We use the fixed points for a single neuron, \n",
    "    # because the components of fixed points for coupled neurons change by 1/(1 \\pm eta) at most.\n",
    "    norma = np.sum(bk_vecs[0]**2)\n",
    "    normb = np.sum(bk_vecs[1]**2)\n",
    "    c_plus, c_minus = 1 + 1/(2*np.sqrt(bk_sigma2)), 1 - 1/(2*np.sqrt(bk_sigma2))\n",
    "    overlap = bk_vecs[0].dot(bk_vecs[1])\n",
    "    m_fixplus =  (c_plus*normb - overlap*c_minus)/(norma*normb-overlap**2)*bk_vecs[0]\n",
    "    m_fixplus += (c_minus*norma - overlap*c_plus)/(norma*normb-overlap**2)*bk_vecs[1]\n",
    "    m_fixminus =  (c_minus*normb - overlap*c_plus)/(norma*normb-overlap**2)*bk_vecs[0]\n",
    "    m_fixminus += (c_plus*norma - overlap*c_minus)/(norma*normb-overlap**2)*bk_vecs[1]\n",
    "    \n",
    "    # Run nsimuls simulations with a different random noise every time.\n",
    "    rgen_meta = np.random.default_rng(seed=((seed0*52441435) % 1239025234))\n",
    "    pearson_container = np.zeros(nsimuls)\n",
    "    init_vecs_container = np.zeros([nsimuls, 2, 2])\n",
    "    for i in range(nsimuls):\n",
    "        init_vecs_container[i] = init_ampli * rgen_meta.normal(size=4).reshape(2, 2)\n",
    "        seedi = seed0 + i**2\n",
    "        result = integrate_ibcm_network(init_vecs_container[i], update_ou_2inputs, \n",
    "                        back_init, bk_update_params_loc, tmax, dt, \n",
    "                        learnrate=learnrate, seed=seedi, noisetype=\"normal\", \n",
    "                        tavg=tavg, coupling=coupling)\n",
    "\n",
    "        pearson_container[i] = np.corrcoef(result[-3][transient:].T)[0, 1]  # off-diagonal element of the correl matrix\n",
    "    \n",
    "    return pearson_container, init_vecs_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_run_statistics(correls, all_init_vecs):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes = axes.flatten()\n",
    "    ax = axes[0]\n",
    "    ax.hist(correls)\n",
    "    ax.set(xlabel=\"Pearson correlation\", ylabel=\"Number of simulations\")\n",
    "\n",
    "    print(np.count_nonzero(correls < 0) / correls.size)\n",
    "    # Study those cases where both went to same fixed point\n",
    "    args_where_same, = np.nonzero(correls > 0)\n",
    "    ax = axes[1]\n",
    "    if len(args_where_same) > 0:\n",
    "        max_comp = np.amax(np.abs(all_init_vecs[args_where_same])) * 1.05\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return 1\n",
    "    ax.set_xlim([-max_comp, max_comp])\n",
    "    ax.set_ylim([-max_comp, max_comp])\n",
    "    colors = sns.color_palette(n_colors=len(args_where_same))\n",
    "    for i in range(len(args_where_same)):\n",
    "        vecs = all_init_vecs[args_where_same[i]]\n",
    "        arrowprops = dict(color=colors[i], arrowstyle=\"->\")\n",
    "        ax.annotate(\"\", xy=vecs[0], xytext=(0, 0), arrowprops=arrowprops)\n",
    "        ax.annotate(\"\", xy=vecs[1], xytext=(0, 0), arrowprops=arrowprops)\n",
    "        ax.plot(vecs[0, 0:1], vecs[0, 1:2], ls=\"none\", marker=\"o\", alpha=0.0)\n",
    "        ax.plot(vecs[1, 0:1], vecs[1, 1:2], ls=\"none\", marker=\"o\", alpha=0.0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(\"Initial vectors when both neurons\\nconverge to same fixed point\")\n",
    "    fig.set_size_inches(6, 3.5)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_eta = 0.2\n",
    "correl_samples, initvecs_samples = study_origin_basins_2neurons(0.01, sigma2_nu, back_components, \n",
    "            learnrate=0.02, seed0=14124779, tavg=20, coupling=coupling_eta, nsimuls=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_run_statistics(correl_samples, initvecs_samples)\n",
    "# fig.savefig(\"figures/two_odors/two_ibcm_neurons_opposite_fixed_pts_statistics.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
