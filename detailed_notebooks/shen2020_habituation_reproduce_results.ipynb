{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "# For multiprocessing\n",
    "import multiprocessing\n",
    "from psutil import cpu_count\n",
    "# This one, with logical=False, is better than multiprocessing.cpu_count\n",
    "# https://stackoverflow.com/questions/40217873/multiprocessing-use-only-the-physical-cores\n",
    "\n",
    "# Change the forking method used by multiprocessing, avoids errors on Mac OS Catalina\n",
    "# e.g. https://github.com/matplotlib/matplotlib/issues/15410\n",
    "multiprocessing.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to model for odorants and receptors\n",
    "From Reddy et al. 2018, the way to define an odorant is to specify a vector of binding affinities, $\\vec{\\kappa}$, and a vector of activation efficacies, $\\vec{\\eta}$. For a background, we define $n_{mix}$ such odorants and combine them into $\\vec{\\eta}_{mix}$ and $\\vec{\\kappa}_{mix}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_odorant(n_rec, rgen, lambda_in=0.1):\n",
    "    \"\"\" Generate vectors eta and kappa^-1 for an odorant, with antagonism parameter rho. \n",
    "    \n",
    "    Args:\n",
    "        n_rec (int): number of receptor types, length of vectors\n",
    "        mean_in (float): average value of the input vector. \n",
    "        rgen (np.random.Generator): random generate (numpy >= 1.17)\n",
    "    Returns:\n",
    "        kappa1_vec (np.ndarray): 1d vector of receptor activities\n",
    "    \"\"\"\n",
    "    return rgen.exponential(scale=1.0/lambda_in, size=n_rec)\n",
    "\n",
    "def generate_background(n_rec, rgen, lambda_in=10.0):\n",
    "    \"\"\" Generate vectors eta and kappa^-1 for an odorant, with antagonism parameter rho. \n",
    "    \n",
    "    Args:\n",
    "        n_rec (int): number of receptor types, length of vectors\n",
    "        mean_in (float): average value of the input vector. \n",
    "        rgen (np.random.Generator): random generate (numpy >= 1.17)\n",
    "    Returns:\n",
    "        kappa1_vec (np.ndarray): 1d vector of receptor activities\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"You should use generate_odorant instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "inputs_test = []\n",
    "for i in range(100):\n",
    "    odi = generate_odorant(50, np.random.default_rng(seed=73924+i))\n",
    "    inputs_test.append(odi)\n",
    "inputs_test = np.concatenate(inputs_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following histograms should look gaussian with standard deviations 1 and 4\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5, 2.5)\n",
    "ax.hist(inputs_test)\n",
    "ax.set_xlabel(r\"Receptor input\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the olfactory network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_neural_tag(s_vec, w_vec, projmat, kc_sparsity=0.05, adapt_kc=True, n_pn_per_kc=3, fix_thresh=None):\n",
    "    \"\"\" Given the parameters of the Shen 2020 neural network, project the input layer s_vec\n",
    "    with the inhibitory feedback weights w_vec to the sparse kenyon cell (KC) output, \n",
    "    thresholding KCs below kc_thresh and then keeping only a fraction kc_sparsity of active KCs.\n",
    "    \n",
    "    Args:\n",
    "        s_vec (np.ndarray): input vector, activation of each receptor type\n",
    "        w_vec (np.ndarray): inhibition weights from LN1 to PN neurons\n",
    "        projmat (np.ndarray or sp.sparse.csr_matrix): projection matrix from PNs to KCs, \n",
    "            shape n_kc x n_receptors. Will use the .dot method of the matrix. \n",
    "\n",
    "    Returns:\n",
    "        z_set (set): sparse neural tag for the given activation odor. List of active KCs. \n",
    "    \"\"\"\n",
    "    # Extract useful information\n",
    "    n_kc = projmat.shape[0]\n",
    "    n_rec = projmat.shape[1]\n",
    "    # Number of connections from PNs to each KC. If three, then threshold equals mean of inputs\n",
    "    # Otherwise we need to correct for the fact that the more KCs project to each PN, \n",
    "    # the higher the signal of each PN will be compared to the case described in the paper (3 KCs to each PN)\n",
    "    if fix_thresh is not None:\n",
    "        kc_thresh = fix_thresh\n",
    "    elif adapt_kc:\n",
    "        kc_thresh = np.mean(s_vec) * n_pn_per_kc / 3\n",
    "    else:\n",
    "        kc_thresh = np.mean(s_vec)\n",
    "\n",
    "    # 1. Project on PNs, including inhibition from LN1\n",
    "    x_vec = np.maximum(s_vec - w_vec, 0)\n",
    "    #x_vec = s_vec - w_vec\n",
    "    #x_vec[x_vec<0] = 0\n",
    "    \n",
    "    # 2. Project on KCs\n",
    "    y_vec = projmat.dot(x_vec)\n",
    "    \n",
    "    # 3. Threshold noise:: will consider only positions in mask. \n",
    "    mask = (y_vec >= kc_thresh).astype(bool)\n",
    "    y_vec[np.logical_not(mask)] = 0.0\n",
    "    #mask2 = (projmat.dot(s_vec/6) >= kc_thresh)\n",
    "    #print(\"Non-zero elements after thresholding by mean {}: \".format(kc_thresh), np.count_nonzero(mask))\n",
    "    #print(\"Compare to expected:\", np.count_nonzero(mask2))\n",
    "    \n",
    "    # 4. Binarize: keep the np.ceil(0.05*n_kc) most active KCs non-zero KCs, \n",
    "    # or all the nonzero ones. Return a set of the indices of those cells. \n",
    "    # No arbitrary tie breaks, so can't just sort and take the first 0.05n_kc args\n",
    "    # TODO: make sure this is a fine format and we don't need the full vector\n",
    "    # Otherwise, use an array of booleans for the binary vector, or a sp.sparse?\n",
    "    thresh_keep = np.quantile(y_vec, 1.0 - kc_sparsity)\n",
    "    z_set = set(np.nonzero(np.logical_and(mask, y_vec >= thresh_keep))[0])\n",
    "    #y_vec[y_vec < thresh_keep] = 0.0\n",
    "    #z_set = set(np.nonzero(y_vec)[0])\n",
    "    return z_set\n",
    "\n",
    "def jaccard(s1, s2):\n",
    "    if len(s1) > 0 or len(s2) > 0:  # includes the special case of s1 and s2 empty sets\n",
    "        return len(s1 & s2) / len(s1 | s2)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_proj_mat(n_kc, n_rec, rgen, fraction_filled=6/50):\n",
    "    n_per_row = int(fraction_filled*n_rec)\n",
    "    data = np.ones(n_per_row*n_kc, dtype=\"uint8\")\n",
    "    row_ind = np.arange(n_per_row*n_kc, dtype=int) // n_per_row\n",
    "    col_ind = np.ones((n_kc, n_per_row), dtype=\"int\")\n",
    "    for i in range(n_kc):\n",
    "        col_ind[i] = rgen.choice(n_rec, size=n_per_row, replace=False)\n",
    "    mat = sp.sparse.csr_matrix((data, (row_ind, col_ind.ravel())), shape=(n_kc, n_rec), dtype=\"uint8\")\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "# Building a random matrix for testing\n",
    "rdgen_test = np.random.default_rng(8438)\n",
    "proj_mat = create_sparse_proj_mat(2000, 50, rdgen_test)\n",
    "\n",
    "odor_test = generate_odorant(50, rdgen_test)\n",
    "# Low concentration regime, where very few KC get activated at all\n",
    "tag_test = project_neural_tag(odor_test, np.zeros(50), proj_mat)\n",
    "print(len(tag_test))\n",
    "print(tag_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check locality preservation: generate a bunch of odorants, compute their tags, \n",
    "# and compare the similarity of their tags based on the similariy of the s_vecs \n",
    "# (or the eta vecs and kappa_vecs?)\n",
    "# This isn't the best test, but any positive correlation is a good sign. \n",
    "n_test = 1000\n",
    "tag_test = project_neural_tag(odor_test, np.zeros(50), proj_mat)\n",
    "tags_distances = np.zeros(n_test)\n",
    "inputs_distances = np.zeros(n_test)\n",
    "for i in range(n_test):  \n",
    "    odor_test2 = generate_odorant(50, rdgen_test)\n",
    "    tag_test2 = project_neural_tag(odor_test2, np.zeros(50), proj_mat)\n",
    "    # Try the cosine distance, increasing for more similar vectors\n",
    "    inputs_distances[i] = odor_test.dot(odor_test2) / np.sqrt(np.sum(odor_test**2)) /  np.sqrt(np.sum(odor_test2**2))\n",
    "    # Rather than just the norm of the difference vector\n",
    "    #inputs_distances[i] = np.sqrt(np.sum((odor_test - odor_test2)**2))\n",
    "    tags_distances[i] = jaccard(tag_test, tag_test2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(inputs_distances, tags_distances, s=9)\n",
    "#ax.set(xlabel=r\"$|s_1 - s_2|$\", ylabel=r\"Jaccard(tag1, tag2)\")\n",
    "ax.set(xlabel=r\"$\\cos(\\theta_{12})$\", ylabel=r\"Jaccard(tag1, tag2)\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to habituation and background fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def time_evolve_habituation_fixed(w_vec0, backgnd, nstep, learnrates):\n",
    "    \"\"\" Take initial conditions for w_i and C_tot, evolve them in time against a fixed odor backgnd. \n",
    "    Nothing random here as we don't fluctuate the odor. \n",
    "    \n",
    "    Args:\n",
    "        w_vec0 (np.ndarray): initial weights 1D vector, should be of same length \n",
    "            as vectors in backgnd (number of receptors)\n",
    "        backgnd (np.ndarray): input vector of the background against which we habituate\n",
    "        nstep (int): number of time steps to take. \n",
    "    \n",
    "    Returns:\n",
    "        w_vect (np.ndarray): final weights vector\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    if nstep > 1e6:\n",
    "        raise ValueError(\"Consider asking for less than 1e6 steps at a time\")\n",
    "    alpha, beta = learnrates\n",
    "    \n",
    "    # Initialize variables\n",
    "    w_vec = w_vec0.copy()\n",
    "    t = 0  # number of steps taken\n",
    "    \n",
    "    # Iterate until satisfing the number of steps asked for\n",
    "    while t < nstep:\n",
    "        # Update w, remove the max(s-w, 0) thing for the update rule, see if it still works (it should)\n",
    "        #x_vec = np.maximum(backgnd - w_vec, 0)\n",
    "        #w_vec = w_vec + alpha * x_vec - beta* w_vec\n",
    "        w_vec = w_vec + alpha * backgnd - (alpha + beta)* w_vec\n",
    "        t += 1\n",
    "\n",
    "    return w_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_odorants(od1, od2, frac1):\n",
    "    \"\"\" Compute the new input vector after linearly combining two odorant vectors, \n",
    "    as frac1*od1 + (1-frac1)*od2\n",
    "    \n",
    "    Args:\n",
    "        od1, od2 (np.ndarrays): input vectors of odorants 1 and 2\n",
    "        frac1 (float): number between 0 and 1, proportion of od1 in new mixture. \n",
    "    Returns:\n",
    "        od_mix (np.ndarrays): input vector of the mixture.  \n",
    "    \"\"\"\n",
    "    if frac1 < 0.0 or frac1 > 1.0:\n",
    "        raise ValueError(\"frac1 should be a float in [0.0, 1.0], not {}\".format(frac1))\n",
    "    return frac1*od1 + (1-frac1)*od2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_test_habituation(seed=1484389):\n",
    "    rdgen_test = np.random.default_rng(seed=seed)\n",
    "    backgnd_test = generate_odorant(50, rdgen_test)\n",
    "    w_vec_test = time_evolve_habituation_fixed(np.zeros(50), backgnd_test, 50, (0.05, 0.01))\n",
    "    \n",
    "    fig, axes =  plt.subplots(1, 2)\n",
    "    fig.set_size_inches(5.5, 2.5)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    axes[0].bar(range(len(w_vec_test)), w_vec_test, width=.5, color=\"k\")\n",
    "    axes[0].set(xlabel=\"PN index i [-]\", ylabel=r\"$w_i$ [-]\")  \n",
    "    \n",
    "    # Compare the output tags before and after habituation\n",
    "    adapt_ratio = 0.05 / (0.05 + 0.01)\n",
    "    tag_test_initial = project_neural_tag(backgnd_test, np.zeros(50), proj_mat, n_pn_per_kc=6)\n",
    "    tag_test_final = project_neural_tag(backgnd_test, w_vec_test, proj_mat, n_pn_per_kc=6)\n",
    "    # print(tag_test_final)  # Should be empty set in noiseless case\n",
    "    print(tag_test_final)\n",
    "    tag_vector_final = np.zeros(proj_mat.shape[0])\n",
    "    tag_vector_final[list(tag_test_final)] = 1.0\n",
    "    tag_vector_initial = np.zeros(proj_mat.shape[0])\n",
    "    tag_vector_initial[list(tag_test_initial)] = 1.0\n",
    "    \n",
    "    barprops = dict(aspect='auto', interpolation='nearest')\n",
    "    axes[1].imshow(tag_vector_final.reshape((10, -1)), **barprops, cmap='binary')\n",
    "    #axes[2].imshow(tag_vector_initial.reshape((1, -1)), **barprops, cmap=\"gray\")\n",
    "    axes[1].set(xlabel=\"KC index i [-]\", ylabel=r\"$z_i$ [-]\")\n",
    "    #axes[1].set_axis_off()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Jaccard initial-final:\", jaccard(tag_test_initial, tag_test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_test_habituation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary figure 3\n",
    "Similarity of odor tag B after habituation to A to B's original tag, and its correlation to the similarity between A and B. \n",
    "\n",
    "This is the real way to know whether my code reproduces correctly the model. The test is suggested by Shen, Dasgupta, Navlakha, in the section titled \"Neural Tags Remain Robust after Habituation to Another Odor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of odors\n",
    "def test_similarity_after_habituation(n_odors=100, n_orn=50, n_kc=2000, n_pn_per_kc=6, \n",
    "                    learnrates=(0.05, 0.01), steps=50, metric=\"jaccard\", adapt_kc=True):\n",
    "    rdgen_test = np.random.default_rng(seed=48225)\n",
    "    odors_test = [generate_odorant(n_orn, rdgen_test) for i in range(n_odors)]\n",
    "    proj_test = create_sparse_proj_mat(n_kc=n_kc, n_rec=n_orn, rgen=rdgen_test, fraction_filled=n_pn_per_kc/n_orn)\n",
    "    # Metric: fraction of original tag still present after habituation to another odor\n",
    "    # The diagonal should be zero or near, as after self-habituation, \n",
    "    # the odor's tag should basically be zero\n",
    "    tag_simil_matrix = np.zeros([len(odors_test), len(odors_test)])\n",
    "    tag_simil_before = np.zeros([len(odors_test), len(odors_test)])\n",
    "\n",
    "    # For all pairs of odors, habituate to one (without noise), then  \n",
    "    # compute the tag of the other alone before and after habituation\n",
    "    wzero = np.zeros(n_orn)\n",
    "    for i in range(len(odors_test)):\n",
    "        odi = odors_test[i]\n",
    "        # Habituation to odi: calculate weights w_i after that\n",
    "        w_vec_afteri = time_evolve_habituation_fixed(wzero, odi, steps, learnrates)\n",
    "        tag_i_alone = project_neural_tag(odi, wzero, proj_test, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc)\n",
    "        for j in range(len(odors_test)):\n",
    "            # Add odj, compute tag of odj alone\n",
    "            odj = odors_test[j]\n",
    "            tag_j_alone = project_neural_tag(odj, wzero, proj_test, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc)\n",
    "            tag_dist_before = jaccard(tag_i_alone, tag_j_alone)\n",
    "            tag_simil_before[i, j] = tag_dist_before\n",
    "            # After habituation\n",
    "            tag_j_habit_i = project_neural_tag(odj, w_vec_afteri, proj_test, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc)\n",
    "            # Compute similarity of the tags after habituation\n",
    "            if metric == \"jaccard\":\n",
    "                tag_dist = jaccard(tag_j_alone, tag_j_habit_i)\n",
    "            elif len(tag_j_alone) > 0:\n",
    "                tag_dist = len(tag_j_alone & tag_j_habit_i) / len(tag_j_alone)\n",
    "            else:\n",
    "                tag_dist = 0\n",
    "            tag_simil_matrix[i, j] = tag_dist\n",
    "            \n",
    "    \n",
    "    return tag_simil_matrix, tag_simil_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = perf_counter()\n",
    "tag_dist_matrix, tag_dist_before = test_similarity_after_habituation(metric=\"percent\", adapt_kc=True)\n",
    "end_t = perf_counter()\n",
    "print(\"Time per pair:\", 1000*(end_t - start_t) / tag_dist_matrix.size, \"ms\")\n",
    "# Plot statistics and average and median, removing diagonal terms\n",
    "distrib_tagsim = tag_dist_matrix[~np.eye(tag_dist_matrix.shape[0], dtype=bool)]\n",
    "distrib_tagbefore = tag_dist_before[~np.eye(tag_dist_before.shape[0], dtype=bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_stats_data = {\n",
    "    \"mean_habit\": np.mean(distrib_tagsim), \n",
    "    \"median_habit\": np.median(distrib_tagsim), \n",
    "    \"mean_ij\": np.mean(distrib_tagbefore), \n",
    "    \"median_ij\": np.median(distrib_tagbefore),\n",
    "    \"std_habit\": np.std(distrib_tagsim), \n",
    "    \"std_ij\": np.std(distrib_tagbefore)\n",
    "}\n",
    "tag_stats_data[\"pearson\"] = (np.mean(distrib_tagbefore*distrib_tagsim) - tag_stats_data[\"mean_habit\"]*tag_stats_data[\"mean_ij\"])/(tag_stats_data[\"std_habit\"]*tag_stats_data[\"std_ij\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_stats_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of J[I(s'; s), I(s')] against J[I(s), I(s')]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(distrib_tagbefore, distrib_tagsim, s=3)\n",
    "ax.set(xlabel=r\"$J[I(s), I(s')]$\", ylabel=r\"$J[I(s'; s), I(s')]$\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(7, 3)\n",
    "axes = axes.flatten()\n",
    "# Pairs of different odorants\n",
    "axes[0].hist(distrib_tagsim)\n",
    "axes[0].text(x=0.02, y=0.92, s=r\"Habituate to different odorant\", \n",
    "             transform=axes[0].transAxes)\n",
    "axes[0].text(x=0.02, y=0.835, s=\"Mean = {:.4f}\".format(np.mean(distrib_tagsim)), \n",
    "             transform=axes[0].transAxes)\n",
    "axes[0].text(x=0.02, y=0.75, s=\"Median = {:.4f}\".format(np.median(distrib_tagsim)), \n",
    "             transform=axes[0].transAxes)\n",
    "axes[0].set(xlabel=r\"$J[z_j(0), z_j(t)]$\", ylabel=\"Frequency\")\n",
    "\n",
    "# Pairs of identical odorants\n",
    "distrib_self_tagsim = tag_dist_matrix[np.eye(tag_dist_matrix.shape[0], dtype=bool)]\n",
    "axes[1].hist(distrib_self_tagsim, color=\"xkcd:gold\")\n",
    "axes[1].text(x=0.99, y=0.9, s=r\"Habituate to same odorant\", \n",
    "             transform=axes[1].transAxes, ha=\"right\")\n",
    "axes[1].text(x=0.99, y=0.8, s=\"Median = {:.4f}\".format(np.median(distrib_self_tagsim)), \n",
    "             transform=axes[1].transAxes, ha=\"right\")\n",
    "axes[1].text(x=0.99, y=0.7, s=\"Mean = {:.4f}\".format(np.mean(distrib_self_tagsim)), \n",
    "             transform=axes[1].transAxes, ha=\"right\")\n",
    "axes[1].set(xlabel=r\"$J[z_i(0), z_i(t)]$\", ylabel=\"Frequency\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag similarity of a mixture after habituation\n",
    "Figure 4B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_deviation(x):\n",
    "    return np.mean(np.abs(x - np.mean(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of odors\n",
    "def mix_similarity_after_habituation(all_odors=None, n_odors=100, n_orn=50, n_kc=2000, n_pn_per_kc=6, \n",
    "        learnrates=(0.05, 0.01), steps=50, metric=\"jaccard\", adapt_kc=True, fix_thresh=None, seed=48225):\n",
    "    # Initialize projection matrix, odors, containers for scores\n",
    "    rdgen_mix = np.random.default_rng(seed=seed)\n",
    "    if all_odors is None:\n",
    "        all_odors = [generate_odorant(n_orn, rdgen_test) for i in range(n_odors)]\n",
    "    else:\n",
    "        n_odors = all_odors.shape[1]\n",
    "        n_orn = all_odors.shape[0]\n",
    "        all_odors = all_odors.values.T  # Each row is an odorant now\n",
    "    proj_mat = create_sparse_proj_mat(n_kc=n_kc, n_rec=n_orn, rgen=rdgen_mix, fraction_filled=n_pn_per_kc/n_orn)\n",
    "    simil_i_before = np.zeros([len(all_odors), len(all_odors)])  # J(s, s'') before\n",
    "    simil_j_before = np.zeros([len(all_odors), len(all_odors)])  # J(s', s'') before\n",
    "    simil_i_after = np.zeros([len(all_odors), len(all_odors)])   # J(s, s'') after\n",
    "    simil_j_after = np.zeros([len(all_odors), len(all_odors)])   # J(s', s'') after\n",
    "    \n",
    "    # For all pairs of odors, habituate to one (without noise), then  \n",
    "    # compute the tag of the other alone before and after habituation\n",
    "    sparse_tags = 0\n",
    "    wzero = np.zeros(n_orn)\n",
    "    projtag_kwargs = dict(adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc, fix_thresh=fix_thresh)\n",
    "    for i in range(len(all_odors)):\n",
    "        odi = all_odors[i]\n",
    "        tag_i_before = project_neural_tag(odi, wzero, proj_mat, **projtag_kwargs)\n",
    "        # Habituation to odi: calculate weights w_i after that\n",
    "        w_vec_afteri = time_evolve_habituation_fixed(wzero, odi, steps, learnrates)\n",
    "        #tag_i_after = project_neural_tag(odi, w_vec_afteri, proj_mat, **projtag_kwargs)\n",
    "        for j in range(len(all_odors)):\n",
    "            if j == i: continue  # Skip odor versus itself\n",
    "            # Add odj, compute tag of odj beforeand of the mix\n",
    "            odj = all_odors[j]\n",
    "            odmix = combine_odorants(odi, odj, 0.8)\n",
    "            \n",
    "            # Compute tags of j and mixture before and after\n",
    "            tag_j_before = project_neural_tag(odj, wzero, proj_mat, **projtag_kwargs)\n",
    "            tag_mix_before = project_neural_tag(odmix, wzero, proj_mat, **projtag_kwargs)\n",
    "            #tag_j_after = project_neural_tag(odj, w_vec_afteri, proj_mat, **projtag_kwargs)\n",
    "            tag_mix_after = project_neural_tag(odmix, w_vec_afteri, proj_mat, **projtag_kwargs)\n",
    "            if len(tag_mix_after) < 100:\n",
    "                sparse_tags += 1\n",
    "            \n",
    "            # Compute the different tag distances, always using Jaccard\n",
    "            simil_i_before[i, j] = jaccard(tag_i_before, tag_mix_before)\n",
    "            simil_j_before[i, j] = jaccard(tag_j_before, tag_mix_before)\n",
    "            simil_i_after[i, j] = jaccard(tag_i_before, tag_mix_after)\n",
    "            simil_j_after[i, j] = jaccard(tag_j_before, tag_mix_after)\n",
    "            \n",
    "    print(\"Encountered {} sparse mix tags after habituation; threshold too high?\".format(sparse_tags))\n",
    "    \n",
    "    return simil_i_before, simil_j_before, simil_i_after, simil_j_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = perf_counter()\n",
    "n_odors = 100\n",
    "sim_mats = mix_similarity_after_habituation(n_odors=n_odors, n_orn=50, n_kc=2000, n_pn_per_kc=6, \n",
    "                    learnrates=(0.05, 0.01), steps=50, metric=\"jaccard\", adapt_kc=True)\n",
    "end_t = perf_counter()\n",
    "print(\"Time per pair:\", 1000*(end_t - start_t)/sim_mats[0].size, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_A_before = sim_mats[0][~np.eye(n_odors).astype(bool)].flatten()\n",
    "samples_A_after = sim_mats[2][~np.eye(n_odors).astype(bool)].flatten()\n",
    "samples_B_before = sim_mats[1][~np.eye(n_odors).astype(bool)].flatten()\n",
    "samples_B_after = sim_mats[3][~np.eye(n_odors).astype(bool)].flatten()\n",
    "\n",
    "# Prepare two barplots: one for A vs mix before and after, one for B vs mix before and after\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "barcolors = [\"grey\", \"blue\"]\n",
    "yerr = [mean_abs_deviation(samples_A_before), mean_abs_deviation(samples_A_after)]\n",
    "axes[0].bar(0, np.median(samples_A_before), yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "axes[0].bar(1, np.mean(samples_A_after), yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "axes[0].scatter(0.075*np.random.normal(size=samples_A_before.size), samples_A_before, s=2,\n",
    "                color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "axes[0].scatter(1+0.075*np.random.normal(size=samples_A_after.size), samples_A_after, s=2,\n",
    "                color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "axes[0].set_title(r\"$s_A(0)$ vs $s_{mix}(t)$\")\n",
    "axes[0].set_ylabel(\"Jaccard similarity\")\n",
    "\n",
    "yerr = [mean_abs_deviation(samples_B_before), mean_abs_deviation(samples_B_after)]\n",
    "axes[1].bar(0, np.median(samples_B_before), yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "axes[1].bar(1, np.median(samples_B_after), yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "axes[1].scatter(0.075*np.random.normal(size=samples_B_before.size), samples_B_before, s=2,\n",
    "                color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "axes[1].scatter(1+0.075*np.random.normal(size=samples_B_after.size), samples_B_after, s=2,\n",
    "                color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "axes[1].set_title(r\"$s_B(0)$ vs $s_{mix}(t)$\")\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].annotate(\"Before\", xy=(0, axes[i].get_ylim()[1]*0.98), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    axes[i].annotate(\"After\", xy=(1, axes[i].get_ylim()[1]*0.98), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([r\"$t=0$\", r\"$t=50$\"])\n",
    "    axes[i].set_xlabel(\"Habituation to A\")\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same as above, but with data\n",
    "The data is what is actually used in the paper, and seems to give better results (i.e. better Jaccard(B, mix) after habituation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mean = 10\n",
    "indata = pd.read_csv('data/hallem2006_TableS1_source.csv', index_col=0, header=0, encoding='utf-8')\n",
    "normed = indata.T - indata.T.min()\n",
    "normed = set_mean * normed/normed.mean()\n",
    "normed.drop(['spontaneous firing rate'], axis=1, inplace=True)\n",
    "normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = perf_counter()\n",
    "n_odors_data = normed.shape[1]\n",
    "sim_mats_data = mix_similarity_after_habituation(all_odors=normed, n_odors=n_odors_data, \n",
    "                    n_orn=normed.shape[0], n_kc=1000, n_pn_per_kc=3, learnrates=(0.05, 0.01), \n",
    "                    steps=50, metric=\"jaccard\", adapt_kc=False, seed=14345)\n",
    "end_t = perf_counter()\n",
    "print(\"Time per pair:\", 1000*(end_t - start_t)/sim_mats[0].size, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_A_before_data = sim_mats_data[0][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_A_after_data = sim_mats_data[2][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_B_before_data = sim_mats_data[1][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_B_after_data = sim_mats_data[3][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "samples_A_before_data = sim_mats_data[0][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_A_after_data = sim_mats_data[2][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_B_before_data = sim_mats_data[1][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_B_after_data = sim_mats_data[3][np.triu_indices(n_odors_data)].flatten()\n",
    "\n",
    "# Prepare two barplots: one for A vs mix before and after, one for B vs mix before and after\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "barcolors = [\"grey\", \"blue\"]\n",
    "yerr = [mean_abs_deviation(samples_A_before_data), mean_abs_deviation(samples_A_after_data)]\n",
    "axes[0].bar(0, np.median(samples_A_before_data), yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "axes[0].bar(1, np.median(samples_A_after_data), yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "axes[0].scatter(0.075*np.random.normal(size=samples_A_before_data.size), samples_A_before_data, s=2,\n",
    "                color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "axes[0].scatter(1+0.075*np.random.normal(size=samples_A_after_data.size), samples_A_after_data, s=2,\n",
    "                color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "axes[0].set_title(r\"$s_A(0)$ vs $s_{mix}(t)$\")\n",
    "axes[0].set_ylabel(\"Median Jaccard similarity\")\n",
    "\n",
    "yerr = [mean_abs_deviation(samples_B_before_data), mean_abs_deviation(samples_B_after_data)]\n",
    "axes[1].bar(0, np.median(samples_B_before_data), yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "axes[1].bar(1, np.median(samples_B_after_data), yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "axes[1].scatter(0.075*np.random.normal(size=samples_B_before_data.size), samples_B_before_data, s=2,\n",
    "                color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "axes[1].scatter(1+0.075*np.random.normal(size=samples_B_after_data.size), samples_B_after_data, s=2,\n",
    "                color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "axes[1].set_title(r\"$s_B(0)$ vs $s_{mix}(t)$\")\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        ylims = axes[i].get_ylim()\n",
    "        axes[i].set_ylim(ylims[0], ylims[1]+0.04)\n",
    "    axes[i].annotate(\"Before\", xy=(0, ylims[1]+0.025), ha=\"center\", va=\"top\", fontsize=10)\n",
    "    axes[i].annotate(\"After\", xy=(1, ylims[1]+0.025), ha=\"center\", va=\"top\", fontsize=10)\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([r\"$t=0$\", r\"$t=50$\"])\n",
    "    axes[i].set_xlabel(\"Habituation to A\")\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after(samp_before, samp_after, figax=None):\n",
    "    if figax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(3, 3)\n",
    "    else:\n",
    "        fig, ax = figax\n",
    "\n",
    "    barcolors = sns.color_palette(\"mako\", n_colors=4)[1:3]\n",
    "    yerr = [mean_abs_deviation(samp_before), mean_abs_deviation(samp_after)]\n",
    "    median_before = np.median(samp_before)\n",
    "    median_after = np.median(samp_after)\n",
    "    print(\"Median before:\", median_before)\n",
    "    print(\"Median_after:\", median_after)\n",
    "    ax.bar(0, median_before, yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "    ax.bar(1, median_after, yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "    ax.scatter(0.075*np.random.normal(size=samp_before.size), samp_before, s=2,\n",
    "                    color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "    ax.scatter(1+0.075*np.random.normal(size=samp_after.size), samp_after, s=2,\n",
    "                    color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "    ax.set_title(r\"$s_A(0)$ vs $s_{mix}(t)$\")\n",
    "    ax.set_ylabel(\"Jaccard similarity\")\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "    ax.set_ylim(ylims[0], ylims[1]+0.1)\n",
    "    ax.annotate(\"Before\", xy=(0, ylims[1]+0.05), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    ax.annotate(\"After\", xy=(1, ylims[1]+0.05), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([r\"$t=0$\", r\"$t=50$\"])\n",
    "    ax.set_xlabel(\"Habituation\")\n",
    "\n",
    "    return [fig, ax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_A_before_data = sim_mats_data[0][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_A_after_data = sim_mats_data[2][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_B_before_data = sim_mats_data[1][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "#samples_B_after_data = sim_mats_data[3][~np.eye(n_odors_data).astype(bool)].flatten()\n",
    "samples_A_before_data = sim_mats_data[0][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_A_after_data = sim_mats_data[2][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_B_before_data = sim_mats_data[1][np.triu_indices(n_odors_data)].flatten()\n",
    "samples_B_after_data = sim_mats_data[3][np.triu_indices(n_odors_data)].flatten()\n",
    "\n",
    "\n",
    "fig, ax = plot_before_after(samples_B_before_data, samples_B_after_data)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/shen2020_related/habituation_constant_background_data.pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rare events depend a lot on the random seed, i.e. the projectino matrix. The mean average deviation thus also depends on it a lot. The median itself, not so much (it oscillates between 0.9607 and 0.9615, depending on the seed. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(samples_B_after_data) - yerr[1])\n",
    "print(np.median(samples_B_after_data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity to $\\tau_0$\n",
    "Check how the tag sparsifies upon habituation as $\\tau_0$ is varied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdgen_tau = np.random.default_rng(8438341)\n",
    "proj_mat_tau = create_sparse_proj_mat(2000, 50, rdgen_tau)\n",
    "backgnd_tau = generate_odorant(50, rdgen_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vec_tau = time_evolve_habituation_fixed(np.zeros(50), backgnd_tau, 50, (0.05, 0.01))\n",
    "\n",
    "# Compute the output tag with different tau_0 values\n",
    "thresholds = [22, 20, 15, 10]\n",
    "tag_initial = project_neural_tag(backgnd_tau, np.zeros(50), proj_mat_tau, fix_thresh=20)\n",
    "tags = [tag_initial]\n",
    "for t in thresholds:\n",
    "    tags.append(project_neural_tag(backgnd_tau, w_vec_tau, proj_mat_tau, fix_thresh=t))\n",
    "\n",
    "fig, axes = plt.subplots(len(thresholds)+1)\n",
    "fig.set_size_inches(8, 0.75*(len(thresholds)+1))\n",
    "print(tag_initial)\n",
    "for i, tag in enumerate(tags):\n",
    "    axes[i].eventplot(list(tag), color=\"k\")\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "axes[0].set_title(\"Initial KC tag\", y=0.7)\n",
    "for i, t in enumerate(thresholds):\n",
    "    axes[i+1].set_title(r\"KC tag for $\\tau_0 = {}$\".format(t), y=0.75)\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "#fig.savefig(\"figures/shen2020_related/tag_sparsity_depends_on_tau0.pdf\", transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance for discrimination post-habituation for different taus\n",
    "# Compute the output tag with different tau_0 values\n",
    "thresholds = [22, 20, 15, 10]\n",
    "all_sim_mats = []\n",
    "n_odors = 50\n",
    "for t in thresholds:\n",
    "    print(\"Threshold = \", t)\n",
    "    sim_mats = mix_similarity_after_habituation(all_odors=None, n_odors=n_odors, \n",
    "                    n_orn=50, n_kc=2000, n_pn_per_kc=6, learnrates=(0.05, 0.01), \n",
    "                    steps=50, metric=\"jaccard\", adapt_kc=False, fix_thresh=t, seed=14345)\n",
    "    all_sim_mats.append(sim_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5, 3)\n",
    "facecolors = sns.color_palette(\"mako\", len(thresholds))\n",
    "sparse_mix_tags = [1617, 1175, 49, 0]\n",
    "for i, t in enumerate(thresholds):\n",
    "    samples_after = all_sim_mats[i][3][~np.eye(n_odors).astype(bool)].flatten()\n",
    "    yerr = mean_abs_deviation(samples_after)\n",
    "    ax.bar(i, np.median(samples_after), yerr=yerr, facecolor=facecolors[i], edgecolor=\"k\", alpha=0.5)\n",
    "    ax.scatter(i+0.075*np.random.normal(size=samples_after.size), samples_after, s=2,\n",
    "                    color=facecolors[i], alpha=0.8, lw=0.5)\n",
    "    ax.annotate(\"{} mix tags\\nwere sparse\".format(sparse_mix_tags[i]), xy=(i, 1.2), fontsize=8, ha=\"center\", va=\"top\")\n",
    "    print(np.median(all_sim_mats[i][2][~np.eye(n_odors).astype(bool)].flatten()))\n",
    "ax.set_xlabel(r\"$\\tau_0$\")\n",
    "ax.set_xticks(range(len(thresholds)))\n",
    "ax.set_xticklabels(thresholds)\n",
    "ax.set_ylabel(r\"Jaccard($s_A(0), s_{mix}(t)$)\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/shen2020_related/habituation_constant_background_effect_tau0.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
