{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence of IBCM and BioPCA habituation to turbulent backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os, json\n",
    "pj = os.path.join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = True\n",
    "# Resources\n",
    "root_dir = pj(\"..\", \"..\", \"..\")\n",
    "data_folder = pj(root_dir, \"results\", \"for_plots\")\n",
    "data_folder_conv = pj(root_dir, \"results\", \"for_plots\", \"convergence\")\n",
    "panels_folder = \"panels/\"\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full = np.asarray(json.load(f))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra aesthetic parameters for this figure\n",
    "# Figures slightly less high, to squeeze four rows of plots\n",
    "plt.rcParams[\"figure.figsize\"] = (plt.rcParams[\"figure.figsize\"][0], 1.6)\n",
    "\n",
    "# More legend rcParams: make everything smaller by 30 %\n",
    "plt.rcParams[\"patch.linewidth\"] = 0.75\n",
    "legend_rc = {\"labelspacing\":0.5, \"handlelength\":2.0, \"handleheight\":0.7, \n",
    "             \"handletextpad\":0.8, \"borderaxespad\":0.5, \"columnspacing\":2.0}\n",
    "for k in legend_rc:\n",
    "    plt.rcParams[\"legend.\"+k] = 0.75 * legend_rc[k]\n",
    "\n",
    "new_color = \"r\"\n",
    "linestyles = [\"-\", \"--\", \":\", (0, (5, 1, 2, 1)), \"-.\"]\n",
    "neuron_styles = linestyles + [(0, (1, 2, 1, 2))]\n",
    "\n",
    "markerstyles = [\"o\", \"s\", \"^\", \"v\", \"X\", \"*\", \"d\", \"h\", \"<\", \"p\", \"P\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ibcm_hgammas_series(t_axis, i_highlights, hgammaser, squeeze=0.65):\n",
    "    # Show three neurons\n",
    "    fig, ax = plt.subplots()\n",
    "    # By default, we squeeze to be able to put matrix of cgammas series besides\n",
    "    fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*squeeze, \n",
    "                        plt.rcParams[\"figure.figsize\"][1])\n",
    "\n",
    "    #ax.axhline(0.0, ls=\"-\", color=(0.8,)*3, lw=0.8)\n",
    "    legend_styles = [[0,]*6, [0,]*6, [0,]*6]\n",
    "    neuron_colors3 = neuron_colors_full[[8, 17, 23]]\n",
    "    clr_back = back_palette[-1]\n",
    "    plot_skp = 20\n",
    "    n_b = hgammaser.shape[2]\n",
    "\n",
    "    # plot all other neurons first, skip some points\n",
    "    for i in range(n_i_ibcm):\n",
    "        if i in i_highlights: \n",
    "            continue\n",
    "        elif i % 2 == 0:   # thinning\n",
    "            continue\n",
    "        else: \n",
    "            for j in range(n_b):\n",
    "                ax.plot(t_axis[::plot_skp], hgammaser[::plot_skp, i, j], color=clr_back, \n",
    "                    ls=\"-\", alpha=1.0-0.1*j, lw=plt.rcParams[\"lines.linewidth\"]-j*0.1)\n",
    "\n",
    "    # Now plot the highlighted neuron\n",
    "    for j in range(n_b):\n",
    "        for i in range(len(i_highlights)):\n",
    "            li, = ax.plot(t_axis[::plot_skp], hgammaser[::plot_skp, i_highlights[i], j], \n",
    "                          color=neuron_colors3[i], ls=\"-\", alpha=1.0-0.1*j, \n",
    "                          lw=plt.rcParams[\"lines.linewidth\"]-j*0.1)\n",
    "            legend_styles[i][j] = li\n",
    "    \n",
    "    # Annotations\n",
    "    ax.set(xlabel=\"Time (min)\", \n",
    "           ylabel=r\"Alignments $\\bar{h}_{i\\gamma} = \\mathbf{\\bar{m}}_i \\cdot \\mathbf{s}_{\\gamma}$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ibcm_hgammas_matrix(hgammas_mat, i_high, squeeze=0.4):\n",
    "    n_i, n_comp = hgammas_mat.shape\n",
    "    neuron_colors3 = neuron_colors_full[[8, 17, 23]]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*squeeze, \n",
    "                        plt.rcParams[\"figure.figsize\"][1])\n",
    "    # Extent: left, right, bottom, top\n",
    "    # Greyscale version\n",
    "    #ax.imshow(hgammas_matrix, cmap=\"Greys\", aspect=0.6, extent=(0.5, n_comp+0.5, 0.5, n_i))\n",
    "    #ax.set_xticks(list(range(1, n_comp+1)))\n",
    "    # Colorful version: add patches manually with fill_between. \n",
    "    # Color highlighted neurons, leave others grayscale!\n",
    "    normed_matrix = (hgammas_mat - hgammas_mat.min()) / (hgammas_mat.max() - hgammas_mat.min())\n",
    "    for i in range(n_i):\n",
    "        # Full rainbow version\n",
    "        #cmap = sns.light_palette(neuron_colors_full[i], as_cmap=True)\n",
    "        # Version where only highlights are colored\n",
    "        if i in i_high:\n",
    "            cmap = sns.light_palette(neuron_colors3[i_high.index(i)], as_cmap=True)\n",
    "        else:\n",
    "            cmap = sns.color_palette(\"Greys\", as_cmap=True)\n",
    "        for j in range(n_comp):\n",
    "            ax.fill_between([-0.5+j, 0.5+j], -0.5+i, 0.5+i, color=cmap(normed_matrix[i, j]))\n",
    "\n",
    "    ax.set_xlim([-0.6, -0.6+n_comp])\n",
    "    ax.set_ylim([-0.6, -0.6+n_i])\n",
    "    ax.set_yticks(list(range(0, 19, 2)) + [21, 23])\n",
    "\n",
    "    for i, lbl in enumerate(ax.get_yticklabels()):\n",
    "        if int(lbl.get_text()) in i_high:\n",
    "            clr = neuron_colors3[i_high.index(int(lbl.get_text()))]\n",
    "            lbl.set_color(clr)\n",
    "            ax.yaxis.get_ticklines()[i].set_color(clr)\n",
    "\n",
    "    ax.set_xlabel(r\"Component $\\gamma$\", size=6)\n",
    "    ax.set_ylabel(r\"IBCM neuron index $i$\", size=6)\n",
    "    cbar = fig.colorbar(mpl.cm.ScalarMappable(\n",
    "        norm=mpl.colors.Normalize(hgammas_mat.min(), hgammas_mat.max()), \n",
    "        cmap=\"Greys\"), ax=ax, aspect=30, pad=0.1)\n",
    "    cbar.set_ticks([])\n",
    "    cbar.set_label(label=r\"Alignments ${\\bar{h}}_{i\\gamma}$, 45 min\", fontsize=6)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax, cbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IBCM simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_specifs(hgamser, transient_frac=5/6):\n",
    "    \"\"\" To compute the odor with which each IBCM aligned most. \"\"\"\n",
    "    duration_loc = hgamser.shape[0]\n",
    "    transient_loc = int(transient_frac * duration_loc)\n",
    "    hgammean = np.mean(hgamser[transient_loc:], axis=0)\n",
    "    # Sorted odor indices, from min to max, of odor alignments for each neuron\n",
    "    aligns_idx_sorted = np.argsort(hgammean, axis=1) \n",
    "    specifs = np.argmax(hgammean, axis=1)\n",
    "    return specifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved simulation\n",
    "with np.load(pj(data_folder_conv, \"ibcm_convergence_hgammas_examples.npz\")) as fp:\n",
    "    hgammas_def = fp[\"hgammas_ser_def\"]\n",
    "    specifs_def = recompute_specifs(hgammas_def)\n",
    "    \n",
    "    hgammas_low = fp[\"hgammas_ser_low\"]\n",
    "    specifs_low = recompute_specifs(hgammas_low)\n",
    "    \n",
    "    hgammas_hi = fp[\"hgammas_ser_hi\"]\n",
    "    specifs_hi = recompute_specifs(hgammas_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel A: good IBCM example, annotation of convergence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_scale = 1.0 / 100.0 / 60.0  # 10 ms steps to min\n",
    "n_i_ibcm = hgammas_def.shape[1]\n",
    "nsteps = hgammas_def.shape[0]\n",
    "n_components = hgammas_def.shape[2]\n",
    "tser_example = np.arange(0.0, 360000.0, 360000 / nsteps) * tu_scale  # in min\n",
    "\n",
    "transient = int(3*tser_example.size/4)\n",
    "i_highlights = [2, 8, 21]  # Neurons to highlight\n",
    "\n",
    "hgammas_matrix = np.mean(hgammas_def[transient:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_ibcm_hgammas_series(tser_example, i_highlights, hgammas_def, squeeze=1.0)\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.15, plt.rcParams[\"figure.figsize\"][1])\n",
    "n_odors = hgammas_def.shape[1]\n",
    "\n",
    "# Extra annotations to explain the convergence metrics\n",
    "arrowprops = {\"arrowstyle\": \"<->\", \"lw\":0.75, \"color\":\"k\"}\n",
    "\n",
    "# Alignment gaps\n",
    "ytop = np.mean(hgammas_def[-10000:-1, i_highlights[2], specifs_def[i_highlights[2]]])\n",
    "ybot = np.mean(hgammas_def[-10000:-1, i_highlights[2], (specifs_def[i_highlights[2]] + 1) % n_odors])\n",
    "ax.annotate(\"\", xy=(63, ybot), xytext=(63, ytop), arrowprops=arrowprops, annotation_clip=False)\n",
    "ax.annotate(\"Alignment\\nstrength \" + r\"$\\Delta h$\", xy=(70, 0.5*(ybot+ytop)), rotation=90, \n",
    "            ha=\"center\", va=\"center\", weight=\"semibold\", annotation_clip=False)\n",
    "\n",
    "# Variance of h_gammas\n",
    "hgampad = 3.0\n",
    "ax.axhline(ytop+hgampad, xmin=0.55, xmax=0.95, ls=\"--\", lw=0.75, color=\"k\")\n",
    "ax.axhline(ytop-hgampad, xmin=0.55, xmax=0.95, ls=\"--\", lw=0.75, color=\"k\")\n",
    "arrowprops.update({\"lw\": 0.5, \"arrowstyle\":\"->\", \"shrinkB\":0.01})\n",
    "xannot = 59.0\n",
    "ax.annotate(\"\", xy=(xannot, ytop+hgampad), xytext=(xannot, ytop+hgampad+2), arrowprops=arrowprops)\n",
    "ax.annotate(\"\", xy=(xannot, ytop-hgampad), xytext=(xannot, ytop-hgampad-2), arrowprops=arrowprops)\n",
    "ax.annotate(r\"$\\bar{h}_{\\gamma, \\mathrm{sp}}$ noise\", \n",
    "           xy=(xannot-1.0, ytop+hgampad+0.1), ha=\"right\", va=\"bottom\", weight=\"semibold\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_hgamma_metrics.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, axi = plot_ibcm_hgammas_matrix(hgammas_matrix, i_highlights)\n",
    "# Extra annotations\n",
    "ax.set_xticks([])\n",
    "arrowprops = {\"arrowstyle\": \"<->\", \"lw\":0.75, \"color\":\"k\"}\n",
    "ax.annotate(\"\", xy=(-1, -1.5), xytext=(3, -1.5), arrowprops=arrowprops, annotation_clip=False)\n",
    "ax.set_xlabel(\"Odors covered\", color=\"k\", labelpad=6.0, weight=\"semibold\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_hgamma_metric_matrix.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel D: illustration for low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_highlights = [0, 4, 16]  # Neurons to highlight. Active ones: 4, 12, 14\n",
    "fig, ax = plot_ibcm_hgammas_series(tser_example, i_highlights, hgammas_low, squeeze=0.65)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_low_mu_example.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgammas_matrix = np.mean(hgammas_low[transient:], axis=0)\n",
    "fig, ax, axi = plot_ibcm_hgammas_matrix(hgammas_matrix, i_highlights)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_low_mu_example_matrix.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel E: illustration for high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_highlights = [18, 20]\n",
    "tsl = slice(5*len(tser_example)//6, None, 1)\n",
    "fig, ax = plot_ibcm_hgammas_series(tser_example[tsl], i_highlights, hgammas_hi[tsl], squeeze=0.65)\n",
    "#ax.annotate(\"Zoom-in\", xy=(tser_example[tsl][0], ax.get_ylim()[1]*0.98), va=\"top\", ha=\"left\", fontsize=6)\n",
    "t_lower = tser_example[tsl][0]\n",
    "t_mid = 0.5 * (tser_example[tsl][0] + tser_example[tsl][-1])\n",
    "y_upper = ax.get_ylim()[1]*0.98\n",
    "ax.annotate(\"Stochastic\\noscillations\", xy=(t_lower-0.2, y_upper), va=\"top\", ha=\"left\", fontsize=5)\n",
    "ax.annotate(\"\", xytext=(t_mid-0.2, y_upper*0.92), \n",
    "            xy=(t_mid+1.5, y_upper*0.75), va=\"top\", ha=\"left\", fontsize=5, \n",
    "            arrowprops={\"arrowstyle\":\"->\"})\n",
    "ax.annotate(\"Some\\nalignment\", xy=(59.0, y_upper), va=\"top\", ha=\"center\", fontsize=5)\n",
    "ax.annotate(\"\", xytext=(59.0, y_upper*0.82), \n",
    "            xy=(59.0, y_upper*0.3), va=\"top\", ha=\"left\", fontsize=5, \n",
    "            arrowprops={\"arrowstyle\":\"->\"})\n",
    "ax.set_xlabel(\"Time, zoom-in (min)\")\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_high_mu_example.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgammas_matrix = np.mean(hgammas_hi[transient:], axis=0)\n",
    "fig, ax, axi = plot_ibcm_hgammas_matrix(hgammas_matrix, i_highlights)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_high_mu_example_matrix.pdf\"), \n",
    "                transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel B: alignment gap vs rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of learning rate into inverse seconds units\n",
    "dtscale_s = 10.0 / 1000.0  # to convert time step units to seconds\n",
    "factor_mu_s = 1.0 / dtscale_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_odor_coverage(specifs):\n",
    "    nmu, ntau = specifs.shape[:2]\n",
    "    cov = np.zeros(specifs.shape[:3])\n",
    "    for i in range(nmu):\n",
    "        for j in range(ntau):\n",
    "            for k in range(specifs.shape[2]):\n",
    "                cov[i, j, k] = np.unique(specifs[i, j, k]).size\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "with np.load(pj(data_folder_conv, \"convergence_vs_ibcm_rates_results_3odors.npz\")) as conv_results:\n",
    "    mutau_grid = conv_results[\"mutau_grid\"]\n",
    "    mutau_grid[0] *= factor_mu_s  # convert mu into s^-1\n",
    "    mutau_grid[1] *= dtscale_s  # convert tau into s\n",
    "    align_gaps = conv_results[\"align_gaps\"]\n",
    "    gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "    hgamma_varis = conv_results[\"hgamma_varis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "       ylabel=r\"Alignment strength $\\Delta h$\" + \"\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$ (s)\", \n",
    "          loc=\"upper left\", frameon=False, fontsize=5, bbox_to_anchor=(0.0, 1.05))\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_3odor_alignment_gap_lineplot.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel C: aligment noise\n",
    "We define the alignment noise as the ratio between the standard deviation over time of the specific alignment, $\\sigma[h_\\mathrm{specif}]$, and the average alignment strength $\\langle \\Delta h \\rangle$, each averaged across simulations. This gives a sense of the amplitude of $h$ fluctuations compared to the gap between $h_\\mathrm{sp}$ and $h_\\mathrm{ns}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vari_specifs(allhvaris, specifs):\n",
    "    vari_specifs = np.zeros(allhvaris.shape[:4])\n",
    "    n_mu, n_tau, n_seed, n_i = specifs.shape[:4]\n",
    "    for i in range(n_mu):\n",
    "        for j in range(n_tau):\n",
    "            for k in range(n_seed):\n",
    "                vari_specifs[i, j, k] = allhvaris[i, j, k][np.arange(n_i), specifs[i, j, k]]\n",
    "    return vari_specifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "vari_specifs = get_vari_specifs(hgamma_varis, gamma_specifs)\n",
    "\n",
    "# Square root from variance to standard deviation\n",
    "# and normalize by the average alignment gap\n",
    "stdev_specifs = np.sqrt(vari_specifs) / np.mean(align_gaps, axis=(2,3), keepdims=True)\n",
    "\n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "       #ylabel=r\"Standard dev. of $h_{\\gamma,sp}$\" + \"\\n\" + r\"scaled by average $\\Delta h$\", \n",
    "       ylabel=\"Alignment noise,\\n\" + r\"$\\sigma[h_\\mathrm{sp}] / \\langle \\Delta h \\rangle$\",\n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$ (s)\", ncol=2, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_3odor_alignment_noise.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel F: alignment strength vs odor number\n",
    "\n",
    " Alignment gap, average fraction odors covered as a function of number of odors. \n",
    " \n",
    " Pick one $\\tau$, show lines for different $\\mu$. \n",
    " \n",
    " Legend will be shared with the next panel too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_align_gaps = []\n",
    "concatenated_odor_coverages = []\n",
    "concatenated_hgams_varis = []\n",
    "nodors_range = np.asarray([3, 4, 5, 6, 8])\n",
    "f_name_prefix = \"convergence_vs_ibcm_rates_results_\"\n",
    "for n in nodors_range:\n",
    "    try:\n",
    "        fp = np.load(pj(data_folder_conv, f_name_prefix + f\"{n}odors.npz\"))\n",
    "    except FileNotFoundError: \n",
    "        continue\n",
    "    else:\n",
    "        concatenated_align_gaps.append(fp[\"align_gaps\"])\n",
    "        specifs_n = fp[\"gamma_specifs\"]\n",
    "        concatenated_odor_coverages.append(compute_odor_coverage(specifs_n))\n",
    "        vari_specifs = get_vari_specifs(fp[\"hgamma_varis\"], fp[\"gamma_specifs\"])\n",
    "        concatenated_hgams_varis.append(vari_specifs)\n",
    "        mutau_grid = fp[\"mutau_grid\"]\n",
    "        mutau_grid[0] *= factor_mu_s  # convert mu into s^-1\n",
    "        mutau_grid[1] *= dtscale_s  # convert tau into s\n",
    "concatenated_align_gaps = np.stack(concatenated_align_gaps)\n",
    "concatenated_odor_coverages = np.stack(concatenated_odor_coverages)\n",
    "concatenated_hgams_varis = np.stack(concatenated_hgams_varis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_tau_j = 5  # 1600\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*0.91, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "n_mu = mu_range.shape[0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    gap_mean_line_mu = np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_mu = np.std(np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, gap_mean_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, gap_mean_line_mu-gap_std_line_mu, \n",
    "                   gap_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Alignment strength\\n(larger is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "#ax.legend(handles=handles, labels=labels, \n",
    "#          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "#          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "# Annotate with tau\n",
    "ax.set_title(r\"$\\tau_{\\Theta} = \" + \"{:d}$ s\".format(int(mutau_grid[1, 0, chosen_tau_j])), y=0.9)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_alignment_vs_nodors_nolegend.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel G: odor coverage vs odor number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# No legend, shared from previous plot\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*0.91, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    coverage_i = concatenated_odor_coverages[:, i, chosen_tau_j]\n",
    "    coverage_line_mu = np.mean(coverage_i, axis=1) / nodors_range\n",
    "    coverage_std_mu = np.std(coverage_i, axis=1) / nodors_range  # std across seed\n",
    "    ax.plot(nodors_range, coverage_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, coverage_line_mu-coverage_std_mu, \n",
    "                   coverage_line_mu+coverage_std_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Average fraction\\nodors covered\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "#ax.legend(handles=handles, labels=labels, \n",
    "#          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "#          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "#ax.legend(handles=handles, labels=labels, ncol=3,\n",
    "#          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "#          loc=\"lower center\", frameon=False)\n",
    "#ax.set_ylim([0.4, 1.15])\n",
    "# Annotate with tau\n",
    "ax.set_title(r\"$\\tau_{\\Theta} = \" + \"{:d}$ s\".format(int(mutau_grid[1, 0, chosen_tau_j])), y=0.9)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_odor_coverage_vs_nodors_nolegend.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel H: alignment noise as a function of odor number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the standard deviation across seeds and neurons, divide by the alignment gap average\n",
    "# Axes are indexed [n_odor, n_mu, n_tau, n_seeds, n_neurons, ...]\n",
    "align_noises_vs_nodor = (np.sqrt(concatenated_hgams_varis)\n",
    "                         / np.mean(concatenated_align_gaps, axis=3, keepdims=True))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.12, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    std_mean_line_mu = np.mean(align_noises_vs_nodor[:, i, chosen_tau_j], axis=(1, 2))\n",
    "    std_std_line_mu = np.std(np.mean(align_noises_vs_nodor[:, i, chosen_tau_j], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, std_mean_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, std_mean_line_mu-std_std_line_mu, \n",
    "                   std_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Alignment noise\", #,\" + r\"$\\sigma[h_\\mathrm{sp}] / \\langle \\Delta h \\rangle$\",\n",
    "      yscale=\"log\")\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_alignment_noise_vs_nodors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel F alternate: two combined vertical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_tau_j = 5  # 1600\n",
    "fig, axes = plt.subplots(2, 1, sharex=\"col\")\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.05, plt.rcParams[\"figure.figsize\"][1]*2.1)\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "\n",
    "\n",
    "# First plot: alignment gap\n",
    "ax = axes[0]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    gap_mean_line_mu = np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_mu = np.std(np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, gap_mean_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, gap_mean_line_mu-gap_std_line_mu, \n",
    "                   gap_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(#xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Alignment strength\\n(larger is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "# Annotate with tau\n",
    "ax.set_title(r\"$\\tau_{\\Theta} = \" + \"{:d}$ s\".format(int(mutau_grid[1, 0, chosen_tau_j])), y=0.9)\n",
    "\n",
    "\n",
    "# Second plot: odor coverage\n",
    "ax = axes[1]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    coverage_i = concatenated_odor_coverages[:, i, chosen_tau_j]\n",
    "    coverage_line_mu = np.mean(coverage_i, axis=1) / nodors_range\n",
    "    coverage_std_mu = np.std(coverage_i, axis=1) / nodors_range  # std across seed\n",
    "    ax.plot(nodors_range, coverage_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, coverage_line_mu-coverage_std_mu, \n",
    "                   coverage_line_mu+coverage_std_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Average fraction\\nodors covered\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "#ax.legend(handles=handles, labels=labels, ncol=3,\n",
    "#          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "#          loc=\"lower center\", frameon=False)\n",
    "#ax.set_ylim([0.4, 1.15])\n",
    "# Annotate with tau\n",
    "#ax.set_title(r\"$\\tau_{\\Theta} = \" + \"{:d}$ s\".format(int(mutau_grid[1, 0, chosen_tau_j])), y=0.9)\n",
    "\n",
    "\n",
    "fig.tight_layout(h_pad=3.0)\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_ibcm_vs_nodor_combined.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel I: compensation between concentration scale and learning rate\n",
    "Convergence is similar if we keep $\\mu c_0^2 = \\mathrm{cst}$ as we vary the concentration scale $c_0$. When we also start varying the average conc. by changing time statistics (like whiff and blank concentrations), then we can keep $\\mu \\langle c \\rangle^2 = \\mathrm{cst}$ to get approximately the same scaling, although it's not perfect. \n",
    "\n",
    "One reason for this is that the convergence time of the first phase scales as $1/(\\mu \\langle c \\rangle^2 + \\sigma^2)$, so as $1/(\\mu c_0^2)$ (see analytical results on $h_\\mathrm{d}$); to keep this time constant, we need to keep $\\mu c_0^2 = \\mathrm{cst}$. \n",
    "\n",
    "The other reason is that the same scaling requirement persists at the fixed point. There, $h_{\\mathrm{sp}}$ and $h_{\\mathrm{ns}}$ scale as $1/c_0$ (e.g. they have terms like $\\langle c \\rangle / \\sigma^2$, etc.). Since $h = \\mathbf{m}^T \\mathbf{s} \\sim m c_0$, this means that the synaptic weights $ m \\sim 1/c_0^2$. Thus, terms in the IBCM equation scale as \n",
    "\n",
    "$$ h^2 \\mathbf{s} \\sim 1/c_0 $$\n",
    "$$ h \\Theta \\mathbf{s} \\sim h^3 \\mathbf{s} \\sim 1/c_0^2 $$\n",
    "$$ -\\delta \\mathbf{m} \\sim 1 / c_0^2 \\quad \\mathrm{(decay\\, term)} $$\n",
    "\n",
    "so we need to use a learning rate $\\mu = \\mu_0 c_0^2$ to maintain the same overall scale for the last two terms in the ODE. \n",
    "\n",
    "\n",
    "Note also that the convergence is not exactly the same in the Law and Cooper variant, where the effective learning rate is normalized by $k_{\\Theta} + \\Theta$ and so varies differently, through $\\Theta$'s magnitude, depending on $c_0$. \n",
    "\n",
    "### Scaling of $h$ with the background magnitude\n",
    "Importantly, note that $h \\sim 1/c_0$ at steady-state, so the alignment gap needs to be looked at relative to the background scale: plot $\\Delta h c_0$. \n",
    "\n",
    "Another option, more agnostic to $c_0$ directly, would be to scale relative to the analytical prediction $h_{\\mathrm{sp}}$, which gives the expected scale of the alignment gap, irrespective of whether the simulations reached it or not. Finding $\\Delta h / (h_\\mathrm{sp} - h_\\mathrm{ns})$ near $1$ would be good, far from 1 would be bad (means it hasn't converged). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "with np.load(pj(data_folder_conv, \"convergence_vs_background_ampli_results_3odors.npz\")) as conv_results:\n",
    "    muc0_grid = conv_results[\"muc0_grid\"]\n",
    "    muc0_grid[0] *= factor_mu_s  # convert mu into s^-1\n",
    "    align_gaps_c0 = conv_results[\"align_gaps\"]\n",
    "    gamma_specifs_c0 = conv_results[\"gamma_specifs\"]\n",
    "    hgamma_varis_c0 = conv_results[\"hgamma_varis\"]\n",
    "    \n",
    "    # Predicted gaps to scale the alignment gap and the variance\n",
    "    align_gaps_th = conv_results[\"gaps_th\"]\n",
    "    align_gaps_c0_scaled = align_gaps_c0 / align_gaps_th[:, :, None, None]\n",
    "    hgamma_varis_c0_scaled = hgamma_varis_c0 / align_gaps_th[:, :, None, None, None]**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulations with large $\\mu$ and large $c_0$ give very large IBCM fluctuations\n",
    "# as can be checked manually by running simulations with these parameters.\n",
    "# so their large alignment is an artifact. We filter out seeds (axis 2) \n",
    "# where at least one neuron (max along axis 3) have an alignment (axis4) above a threshold. \n",
    "# The threshold is a standard deviation in h_gamma larger than 8x the theoretical h gap\n",
    "# in other words, a scaled variance (hgamma_varis_c0_scaled) larger than 64. \n",
    "seeds_excess_variance = hgamma_varis_c0_scaled.max(axis=4).max(axis=3) > 64.0\n",
    "# Neurons in these fluctuating seeds do not really align -- set their gap to zero\n",
    "align_gaps_c0_scaled_corrected = np.copy(align_gaps_c0_scaled)\n",
    "n_i = align_gaps_c0_scaled.shape[3]  # number of neurons\n",
    "mask = np.tile(seeds_excess_variance[:, :, :, None], (1, 1, 1, n_i))\n",
    "align_gaps_c0_scaled_corrected[mask] = 0.0\n",
    "\n",
    "# Print how many seeds in each grid point have large fluctuations \n",
    "# and thus artifactual alignment strength\n",
    "print(seeds_excess_variance.sum(axis=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_mu, n_c0 = muc0_grid.shape[1:3]\n",
    "\n",
    "mu_range = muc0_grid[0, :, 0]\n",
    "c0_range = muc0_grid[1, 0, :]  # c0 varies along axis 1\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(np.log10(muc0_grid[1]), np.log10(muc0_grid[0]), \n",
    "                   np.mean(align_gaps_c0_scaled_corrected, axis=(2, 3)), \n",
    "                   cmap=\"viridis\", vmin=0.0)\n",
    "ax.set(ylabel=r\"$\\log_{10}$ learning rate $\\mu$\", \n",
    "       xlabel=r\"$\\log_{10}$ concentration scale $c_0$\")#, yscale=\"log\", xscale=\"log\")\n",
    "#ax.set_ylim(mu_range[0], mu_range[-1])\n",
    "#ax.set_xlim(c0_range[0], c0_range[-1])\n",
    "fig.colorbar(im, label=r\"Scaled alignment $\\Delta h$\")\n",
    "\n",
    "# Add a theoretical line mu \\sim 1/c_0^2\n",
    "# Reference point is mu = 7.5e-4 * factor_mu_s, c_0 = 0.6\n",
    "mu_ref = mu_range[2]\n",
    "c0_ref = c0_range[2]\n",
    "mu_c0_ref_line = mu_ref * (c0_ref / c0_range) ** 2.0\n",
    "# Offset this line, multiply c0 by a factor < 0\n",
    "ax.plot(np.log10(c0_range), np.log10(mu_c0_ref_line), ls=\"--\", color=\"w\")\n",
    "ax.annotate(r\"$\\mu c_0^2 = \\mathrm{cst}$\", xy=(np.log10(c0_ref)-0.2, np.log10(mu_ref)-1.25), \n",
    "            rotation=-45, color=\"w\")\n",
    "ax.annotate(\"Slow\\nconvergence\", xy=(np.log10(c0_range[0]), np.log10(mu_range[-1])), \n",
    "           ha=\"left\", va=\"bottom\", color=\"w\", fontsize=6)\n",
    "ax.annotate(r\"Large $\\bar{h}_{i\\gamma}$\" + \"\\nfluctuations\", xy=(np.log10(c0_range[-1]), np.log10(mu_range[0])), \n",
    "           ha=\"right\", va=\"top\", color=\"w\", fontsize=6)\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_backscale_alignment_gap.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel J: as a function of whiff and blank duration\n",
    "The learning rate is adapted to the average concentration. \n",
    "\n",
    "Alignment gap $\\Delta h \\sim 1/\\langle c \\rangle$, and $\\langle c \\rangle \\sim \\chi$ the probability of a whiff, so we plot $\\chi \\Delta h$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "with np.load(pj(data_folder_conv, \"convergence_vs_turbulence_strength_results_3odors.npz\")) as conv_results:\n",
    "    twtb_grid = conv_results[\"twtb_grid\"]\n",
    "    twtb_grid[0] *= dtscale_s  # convert t_whiffs into s\n",
    "    twtb_grid[1] *= dtscale_s  # convert t_blanks into s\n",
    "    align_gaps_twtb = conv_results[\"align_gaps\"]\n",
    "    gamma_specifs_twtb = conv_results[\"gamma_specifs\"]\n",
    "    hgamma_varis_twtb = conv_results[\"hgamma_varis\"]\n",
    "    # Predicted gaps to scale the alignment gap and the variance\n",
    "    align_gaps_th = conv_results[\"gaps_th\"]\n",
    "    align_gaps_twtb_scaled = align_gaps_twtb / align_gaps_th[:, :, None, None]\n",
    "    hgamma_varis_twtb_scaled = hgamma_varis_twtb / align_gaps_th[:, :, None, None, None]**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tw, n_tb = twtb_grid.shape[1:3]\n",
    "\n",
    "tw_range = twtb_grid[0, :, 0]  # ij meshgrid indexing, tw is rows (y axis)\n",
    "tb_range = twtb_grid[1, 0, :]\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "# x axis is t_b, y axis is t_w\n",
    "im = ax.pcolormesh(np.arange(tb_range.shape[0]), np.arange(tw_range.shape[0]), \n",
    "                   np.mean(align_gaps_twtb_scaled, axis=(2, 3)), cmap=\"viridis\", vmin=0.0)\n",
    "ax.set(xlabel=r\"Blank max. duration (s)\", ylabel=r\"Whiff max. duration (s)\")\n",
    "#ax.set_xlim(tb_range[0], tb_range[-1])\n",
    "#ax.set_ylim(tw_range[0], tw_range[-1])\n",
    "ax.set_xticks(np.arange(tw_range.shape[0]))\n",
    "ax.set_yticks(np.arange(tb_range.shape[0]))\n",
    "def label_formatter(x):\n",
    "    if x < 0.1:\n",
    "        return \"{:.2f}\".format(x)\n",
    "    elif x < 1.0:\n",
    "        return \"{:.1f}\".format(x)\n",
    "    else:\n",
    "        return \"{:d}\".format(int(x))\n",
    "ax.set_xticklabels((label_formatter(a) for a in tw_range))\n",
    "ax.set_yticklabels((label_formatter(a) for a in tb_range))\n",
    "tau_avg = 2000.0 * dtscale_s\n",
    "# x coords are 0, ..., 6 for t=2*10^1 * 0.01 s, ..., t=2*10^4 * 0.01 s, etc.\n",
    "# so convert tau in unit steps to coords as t=2*10^{x/2+1-2} -> x = 2*(log10(t/2) + 1)\n",
    "def s_to_x(s):\n",
    "    return (2.0 * (np.log10(s/2) + 1.0))\n",
    "taux, tauy = s_to_x(tau_avg), s_to_x(tau_avg)\n",
    "axis_to_data = ax.transAxes + ax.transData.inverted()\n",
    "data_to_axis = axis_to_data.inverted()\n",
    "ax.axvline(taux, ymax=data_to_axis.transform((taux, tauy))[1], ls=\"--\", color=\"w\")\n",
    "ax.axhline(tauy, xmax=data_to_axis.transform((taux, tauy))[0], ls=\"--\", color=\"w\")\n",
    "ax.annotate(r\"$\\tau_{\\Theta}$\", xy=(taux/2, tauy+0.1), ha=\"center\", va=\"bottom\", color=\"w\")\n",
    "ax.annotate(r\"$\\tau_{\\Theta}$\", xy=(taux+0.1, tauy/2), ha=\"left\", va=\"center\", color=\"w\")\n",
    "\n",
    "fig.colorbar(im, label=r\"Scaled alignment $\\Delta h$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_whiffblanks_duration_alignment_gap.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last two panels: BioPCA versus turbulence strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "with np.load(pj(data_folder_conv, \"biopca_convergence_vs_background_ampli_3odors.npz\")) as conv_results:\n",
    "    muc0_grid_pca = conv_results[\"muc0_grid\"]\n",
    "    muc0_grid_pca[0] *= factor_mu_s  # convert mu into s^-1\n",
    "    true_pvs = conv_results[\"true_pvs\"]\n",
    "    learn_pvs = conv_results[\"learn_pvs\"]\n",
    "    vari_pvs = conv_results[\"vari_pvs\"]\n",
    "    align_errs = conv_results[\"align_errs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(a, axis=-1):\n",
    "    return np.sqrt(np.sum(a**2, axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_c0 = muc0_grid_pca.shape[1:3]\n",
    "\n",
    "mu_range = muc0_grid_pca[0, :, 0]\n",
    "c0_range = muc0_grid_pca[1, 0, :]  # c0 varies along axis 1\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.05, \n",
    "                    plt.rcParams[\"figure.figsize\"][1])\n",
    "\n",
    "im = ax.pcolormesh(np.log10(muc0_grid_pca[1]), np.log10(muc0_grid_pca[0]), \n",
    "                   np.mean(np.log10(align_errs), axis=2), \n",
    "                   cmap=\"viridis_r\")\n",
    "\n",
    "ax.annotate(\"Slow\\nconvergence\", xy=(np.log10(c0_range[0]), np.log10(mu_range[0])), \n",
    "           ha=\"left\", va=\"bottom\", color=\"w\", fontsize=6)\n",
    "ax.annotate(\"Large fluctuations\", xy=(0.5*np.log10(c0_range[-1]*c0_range[0]), np.log10(mu_range[-1])), \n",
    "           ha=\"center\", va=\"top\", color=\"w\", fontsize=6)\n",
    "ax.set(ylabel=r\"$\\log_{10}$ learning rate $\\mu$\", \n",
    "       xlabel=r\"$\\log_{10}$ concentration scale $c_0$\")#, yscale=\"log\", xscale=\"log\")\n",
    "#ax.set_ylim(mu_range[0], mu_range[-1])\n",
    "#ax.set_xlim(c0_range[0], c0_range[-1])\n",
    "fig.colorbar(im, label=r\"$\\log_{10}$ alignment error\" +\"\\n(brighter is better)\")\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_biopca_backscale_alignerr.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of turbulence strength?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "with np.load(pj(data_folder_conv, \"biopca_convergence_vs_turbulence_strength_3odors.npz\")) as conv_results:\n",
    "    twtb_grid = conv_results[\"twtb_grid\"]\n",
    "    twtb_grid[0] *= dtscale_s  # convert t_whiffs into s\n",
    "    twtb_grid[1] *= dtscale_s  # convert t_blanks into s\n",
    "    true_pvs_twtb = conv_results[\"true_pvs\"]\n",
    "    learn_pvs_twtb = conv_results[\"learn_pvs\"]\n",
    "    vari_pvs_twtb = conv_results[\"vari_pvs\"]\n",
    "    align_errs_twtb = conv_results[\"align_errs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tw, n_tb = twtb_grid.shape[1:3]\n",
    "\n",
    "tw_range = twtb_grid[0, :, 0]  # ij meshgrid indexing, tw is rows (y axis)\n",
    "tb_range = twtb_grid[1, 0, :]\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.05, \n",
    "                    plt.rcParams[\"figure.figsize\"][1])\n",
    "# x axis is t_b, y axis is t_w\n",
    "\n",
    "# Alignment error\n",
    "im = ax.pcolormesh(np.arange(tb_range.shape[0]), np.arange(tw_range.shape[0]), \n",
    "                   np.mean(np.log10(align_errs_twtb), axis=2), cmap=\"viridis_r\")\n",
    "\n",
    "# CV (sigma/mean) in eigenvalues\n",
    "#im = ax.pcolormesh(np.arange(tb_range.shape[0]), np.arange(tw_range.shape[0]), \n",
    "#                   np.mean(np.sqrt(vari_pvs_twtb)/true_pvs_twtb[:, :, :, :3], axis=(2,3)), cmap=\"viridis_r\")\n",
    "\n",
    "# Difference in eigenvalues\n",
    "#im = ax.pcolormesh(np.arange(tb_range.shape[0]), np.arange(tw_range.shape[0]), \n",
    "#                   np.mean(np.log10(learn_pvs_twtb/true_pvs_twtb[:, :, :, :3]), axis=(2,3)), cmap=\"viridis\")\n",
    "\n",
    "\n",
    "ax.set(xlabel=r\"Blank max. duration (s)\", ylabel=r\"Whiff max. duration (s)\")\n",
    "#ax.set_xlim(tb_range[0], tb_range[-1])\n",
    "#ax.set_ylim(tw_range[0], tw_range[-1])\n",
    "ax.set_xticks(np.arange(tw_range.shape[0]))\n",
    "ax.set_yticks(np.arange(tb_range.shape[0]))\n",
    "def label_formatter(x):\n",
    "    if x < 0.1:\n",
    "        return \"{:.2f}\".format(x)\n",
    "    elif x < 1.0:\n",
    "        return \"{:.1f}\".format(x)\n",
    "    else:\n",
    "        return \"{:d}\".format(int(x))\n",
    "ax.set_xticklabels((label_formatter(a) for a in tw_range))\n",
    "ax.set_yticklabels((label_formatter(a) for a in tb_range))\n",
    "\n",
    "fig.colorbar(im, label=r\"$\\log_{10}$ alignment error\" + \"\\n (brighter is better)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Maybe show in referee response only, since it is similar to IBCM\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_biopca_whiffblanks_alignerr.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of the number of odors on BioPCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_align_errs = []\n",
    "concatenated_pvs_diffs = []\n",
    "concatenated_pvs_cv = []\n",
    "nodors_range = np.asarray([3, 4, 5, 6, 8])\n",
    "f_name_prefix = \"biopca_convergence_vs_background_ampli_\"\n",
    "for n in nodors_range:\n",
    "    try:\n",
    "        fp = np.load(pj(data_folder_conv, f_name_prefix + f\"{n}odors.npz\"))\n",
    "    except FileNotFoundError: \n",
    "        continue\n",
    "    else:\n",
    "        concatenated_align_errs.append(fp[\"align_errs\"])\n",
    "        pvs_cv_n = np.mean(np.sqrt(fp[\"vari_pvs\"]) / fp[\"true_pvs\"][:, :, :, :n], axis=3)\n",
    "        concatenated_pvs_cv.append(pvs_cv_n)\n",
    "        pvs_log_diff = l2_norm(np.log10(fp[\"learn_pvs\"] / fp[\"true_pvs\"][:, :, :, :n]), axis=3)\n",
    "        concatenated_pvs_diffs.append(pvs_log_diff)\n",
    "        \n",
    "concatenated_pvs_cv = np.stack(concatenated_pvs_cv)\n",
    "concatenated_pvs_diffs = np.stack(concatenated_pvs_diffs)\n",
    "concatenated_align_errs = np.stack(concatenated_align_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muc0_grid_pca[1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_c0_j = 2  # 0.6\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.1, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = muc0_grid_pca[0, :, 0]\n",
    "n_mu = mu_range.shape[0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    y = np.log10(concatenated_align_errs[:, i, chosen_c0_j])\n",
    "    err_mean_line_mu = np.mean(y, axis=1)\n",
    "    err_std_line_mu = np.std(y, axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, err_mean_line_mu, color=colors[i], \n",
    "            label=\"{:.2f}\".format(muc0_grid_pca[0, i, 0]*60.0), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, err_mean_line_mu-err_std_line_mu, \n",
    "                   err_mean_line_mu+err_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=r\"$\\log_{10}$ alignment error\" + \"\\n(smaller is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{min^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_biopca_alignerr_vs_nodors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_c0_j = 2  # index 0: 0.0375, index 4: 9.6, index 2: 0.6\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.1, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = muc0_grid_pca[0, :, 0]\n",
    "n_mu = mu_range.shape[0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    y = np.log10(concatenated_pvs_diffs)\n",
    "    err_mean_line_mu = np.mean(y[:, i, chosen_c0_j], axis=1)\n",
    "    err_std_line_mu = np.std(y[:, i, chosen_c0_j], axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, err_mean_line_mu, color=colors[i], \n",
    "            label=\"{:.2f}\".format(muc0_grid_pca[0, i, 0]*60.0), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, err_mean_line_mu-err_std_line_mu, \n",
    "                   err_mean_line_mu+err_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=r\"$\\log_{10}$ error on PVs\" + \"\\n(smaller is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{min^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "# Maybe show to referees only, since similar to alignment error and avoids introducing\n",
    "# an extra metric in the manuscript\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_biopca_pvs_error_vs_nodors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_c0_j = 2  # index 0: 0.0375, index 4: 9.6, index 2: 0.6\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.1, plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = muc0_grid_pca[0, :, 0]\n",
    "n_mu = mu_range.shape[0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    y = np.log10(concatenated_pvs_cv)\n",
    "    err_mean_line_mu = np.mean(y[:, i, chosen_c0_j], axis=1)\n",
    "    err_std_line_mu = np.std(y[:, i, chosen_c0_j], axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, err_mean_line_mu, color=colors[i], \n",
    "            label=\"{:.2f}\".format(muc0_grid_pca[0, i, 0]*60.0), marker=markerstyles[i], ms=3)\n",
    "    ax.fill_between(nodors_range, err_mean_line_mu-err_std_line_mu, \n",
    "                   err_mean_line_mu+err_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"# odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=r\"$\\log_{10}$ error on PVs\" + \"\\n(smaller is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{min^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_biopca_pvs_vari_vs_nodors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
