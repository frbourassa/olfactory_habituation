{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "# For multiprocessing\n",
    "import multiprocessing\n",
    "from psutil import cpu_count\n",
    "# This one, with logical=False, is better than multiprocessing.cpu_count\n",
    "# https://stackoverflow.com/questions/40217873/multiprocessing-use-only-the-physical-cores\n",
    "\n",
    "# Change the forking method used by multiprocessing, avoids errors on Mac OS Catalina\n",
    "# e.g. https://github.com/matplotlib/matplotlib/issues/15410\n",
    "multiprocessing.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to model for odorants and receptors\n",
    "From Reddy et al. 2018, the way to define an odorant is to specify a vector of binding affinities, $\\vec{\\kappa}$, and a vector of activation efficacies, $\\vec{\\eta}$. For a background, we define $n_{mix}$ such odorants and combine them into $\\vec{\\eta}_{mix}$ and $\\vec{\\kappa}_{mix}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.shen2020_habituation_model import generate_odorant, generate_background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the olfactory network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.shen2020_habituation_model import project_neural_tag, jaccard, create_sparse_proj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to habituation and background fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.shen2020_habituation_model import time_evolve_habituation_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.shen2020_habituation_model import combine_odorants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation with fluctuating background\n",
    "Define here the function that takes an initial background and weight vector, and simulates its habituation with fluctuations of the background. \n",
    "\n",
    "### Background fluctuation model\n",
    "Given two odorants in the background, A and B, moderately fluctuate the proportion of those two odorants in the mixture, on a time scale much faster than habituation itself. For now, we don't even fluctuate the total concentration (average of the input vector), assuming there is rescaling happening somewhere in the entry layers.\n",
    "\n",
    "Let $f \\in [0, 1]$ be the fraction of odorant A in the background. We want $f$ to be on average $0.5$ and fluctuate mildly. To simplify things, I simulate it as a Gaussian process with standard deviation $\\sigma_f$ on the order of $0.1$ and clip the value of $f$ if it ever reaches a value outside of $[0, 1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting those two things is computationally inefficient but more flexible for early development. \n",
    "# Eventually, write specialized functions where the update is made within the habituation function\n",
    "def update_fluct_proportion(bkvec, f_bk, rates, odor_pair, noises):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        bkvec: ORN activation vector by the background (not needed, to respect the call signature)\n",
    "        f_bk: array of length 1, containing proportion of odorants\n",
    "        rates: tau_f, mean_f, vari_f, Gaussian parameters and time scale of f's fluctuation\n",
    "        odor_pair: extra arguments, here a pair of 2 single odorants that compose the background. \n",
    "        noises: pre-generated normal(0, 1) samples, one per component in f_bk\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    tau_f, mean_f, vari_f = rates\n",
    "    \n",
    "    # Update f\n",
    "    f_bk[0] = f_bk[0] - (f_bk[0]-mean_f)/tau_f + np.sqrt(2*vari_f/tau_f) * noises[0]\n",
    "    f_bk = np.clip(f_bk, 0, 1)\n",
    "    \n",
    "    # Compute new background vector\n",
    "    bkvec = combine_odorants(odor_pair[0], odor_pair[1], f_bk[0])\n",
    "    return bkvec, f_bk\n",
    "\n",
    "def habituate_fluct_background(w_vec0, back_vec0, state_vec0, nstep, learnrates, bfluct_rates, rndgen, \n",
    "                               bfluct_args=(), bk_update=update_fluct_proportion):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        back_vec0: initial background vector of ORN activations\n",
    "        state_vec0: initial background state vector (e.g. concentration, proportions)\n",
    "        bk_update: general function called at each time step to update the background (Markov process). \n",
    "            Call signature: back_vec, state_vec, bfluct_rates, bfluct_args, noises\n",
    "            where back_vec is the current background vector, state_vec is a vector of other background\n",
    "            properties, e.g. concentration and proportions, and bfluct_params is a tuple\n",
    "            of other arguments needed by the chosen bk_update\n",
    "        rndgen (np.random.Generator): the random generator\n",
    "        bfluct_rates: tuple of rates for the stochastic process of the background fluctuation\n",
    "        bfluct_params: other parameters for the background fluctuations\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    if nstep > 1e6:\n",
    "        raise ValueError(\"Consider asking for less than 1e6 steps at a time\")\n",
    "    alpha, beta = learnrates\n",
    "    \n",
    "    # Initialize variables\n",
    "    w_vec = w_vec0.copy()\n",
    "    t = 0  # number of steps taken\n",
    "    \n",
    "    # Initial the background properties (do not modify in-place)\n",
    "    back_vec = back_vec0.copy()\n",
    "    state_vec = state_vec0.copy()\n",
    "    state_course = np.zeros((nstep, state_vec.shape[0]))\n",
    "    \n",
    "    # Pre-generate normal samples for each time step (assuming a Langevin equation)\n",
    "    all_noises = rndgen.normal(size=state_vec0.size*nstep).reshape(nstep, -1)\n",
    "    \n",
    "    # Iterate until satisfing the number of steps asked for\n",
    "    while t < nstep:\n",
    "        # Update the background\n",
    "        back_vec, state_vec = bk_update(back_vec, state_vec, bfluct_rates, bfluct_args, all_noises[t])\n",
    "        state_course[t] = state_vec\n",
    "        # Update w, remove the max(s-w, 0) thing for the update rule, see if it still works (it should)\n",
    "        #x_vec = np.maximum(back_vec - w_vec, 0)\n",
    "        #w_vec = w_vec + alpha * x_vec - beta* w_vec\n",
    "        w_vec = w_vec + alpha * back_vec - (alpha + beta)* w_vec\n",
    "        t += 1\n",
    "\n",
    "    return w_vec, back_vec, state_course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: introducing a new pure odor after habituating to a background\n",
    "This gives as much chance as possible to the habituation scheme, because the background is completely replaced with a new odor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_deviation(x):\n",
    "    return np.mean(np.abs(x - np.mean(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of odors\n",
    "def similarity_habituation_proportion(all_odors=None, n_odors=100, n_orn=50, \n",
    "        n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "        bfluct_rates=(4, 0.5, 0.1), adapt_kc=True, fix_thresh=None, seed=48225):\n",
    "    \"\"\"Using update_fluct_proportion as the background fluctuation, habituate nreps times\n",
    "    to nbacks randomly selected pairs of odorants as a background, for each odorant in the set. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize projection matrix, odors, containers for scores\n",
    "    rdgen_mix = np.random.default_rng(seed=seed)\n",
    "    if all_odors is None:\n",
    "        all_odors = np.vstack([generate_odorant(n_orn, rdgen_mix) for i in range(n_odors)])\n",
    "    else:\n",
    "        n_odors = all_odors.shape[1]\n",
    "        n_orn = all_odors.shape[0]\n",
    "        all_odors = all_odors.values.T  # Each row is an odorant now\n",
    "    proj_mat = create_sparse_proj_mat(n_kc=n_kc, n_rec=n_orn, rgen=rdgen_mix, fraction_filled=n_pn_per_kc/n_orn)\n",
    "    simil_before = np.zeros([n_odors, nbacks, nreps])  # J(odor, s'') before\n",
    "    simil_after = np.zeros([n_odors, nbacks, nreps])   # J(odor, s'') after\n",
    "    \n",
    "    # Don't save the backgrounds or their time courses for now.\n",
    "    # Just save the last one for each odorant actually, to check it's indeed the desired process\n",
    "    last_conc_runs = np.zeros((n_odors, steps))\n",
    "    \n",
    "    # For each odor, habituate to a random choice of a pair of odors, nback times\n",
    "    # and run nruns time each background, because the time course is stochastic.\n",
    "    wzero = np.zeros(n_orn)\n",
    "    f_vec0 = 0.5*np.ones(1)\n",
    "    projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc, fix_thresh=fix_thresh)\n",
    "    for i in range(n_odors):\n",
    "        odi = all_odors[i]\n",
    "        tag_i_alone = project_neural_tag(odi, wzero, proj_mat, **projtag_kwargs)\n",
    "        other_odors = list(range(n_odors))\n",
    "        other_odors.remove(i)\n",
    "        for j in range(nbacks):\n",
    "            # Select a background randomly\n",
    "            od1, od2 = all_odors[rdgen_mix.choice(other_odors, replace=False, size=2).astype(int)]\n",
    "            back_odor0 = combine_odorants(od1, od2, 0.5)\n",
    "            w_vec0 = np.zeros(n_orn)\n",
    "            # Compute tag of odi mixed with that background, prior to habituation\n",
    "            odmix = combine_odorants(back_odor0, odi, 0.8)\n",
    "            tag_mix_before = project_neural_tag(odmix, w_vec0, proj_mat, **projtag_kwargs)\n",
    "            # Make k runs with that background\n",
    "            for k in range(nreps):\n",
    "                w_vec, back_odor, f_vec = habituate_fluct_background(w_vec0, \n",
    "                    back_odor0, f_vec0, steps, learnrates, bfluct_rates,  \n",
    "                    rdgen_mix, bfluct_args=(od1, od2), bk_update=update_fluct_proportion)\n",
    "                # Compute the tag of odor i after habituation to this background, \n",
    "                # mixed with back_vec\n",
    "                odmix = combine_odorants(back_odor, odi, 0.8)\n",
    "                tag_mix_after = project_neural_tag(odmix, w_vec, proj_mat, **projtag_kwargs)\n",
    "                \n",
    "                # Compute the Jaccard of i's tag alone with the mix before and after habituation\n",
    "                simil_before[i, j, k] = jaccard(tag_i_alone, tag_mix_before)\n",
    "                simil_after[i, j, k] = jaccard(tag_i_alone, tag_mix_after)\n",
    "                \n",
    "        # Before moving to next odorant, save latest f time course\n",
    "        last_conc_runs[i] = f_vec.flatten()\n",
    "    \n",
    "    return simil_before, simil_after, last_conc_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = perf_counter()\n",
    "n_odors = 100\n",
    "sim_mats = similarity_habituation_proportion(all_odors=None, n_odors=20, n_orn=50, \n",
    "        n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "        bfluct_rates=(4, 0.5, 0.04), adapt_kc=True, fix_thresh=None, seed=48225)\n",
    "end_t = perf_counter()\n",
    "print(\"Time per run:\", 1000*(end_t - start_t)/sim_mats[0].size, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after(samp_before, samp_after, figax=None):\n",
    "    if figax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig, ax = figax\n",
    "\n",
    "    barcolors = sns.color_palette(\"mako\", n_colors=4)[1:3]\n",
    "    yerr = [mean_abs_deviation(samp_before), mean_abs_deviation(samp_after)]\n",
    "    median_before = np.median(samp_before)\n",
    "    median_after = np.median(samp_after)\n",
    "    print(\"Median before:\", median_before)\n",
    "    print(\"Median_after:\", median_after)\n",
    "    ax.bar(0, median_before, yerr=yerr[0], facecolor=barcolors[0], edgecolor=\"k\", alpha=0.5)\n",
    "    ax.bar(1, median_after, yerr=yerr[1], facecolor=barcolors[1], edgecolor=\"k\", alpha=0.5)\n",
    "    ax.scatter(0.075*np.random.normal(size=samp_before.size), samp_before, s=2,\n",
    "                    color=barcolors[0], alpha=0.7, lw=0.5)\n",
    "    ax.scatter(1+0.075*np.random.normal(size=samp_after.size), samp_after, s=2,\n",
    "                    color=barcolors[1], alpha=0.8, lw=0.5)\n",
    "    ax.set_title(r\"$s_A(0)$ vs $s_{mix}(t)$\")\n",
    "    ax.set_ylabel(\"Jaccard similarity\")\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "    ax.set_ylim(ylims[0], ylims[1]+0.1)\n",
    "    ax.annotate(\"Before\", xy=(0, ylims[1]+0.05), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    ax.annotate(\"After\", xy=(1, ylims[1]+0.05), ha=\"center\", va=\"top\", fontsize=8)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([r\"$t=0$\", r\"$t=50$\"])\n",
    "    ax.set_xlabel(\"Habituation\")\n",
    "\n",
    "    return [fig, ax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_before = sim_mats[0].flatten()\n",
    "samples_after = sim_mats[1].flatten()\n",
    "\n",
    "# Prepare two barplots: one for A vs mix before and after, one for B vs mix before and after\n",
    "fig, ax = plot_before_after(samples_before, samples_after, figax=None)\n",
    "    \n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/shen2020_related/habituation_fluctuating_proportion.pdf\", transparent=True)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(6, 3)\n",
    "axes = axes.flatten()\n",
    "\n",
    "times = np.arange(sim_mats[2].shape[1])\n",
    "colors = sns.color_palette(\"mako\", n_colors=sim_mats[2].shape[0])\n",
    "for i in range(sim_mats[2].shape[0]):\n",
    "    axes[0].plot(times, sim_mats[2][i], lw=1., color=colors[i])\n",
    "axes[0].set(xlabel=\"Time steps\", ylabel=r\"$f$\")\n",
    "\n",
    "axes[1].hist(sim_mats[2].flatten(), linewidth=1., edgecolor=\"k\", facecolor=colors[len(colors)//2])\n",
    "axes[1].set(xlabel=r\"$f$\", ylabel=\"Frequency\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/shen2020_related/composition_fluctuations.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(sim_mats[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habituation to changing total concentration and composition\n",
    "Change proportion and total concentration independently. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fluct_proportion_conc(bkvec, fc_bk, rates, odor_pair, noises):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        bkvec: ORN activation vector by the background (not needed, to respect the call signature)\n",
    "        fc_bk: array of length 2, containing proportion of odorants and total concentration (average 10)\n",
    "        rates: tau_f, mean_f, vari_f, tau_c, mean_c, vari_c: \n",
    "            Gaussian parameters and time scale of f and c fluctuation\n",
    "        odor_pair: extra arguments, here a pair of 2 single odorants that compose the background. \n",
    "        noises: pre-generated normal(0, 1) samples, one per component in f_bk\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    tau_f, mean_f, vari_f, tau_c, mean_c, vari_c = rates\n",
    "    \n",
    "    # Update f\n",
    "    fc_bk[0] = fc_bk[0] - (fc_bk[0]-mean_f)/tau_f + np.sqrt(2*vari_f/tau_f) * noises[0]\n",
    "    fc_bk[1] = fc_bk[1] - (fc_bk[1]-mean_c)/tau_c + np.sqrt(2*vari_c/tau_c) * noises[1]\n",
    "    fc_bk[0] = np.clip(fc_bk[0], 0, 1)\n",
    "    fc_bk[1] = np.clip(fc_bk[1], a_min=0, a_max=np.inf)  # keep the concentration non-negative\n",
    "    \n",
    "    # Compute new background vector\n",
    "    bkvec = combine_odorants(odor_pair[0], odor_pair[1], fc_bk[0]) * fc_bk[1] / mean_c\n",
    "    return bkvec, fc_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of odors\n",
    "def similarity_habituation_proportion_conc(all_odors=None, n_odors=100, n_orn=50, \n",
    "        n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "        bfluct_rates=(4, 0.5, 0.1, 4, 10, 4), adapt_kc=False, fix_thresh=20, seed=4822537):\n",
    "    \"\"\"Using update_fluct_proportion as the background fluctuation, habituate nreps times\n",
    "    to nbacks randomly selected pairs of odorants as a background, for each odorant in the set. \n",
    "    \"\"\"\n",
    "    # Initialize projection matrix, odors, containers for scores\n",
    "    fix_thresh = bfluct_rates[-2]  # Average concentration should be the fixed threshold\n",
    "    rdgen_mix = np.random.default_rng(seed=seed)\n",
    "    if all_odors is None:\n",
    "        # Normalize each odor to be equal to the fixed threshold\n",
    "        all_odors = np.vstack([generate_odorant(n_orn, rdgen_mix) for i in range(n_odors)])\n",
    "    else:\n",
    "        n_odors = all_odors.shape[1]\n",
    "        n_orn = all_odors.shape[0]\n",
    "        all_odors = all_odors.values.T  # Each row is an odorant now\n",
    "    proj_mat = create_sparse_proj_mat(n_kc=n_kc, n_rec=n_orn, rgen=rdgen_mix, fraction_filled=n_pn_per_kc/n_orn)\n",
    "    simil_before = np.zeros([n_odors, nbacks, nreps])  # J(odor, s'') before\n",
    "    simil_after = np.zeros([n_odors, nbacks, nreps])   # J(odor, s'') after\n",
    "    \n",
    "    # Don't save the backgrounds or their time courses for now.\n",
    "    # Just save the last one for each odorant actually, to check it's indeed the desired process\n",
    "    last_conc_runs = np.zeros((n_odors, steps, 2))\n",
    "    \n",
    "    # For each odor, habituate to a random choice of a pair of odors, nback times\n",
    "    # and run nruns time each background, because the time course is stochastic.\n",
    "    wzero = np.zeros(n_orn)\n",
    "    projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc, fix_thresh=fix_thresh)\n",
    "    fc_vec0 = np.asarray([bfluct_rates[1], bfluct_rates[-2]])\n",
    "    for i in range(n_odors):\n",
    "        odi = all_odors[i]\n",
    "        tag_i_alone = project_neural_tag(odi, wzero, proj_mat, **projtag_kwargs)\n",
    "        other_odors = list(range(n_odors))\n",
    "        other_odors.remove(i)\n",
    "        for j in range(nbacks):\n",
    "            # Select a background randomly\n",
    "            od1, od2 = all_odors[rdgen_mix.choice(other_odors, replace=False, size=2).astype(int)]\n",
    "            back_odor0 = combine_odorants(od1, od2, 0.5)\n",
    "            w_vec0 = np.zeros(n_orn)\n",
    "            # Compute tag of odi mixed with that background, prior to habituation\n",
    "            odmix = combine_odorants(back_odor0, odi, 0.8)\n",
    "            tag_mix_before = project_neural_tag(odmix, w_vec0, proj_mat, **projtag_kwargs)\n",
    "            # Make k runs with that background\n",
    "            for k in range(nreps):\n",
    "                w_vec, back_odor, fc_vec = habituate_fluct_background(w_vec0, \n",
    "                    back_odor0, fc_vec0, steps, learnrates, bfluct_rates,  \n",
    "                    rdgen_mix, bfluct_args=(od1, od2), bk_update=update_fluct_proportion_conc)\n",
    "                # Compute the tag of odor i after habituation to this background, \n",
    "                # mixed with back_vec\n",
    "                odmix = combine_odorants(back_odor, odi, 0.8)\n",
    "                tag_mix_after = project_neural_tag(odmix, w_vec, proj_mat, **projtag_kwargs)\n",
    "                \n",
    "                # Compute the Jaccard of i's tag alone with the mix before and after habituation\n",
    "                simil_before[i, j, k] = jaccard(tag_i_alone, tag_mix_before)\n",
    "                simil_after[i, j, k] = jaccard(tag_i_alone, tag_mix_after)\n",
    "                \n",
    "        # Before moving to next odorant, save latest f time course\n",
    "        last_conc_runs[i] = fc_vec\n",
    "    \n",
    "    return simil_before, simil_after, last_conc_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = perf_counter()\n",
    "n_odors = 100\n",
    "sim_mats_c = similarity_habituation_proportion_conc(all_odors=None, n_odors=20, n_orn=50, \n",
    "        n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "        bfluct_rates=(4, 0.5, 0.04, 4, 10, 4), adapt_kc=False, fix_thresh=20, seed=4822537)\n",
    "end_t = perf_counter()\n",
    "print(\"Time per run:\", 1000*(end_t - start_t)/sim_mats_c[0].size, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_before = sim_mats_c[0].flatten()\n",
    "samples_after = sim_mats_c[1].flatten()\n",
    "\n",
    "# Prepare two barplots: one for A vs mix before and after, one for B vs mix before and after\n",
    "fig, ax = plot_before_after(samples_before, samples_after, figax=None)\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/habituation_fluctuating_composition_concentration.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(6, 3)\n",
    "axes = axes.flatten()\n",
    "\n",
    "times = np.arange(sim_mats_c[2].shape[1])\n",
    "colors = sns.color_palette(\"mako\", n_colors=sim_mats_c[2].shape[0])\n",
    "for i in range(sim_mats_c[2].shape[0]):\n",
    "    axes[0].plot(times, sim_mats_c[2][i, :, 1], lw=1., color=colors[i])\n",
    "axes[0].set(xlabel=\"Time steps\", ylabel=r\"$C$\")\n",
    "\n",
    "axes[1].hist(sim_mats_c[2][:, :, 1].flatten(), linewidth=1., edgecolor=\"k\", facecolor=colors[len(colors)//2])\n",
    "axes[1].set(xlabel=r\"$C$\", ylabel=\"Frequency\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/concentration_fluctuations_check.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next idea: change the number of odorants in the background\n",
    "If we have $N$ odorants int he background, we simulate $N$ independent gaussian stochastic processes for the fractions $f_i$ of odorants, and we normalize their sum to 1 when we compute the mixture background vector. \n",
    "Or, if we wanted the total concentration to fluctuate too, we could just not normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_many_odorants(odlist, props):\n",
    "    total_f = np.sum(props)\n",
    "    if total_f > 0:\n",
    "        props = props / total_f\n",
    "    else:\n",
    "        props = np.zeros(odlist.shape[0])\n",
    "    return np.sum(props[:, np.newaxis] * odlist, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fluct_manybk_conc(bkvec, fc_bk, rates, odor_array, noises):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        bkvec: ORN activation vector by the background (not needed, to respect the call signature)\n",
    "        fc_bk: array of length n_in_bk-2, containing proportion of odorants and total concentration (average 10)\n",
    "        rates: tau_f, mean_f, vari_f, tau_c, mean_c, vari_c: \n",
    "            Gaussian parameters and time scale of f and c fluctuation\n",
    "        odor_array: extra arguments, here an array where each row is one of the odorants in the background\n",
    "        noises: pre-generated normal(0, 1) samples, one per component in f_bk\n",
    "    \"\"\"\n",
    "    # Extract parameters. Assume same tau, mean and variance for all f's\n",
    "    tau_f, mean_f, vari_f, tau_c, mean_c, vari_c = rates\n",
    "    # Assume that odor_array.shape[0] = fc_bk.shape[0]-1, otherwise problems!\n",
    "    \n",
    "    # Update proportions f and concentration c\n",
    "    fc_bk[:-1] = fc_bk[:-1] - (fc_bk[:-1]-mean_f)/tau_f + np.sqrt(2*vari_f/tau_f) * noises[:-1]\n",
    "    fc_bk[-1] = fc_bk[-1] - (fc_bk[-1]-mean_c)/tau_c + np.sqrt(2*vari_c/tau_c) * noises[-1]\n",
    "    \n",
    "    fc_bk[:-1] = np.clip(fc_bk[:-1], 0, 1)\n",
    "    fc_bk[-1] = np.clip(fc_bk[-1], a_min=0, a_max=np.inf)  # keep the concentration non-negative\n",
    "    \n",
    "    # Compute new background vector\n",
    "    # The total concentration changes independently, the same way for all odorants, \n",
    "    # because they are all carried by the same wind. \n",
    "    bkvec = combine_many_odorants(odor_array, fc_bk[:-1]) * fc_bk[-1] / mean_c\n",
    "    return bkvec, fc_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of odors\n",
    "def similarity_habituation_mixback_conc(all_odors=None, n_odors=20, n_in_bk=4, n_orn=50, \n",
    "        n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "        bfluct_rates=(4, 0.5, 0.1, 4, 10, 4), adapt_kc=False, fix_thresh=20, seed=4822537):\n",
    "    \"\"\"Using update_fluct_proportion as the background fluctuation, habituate nreps times\n",
    "    to nbacks randomly selected pairs of odorants as a background, for each odorant in the set. \n",
    "    \"\"\"\n",
    "    # Initialize projection matrix, odors, containers for scores\n",
    "    fix_thresh = bfluct_rates[-2]  # Average concentration should be the fixed threshold\n",
    "    rdgen_mix = np.random.default_rng(seed=seed)\n",
    "    if n_in_bk > n_odors-1:\n",
    "        n_in_bk = n_odors-1\n",
    "    if all_odors is None:\n",
    "        # Normalize each odor to be equal to the fixed threshold\n",
    "        all_odors = np.vstack([generate_odorant(n_orn, rdgen_mix) for i in range(n_odors)])\n",
    "    else:\n",
    "        n_odors = all_odors.shape[1]\n",
    "        n_orn = all_odors.shape[0]\n",
    "        all_odors = all_odors.values.T  # Each row is an odorant now\n",
    "    proj_mat = create_sparse_proj_mat(n_kc=n_kc, n_rec=n_orn, rgen=rdgen_mix, fraction_filled=n_pn_per_kc/n_orn)\n",
    "    simil_before = np.zeros([n_odors, nbacks, nreps])  # J(odor, s'') before\n",
    "    simil_after = np.zeros([n_odors, nbacks, nreps])   # J(odor, s'') after\n",
    "    \n",
    "    # Don't save the backgrounds or their time courses for now.\n",
    "    # Just save the last one for each odorant actually, to check it's indeed the desired process\n",
    "    last_conc_runs = np.zeros((n_odors, steps, n_in_bk+1))\n",
    "    \n",
    "    # For each odor, habituate to a random choice of background odors, nback times\n",
    "    # and run nruns time each background, because the time course is stochastic.\n",
    "    wzero = np.zeros(n_orn)\n",
    "    fc_vec0 = np.asarray([bfluct_rates[1]]*n_in_bk + [bfluct_rates[-2]])\n",
    "    projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=adapt_kc, n_pn_per_kc=n_pn_per_kc, fix_thresh=fix_thresh)\n",
    "    for i in range(n_odors):\n",
    "        odi = all_odors[i]\n",
    "        tag_i_alone = project_neural_tag(odi, wzero, proj_mat, **projtag_kwargs)\n",
    "        other_odors = list(range(n_odors))\n",
    "        other_odors.remove(i)\n",
    "        for j in range(nbacks):\n",
    "            # Select a background randomly\n",
    "            odarray = all_odors[rdgen_mix.choice(other_odors, replace=False, size=n_in_bk).astype(int)]\n",
    "            w_vec0 = np.zeros(n_orn)\n",
    "            back_odor0 = combine_many_odorants(odarray, fc_vec0[:-1])\n",
    "            # Compute tag of odi mixed with that background, prior to habituation\n",
    "            odmix = combine_odorants(back_odor0, odi, 0.8)\n",
    "            tag_mix_before = project_neural_tag(odmix, w_vec0, proj_mat, **projtag_kwargs)\n",
    "            # Make k runs with that background\n",
    "            for k in range(nreps):\n",
    "                w_vec, back_odor, fc_course = habituate_fluct_background(w_vec0, \n",
    "                    back_odor0, fc_vec0, steps, learnrates, bfluct_rates,  \n",
    "                    rdgen_mix, bfluct_args=odarray, bk_update=update_fluct_manybk_conc)\n",
    "                # Compute the tag of odor i after habituation to this background, \n",
    "                # mixed with back_vec\n",
    "                odmix = combine_odorants(back_odor, odi, 0.8)\n",
    "                tag_mix_after = project_neural_tag(odmix, w_vec, proj_mat, **projtag_kwargs)\n",
    "                # Compute the Jaccard of i's tag alone with the mix before and after habituation\n",
    "                simil_before[i, j, k] = jaccard(tag_i_alone, tag_mix_before)\n",
    "                simil_after[i, j, k] = jaccard(tag_i_alone, tag_mix_after)\n",
    "                \n",
    "        # Before moving to next odorant, save latest f time course\n",
    "        last_conc_runs[i] = fc_course\n",
    "    \n",
    "    return simil_before, simil_after, last_conc_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mats_c3 = similarity_habituation_mixback_conc(all_odors=None, n_odors=n_odors, n_in_bk=2, n_orn=50, \n",
    "            n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "            bfluct_rates=(4, 0.5, 0.04, 4, 10, 9), adapt_kc=False, fix_thresh=20, seed=4237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this for different numbers of odors in the background\n",
    "all_sim_mats = [sim_mats]\n",
    "n_odors = 20\n",
    "n_in_back_list = [2, 4, 6, 8, 12, 16]\n",
    "for n in n_in_back_list[1:]:\n",
    "    start_t = perf_counter()\n",
    "    sim_mats_c2 = similarity_habituation_mixback_conc(all_odors=None, n_odors=n_odors, n_in_bk=n, n_orn=50, \n",
    "            n_kc=2000, n_pn_per_kc=6, steps=50, nreps=10, nbacks=10, learnrates=(0.05, 0.01), \n",
    "            bfluct_rates=(4, 0.5, 0.04, 4, 10, 4), adapt_kc=False, fix_thresh=20, seed=4237+n)\n",
    "    end_t = perf_counter()\n",
    "    print(\"Time per run:\", 1000*(end_t - start_t)/sim_mats_c2[0].size, \"ms\")\n",
    "    all_sim_mats.append(sim_mats_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a panel of plots for the different numbers of odorants in the background\n",
    "fig, axes = plt.subplots(2, 3)\n",
    "axes = axes.flatten()\n",
    "fig.set_size_inches(3*3, 2*3)\n",
    "for i in range(len(n_in_back_list)):\n",
    "    samples_before = all_sim_mats[i][0].flatten()\n",
    "    samples_after = all_sim_mats[i][1].flatten()\n",
    "\n",
    "    # Prepare two barplots: one for A vs mix before and after, one for B vs mix before and after\n",
    "    fig, axes[i] = plot_before_after(samples_before, samples_after, figax=[fig, axes[i]])\n",
    "    axes[i].set_title(r\"$n_{back} = \" + r\"{}$\".format(n_in_back_list[i]))\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/habituation_background_complexity.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
