{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to weakly non-Gaussian odor backgrounds\n",
    "Look at a case with three odors. Compare BioPCA and IBCM models for habituation and new odor detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some notes on the setting\n",
    "### Saturation function on IBCM neurons\n",
    "Since the fluctuations are still fast compared to the IBCM neurons, we do not really need saturation functions to prevent numerical divergences (while still using a large time step). Also, the model converges fast enough that we don't need the Law and Cooper, 1994 modification. \n",
    "\n",
    "### Background process\n",
    "We simulate a zero-mean Ornstein-Uhlenbeck process $\\tilde{\\nu}$, then we set the actual odor concentrations to be $\\nu = \\nu_0 + \\tilde{\\nu} + \\epsilon \\tilde{\\nu}^2$. This ensures that the odor concentrations have a third moment of order $\\epsilon$. More precisely, we find that if there are no correlations, we can treat each component $\\nu_{\\gamma}$ as a univariate case, and we then have a third moment of order $\\epsilon$, with only lower-order corrections to the second moment and order $\\epsilon$ corrections to the desired mean $\\nu_0$:\n",
    "\n",
    "$$ \\langle \\nu \\rangle = \\nu_0 + \\epsilon \\sigma^2 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^2 \\rangle = \\sigma^2 + 2 \\epsilon^2 \\sigma^4 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^3 \\rangle = 6 \\epsilon \\sigma^4 + 8 \\epsilon^3 \\sigma^6 $$\n",
    "\n",
    "For multiple odors, $\\tilde{\\nu}$ is a multivariate O-U process as defined in Gardiner's Handbook, and the term $\\epsilon \\tilde{\\nu}^2$ is computed element-wise. But in practice, we only consider independent odors, each can thus be thought of as a scalar O-U process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    fixedpoint_thirdmoment_perturbtheory, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    jacobian_fixedpoint_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues, \n",
    "    lambda_pca_equivalent\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_ifpsp_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor, \n",
    "    compute_optimal_matrices\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    check_conc_samples_powerlaw_exp1,\n",
    "    compute_pca_meankept, \n",
    "    compute_projector_series, \n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_thirdmoment_kinputs, \n",
    "    sample_ss_distrib_thirdmoment, \n",
    "    sample_background_thirdmoment, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    "    tags_list_to_csr_matrix\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)\n",
    "plt.rcParams[\"axes.facecolor\"] = (1,1,1,0)  # transparent background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"optimal\": \"Optimal\", \n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:light green\",\n",
    "    \"optimal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 300  # Half the real number for faster simulations\n",
    "n_components = 3  # Number of background odors\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Simulation duration\n",
    "duration = 320000.0\n",
    "deltat = 1.0\n",
    "skp = 200\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_thirdmoment_kinputs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0xb1942912c9f11e45a71cbde601106048)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "epsilon_nu = 0.2\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu, epsilon_nu]\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical prediction, exact: need moments of nu. \n",
    "variance_nu3 = sigma2 + 2*(epsilon_nu*sigma2)**2\n",
    "mean_nu3 = averages_nu[0] + epsilon_nu*sigma2\n",
    "thirdmoment = 6*epsilon_nu*sigma2**2 + 8*(epsilon_nu*sigma2)**3\n",
    "moments_conc = [mean_nu3, variance_nu3, thirdmoment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 6  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.002  # 0.000001 = 1e-6\n",
    "tau_avg_ibcm = 200\n",
    "coupling_eta_ibcm = 0.5/n_i_ibcm\n",
    "decay_relative_ibcm = 0.005\n",
    "k_c2bar_avg = 1.0  # dummy\n",
    "ssat_ibcm = 100.0 # dummy\n",
    "lambd_ibcm = 0.725\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm\n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"linear\", \n",
    "    \"variant\": \"intrator\", \n",
    "    \"decay\": False\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.2*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions]) * lambd_ibcm\n",
    "\n",
    "# Analytical prediction\n",
    "cs_cn = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1, lambd=lambd_ibcm)\n",
    "cs_cn = cs_cn[:2]\n",
    "c_specif, c_nonspecif = cs_cn\n",
    "lambda_max_pca = lambda_pca_equivalent(cs_cn, moments_conc, n_components, inhib_rates, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back_list, \n",
    "                ibcm_rates, inhib_rates, back_params, duration, \n",
    "                deltat, seed=simul_seed, noisetype=\"normal\",  \n",
    "                skp=skp, **ibcm_options\n",
    ")\n",
    "\n",
    "(tser_ibcm, \n",
    " nuser_ibcm, \n",
    " bkvecser_ibcm, \n",
    " mser_ibcm, \n",
    " cbarser_ibcm, \n",
    " thetaser_ibcm,\n",
    " wser_ibcm, \n",
    " sser_ibcm) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 160000 // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, back_components)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. \n",
    "variance_nu3 = sigma2 + 2*(epsilon_nu*sigma2)**2\n",
    "mean_nu3 = averages_nu[0] + epsilon_nu*sigma2\n",
    "thirdmoment = 6*epsilon_nu*sigma2**2 + 8*(epsilon_nu*sigma2)**3\n",
    "\n",
    "# Compare to numerical values.\n",
    "fullnuser3 = averages_nu.reshape(1, -1) + nuser_ibcm + epsilon_nu*nuser_ibcm**2\n",
    "mean_nu3_sim = np.mean(fullnuser3)  # all odors i.i.d., can average over them. \n",
    "variance_nu3_sim = np.mean((fullnuser3 - mean_nu3_sim)**2)\n",
    "thirdmoment_sim = np.mean((fullnuser3 - mean_nu3_sim)**3)\n",
    "moments_conc = [mean_nu3, variance_nu3, thirdmoment]\n",
    "\n",
    "# Analytical prediction\n",
    "cs_cn = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1, lambd=lambd_ibcm)\n",
    "cs_cn = cs_cn[:2]\n",
    "c_specif, c_nonspecif = cs_cn\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 1.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, back_components, cs_cn, specif_gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cbar2_avg term throughout\n",
    "cbar2_avg_ser = moving_average(cbarser_ibcm*cbarser_ibcm, kernelsize=tau_avg_ibcm)\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm[:-tau_avg_ibcm], cbar2_avg_ser[:-tau_avg_ibcm, i], \n",
    "            color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000)\", ylabel=r\"$\\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=160000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = fullnuser3 - mean_nu3\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "i_colors = sns.color_palette(n_colors=n_dimensions)\n",
    "for j in range(n_i_ibcm):\n",
    "    for i in range(n_dimensions):\n",
    "        axes.flat[j].axhline(analytical_w[i, j], color=i_colors[i], ls=\"--\", \n",
    "                             lw=0.5, alpha=0.7, zorder=-j*n_dimensions-i)\n",
    "    #axes.flat[j].set_ylim([axes.flat[j].get_ylim()[0], np.amax(analytical_w[:, j])*1.1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of average fixed points\n",
    "Check the eigenvalues of the jacobian for one neuron, for every possible specificity. There are $2^{n_B}$ possibilities: choosing specific or not for each odor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_eigenvalues = ibcm_all_largest_eigenvalues(\n",
    "    moments_conc, ibcm_rates, back_components, m3=1.0, cut=1e-16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ibcm_specif_keys = list(all_max_eigenvalues.keys())\n",
    "ibcm_eig_values = np.asarray([all_max_eigenvalues[a] for a in ibcm_specif_keys])\n",
    "reals, imags = np.real(ibcm_eig_values), np.imag(ibcm_eig_values)\n",
    "ibcm_eig_values_specif1 = np.asarray([len(s) == 1 for s in ibcm_specif_keys], dtype=bool)\n",
    "highlights = ibcm_eig_values_specif1\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\")\n",
    "scaleup = 1e3\n",
    "ax.plot(reals[highlights]*scaleup, imags[highlights]*scaleup, marker=\"*\", mfc=\"b\", mec=\"b\", \n",
    "        ls=\"none\", label=\"One odor\", ms=8)\n",
    "ax.plot(reals[~highlights]*scaleup, imags[~highlights]*scaleup, marker=\"o\", mfc=\"k\", mec=\"k\", \n",
    "       ls=\"none\", label=\"0 or 2+ odors\", ms=6)\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.legend(title=\"Specificity\")\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{max}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{max}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 0.00025  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.2\n",
    "lambda_max_pca = lambda_pca_equivalent(cs_cn, moments_conc, n_components, inhib_rates, verbose=True)\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " sser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.statistics import principal_component_analysis\n",
    "from modelfcts.checktools import compute_pca_meankept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, sser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "sser_ideal = bkvecser_ibcm * ideal_factor\n",
    "sser_orthogonal = np.zeros(bkvecser_ibcm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal W manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matrix depends on new odor concentrations, so define them here\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "new_test_concs *= mean_nu3\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Compute optimal W matrix for all new odors possible\n",
    "dummy_rgen = np.random.default_rng(0x13d1faf1db367eed7498000c84dfd345)\n",
    "new_odors_from_distrib = generate_odorant([int(1e5), n_dimensions], dummy_rgen, lambda_in=0.1)\n",
    "new_odors_from_distrib /= l2_norm(new_odors_from_distrib)[:, None]\n",
    "optimal_matrices = compute_optimal_matrices(back_components, new_odors_from_distrib, moments_conc, new_test_concs)\n",
    "\n",
    "# Use the W matrix for the lowest concentration to inhibit the background\n",
    "sser_optimal = bkvecser_ibcm - bkvecser_ibcm.dot(optimal_matrices[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snorm_series = {\n",
    "    \"ibcm\": l2_norm(sser_ibcm), \n",
    "    \"biopca\": l2_norm(sser_pca), \n",
    "    \"avgsub\": l2_norm(sser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(sser_ideal),\n",
    "    \"optimal\": l2_norm(sser_optimal), \n",
    "    \"orthogonal\": l2_norm(sser_orthogonal)\n",
    "}\n",
    "std_options = dict(kernelsize=201, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=201, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(snorm_series[a], **std_options)) for a in snorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(snorm_series[a], **mean_options) for a in snorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "snorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + snorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + snorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background tagging after habituation\n",
    "We create a projection matrix, then compute the tag assigned to the background after inhibition by each habituation model, over time. Hopefully, only IBCM inhibits enough to see tags go to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000 * n_dimensions // 25\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing background tag lengths at various time points for each model\n",
    "sser_dict = {\n",
    "    \"ibcm\": sser_ibcm, \n",
    "    \"biopca\": sser_pca, \n",
    "    \"avgsub\": sser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": sser_ideal, \n",
    "    \"optimal\": sser_optimal, \n",
    "    \"orthogonal\": sser_orthogonal\n",
    "}\n",
    "tag_length_series = {a: np.zeros(tser_ibcm.shape[0]) for a in sser_dict.keys()}\n",
    "for a in sser_dict.keys():\n",
    "    for i in range(0, tag_length_series[a].shape[0]):\n",
    "        if bkvecser_ibcm[i].max() > 0:\n",
    "            tag = project_neural_tag(sser_dict[a][i], bkvecser_ibcm[i], \n",
    "                                 proj_mat, **projection_arguments)\n",
    "        else:\n",
    "            tag = (1,)*int(projection_arguments[\"kc_sparsity\"]*n_kc)\n",
    "        tag_length_series[a][i] = len(tag)\n",
    "tag_length_series_smooth = {a: moving_average(tag_length_series[a], **mean_options)\n",
    "                            for a in tag_length_series}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for model in sser_dict.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    ax.plot(tser_ibcm / 1000, tag_length_series_smooth[model], **props)\n",
    "snorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "ax.set_ylabel(r\"Tag length, $\\mathrm{card}(z)$\")\n",
    "ax.set_xlabel(\"Time (x1000 steps)\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 10\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "# New odor concentrations, defined above\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "# sample_ss_distrib_thirdmoment(means_nu, covmat_nu, epsil, size=1, rgen=None)\n",
    "conc_samples = sample_ss_distrib_thirdmoment(\n",
    "                    averages_nu, steady_covmat, epsil=epsilon_nu, \n",
    "                    size=n_test_times*(n_back_samples-1), rgen=rgen_meta\n",
    "                )  # Shaped [sample, component]\n",
    "back_samples = conc_samples.dot(back_components)\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_yvecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in sser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in sser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in sser_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ideal reduction factor for each concentration\n",
    "ideal_factors = [compute_ideal_factor(c, moments_conc[:2], [n_components, n_dimensions], \n",
    "                    generate_odorant, (dummy_rgen,), reps=200) for c in new_test_concs]\n",
    "# Approximate ideal factors:\n",
    "#ideal_factors = [0.1, 0.18]\n",
    "print(\"Finished computing ideal factors\")\n",
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_yvecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_yvecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_yvecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_yvecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, mixtures, new_test_concs[k], \n",
    "                ideal_factors[k], **avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"optimal\"][i, j, k] = mixtures - mixtures.dot(optimal_matrices[k].T)\n",
    "            mixture_yvecs[\"orthogonal\"][i, j, k] = new_test_concs[k] * x_new_ort\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_yvecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_yvecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_yvecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_yvecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"avgsub\", \"biopca\", \"ibcm\", \"optimal\", \"orthogonal\",  \"ideal\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_non-gaussian_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"avgsub\", \"biopca\", \"ibcm\", \"optimal\", \"orthogonal\"]#,  \"ideal\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_yvecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_non-gaussian_snorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want an unskipped nu time series for the figure, run a dummy simulation for its background\n",
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=1, **avg_options\n",
    ")\n",
    "_, nuser_noskp, _, _, _ = sim_results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results_filename = os.path.join(\"results\", \"for_plots\", \"sample_non-gaussian_simulation.npz\")\n",
    "np.savez_compressed(\n",
    "    results_filename, \n",
    "    tser=tser_ibcm, \n",
    "    nuser=nuser_noskp,   # Can get bkvecser from that and components\n",
    "    mbarser=mbarser,  # Can get cbars_gamma_ser from that and components\n",
    "    cbarser=cbarser_ibcm,\n",
    "    wser=wser_ibcm, \n",
    "    sser=sser_ibcm, \n",
    "    back_vecs=back_components, \n",
    "    cs_cn=cs_cn,\n",
    "    analytical_w=analytical_w,  # Can get analytical sser from that, cs_cn, and specif_gammas\n",
    "    specif_gammas=specif_gammas, \n",
    "    skp=skp,\n",
    "    averages_nu0=averages_nu, \n",
    "    epsilon=epsilon_nu, \n",
    "    ibcm_eig_values=ibcm_eig_values\n",
    ")\n",
    "\n",
    "with open(os.path.join(\"results\", \"for_plots\", \"ibcm_eigenvalues_keys_non-gaussian_example.json\"), \"w\") as f:\n",
    "    json.dump(ibcm_specif_keys, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhib_rates[1] / (inhib_rates[1] + inhib_rates[0]*moments_conc[1]*(cs_cn[1]-cs_cn[0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relative norm of the WLM projection of the parallel component:\", \n",
    "      np.sqrt(np.sum((wser_ibcm[-1].dot(mser_ibcm[-1]).dot(x_new_par))**2) / np.sum(x_new_par**2)))\n",
    "print(\"Relative norm of the WLM projection of the orthogonal component:\", \n",
    "      np.sqrt(np.sum((wser_ibcm[-1].dot(mser_ibcm[-1]).dot(x_new_ort))**2) / np.sum(x_new_ort**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(50), mixture_yvecs[\"ibcm\"][0, 0, 0, 0, :50], width=0.5, label=\"IBCM\")\n",
    "plt.bar(np.arange(50)+0.5, mixture_yvecs[\"orthogonal\"][0, 0, 0, 0, :50], width=0.5, label=\"Orthogonal\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
