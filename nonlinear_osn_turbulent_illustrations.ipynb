{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbulent backgrounds with nonlinear OSN model\n",
    "\n",
    "## Full model\n",
    "We use a simplified version of the model from Kadakia and Emonet, because OSN adaptation is easy to include into it afterwards. \n",
    "\n",
    "The olfactory receptors (OR) have Orco co-receptors. Each OR-Orco complex has an active and an inactive state, with affinities $K^*_{i \\mu}$, $K_{i \\mu}$ respectively for odor $\\mu$, receptor index $i$. Given odor concentrations $c_\\mu$, the quasi-static OSN firing rate in response is\n",
    "\n",
    "$$ A_i = F_\\mathrm{max} \\left[1 + e^{\\epsilon_i(t)} \\left( \\frac{1 + \\sum_\\mu K_{i \\mu} c_\\mu }{1 + \\sum_\\mu K^*_{i \\mu} c_\\mu }  \\right)  \\right]^{-1} $$\n",
    "\n",
    "where $\\epsilon_i(t)$ is the free energy difference between the unbound states in the inactive and active conformations for OR type $i$, in units of $k_\\mathrm{B} T$. This free energy difference changes with feedback from OSN activity, with an adaptation time scale of $250$ ms, but here, we will keep $\\epsilon_i$ fixed; the point of the model is to have a nonlinear receptor activation with easy inclusion of adaptation later. \n",
    "\n",
    "## Simplified form\n",
    "\n",
    "The full activation function can be simplified in the regime where $K c \\ll 1$, which is used by Kadakia and Emonet (in scaled units, they set $K = 0.01$ compared to $K^*$ in the range $10^3-10^4$). Assuming also that $K^* c \\gg 1$, we can neglect the $+1$ and simplify to\n",
    "\n",
    "$$ A_i = F_\\mathrm{max}  \\frac{ \\sum_\\mu K^*_{i \\mu} c_\\mu}{e^{\\epsilon_i} + \\sum_\\mu K^*_{i \\mu} c_\\mu} $$\n",
    "\n",
    "which amounts to a Michaelis-Menten function of the linear combination of odor activations, $\\sum_\\mu K^*_{i \\mu} c_\\mu$, with Michaelis-Menten constant $e^{\\epsilon_i}$ controlling the level of OSN saturation and thus of nonlinearity. Then, odors defined by a single vector of active complex affinities, $\\mathbf{K}^*_\\mu$. \n",
    "\n",
    "\n",
    "## Implementation and affinities distribution\n",
    "\n",
    "The affinities $K^*_{i \\mu}$ and $K_{i \\mu}$ are sampled i.i.d. from a distribution of OSN affinities (inverse EC50s) measured experimentally by Si et al., *Neuron*, 2019. We fit this empirical distribution of affinities with a complementary cumulative distribution function of the form\n",
    "\n",
    "$$ G_X(x) = \\mathbb{P}[X > x] = \\tanh\\left(\\frac{1}{b x^{\\alpha}} \\right) $$\n",
    "\n",
    "which has a power-law tail $G_X \\sim x^{-\\alpha}$ for large $x$, but also a cutoff at low $x$ very similar to the experimental data -- much better, at any rate, than a pure power law. See the fits in the notebook ``si2019_hill_tanh_distribution_fits.ipynb``. \n",
    "\n",
    "Looking at the code from Kadakia and Emonet, \n",
    "https://github.com/elifesciences-publications/ORN-WL-gain-control/blob/master/src/four_state_receptor_CS.py ,\n",
    "the $K$ parameters are all equal to $1/10^2 = 0.01$ exactly. We do the same, since this ensures each odor has its $K^*$ larger than $K$. Note this puts the model in the large $K^*$, small $K$ regime highlighted above, so $K$ could be neglected entirely if we wanted. \n",
    "\n",
    "The free energy default value is $5.0$ in Kadakia and Emonet's code, without variance between odors. \n",
    "\n",
    "## Purpose of this notebook\n",
    "We generate illustrations of the nonlinear manifold and simulation examples of habituation at different levels of nonlinearity. The full simulations to assess performance vs nonlinearity are in the script ``supplementary_scripts/run_performance_nl_osn.py``. We save lines and data for final plotting of the supplementary figure, including:\n",
    " - Examples of the simplified OSN activation curves, for different $K^*$ and $\\epsilon_i$. \n",
    " - A 2D background manifold plotted in 3D, \n",
    " - The Si et al., 2019 CCDF and the best parameters for the tanh fit are saved in ``si2019_hill_tanh_distribution_fits.ipynb``\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "import json\n",
    "from os.path import join as pj\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    compute_mbars_hgammas_hbargammas,\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix\n",
    ")\n",
    "from modelfcts.average_sub import integrate_inhib_average_sub_skip\n",
    "from modelfcts.average_sub import integrate_inhib_average_sub_skip\n",
    "from modelfcts.ideal import compute_optimal_matrix_fromsamples\n",
    "from modelfcts.checktools import analyze_pca_learning\n",
    "from modelfcts.backgrounds import sample_ss_conc_powerlaw\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_pca_results, \n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main new functions: background update with OSN model\n",
    "from modelfcts.distribs import inverse_transform_tanhcdf\n",
    "from modelfcts.nonlin_adapt_osn import (\n",
    "    generate_odor_tanhcdf, \n",
    "    combine_odors_affinities, \n",
    "    update_powerlaw_times_concs_affinities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = \"\"\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"nonlin_adapt\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"nonlin_adapt\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters common to all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for toy and full simulations\n",
    "inhib_rates = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "combine_fct = combine_odors_affinities\n",
    "update_fct = update_powerlaw_times_concs_affinities\n",
    "\n",
    "# Scale of affinity vectors: default\n",
    "kscale = 5e-4  # default is 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_initialization_toy(n_dim, n_comp, rgen, epsil=4.5):\n",
    "    \"\"\" Common template for background initialization \"\"\"\n",
    "    # Seed for background simulation, to make sure all models are the same\n",
    "    simseed = seed_from_gen(rgen)\n",
    "    back_pms = default_background_params(n_comp)\n",
    "\n",
    "    # Background odors: epsils controls the nonlinearity strength, will be adjusted\n",
    "    eps_vec = np.full(n_dim, epsil)\n",
    "    back_comp = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Scaling with the max instead of the mean as in full simulations; doesn't matter, \n",
    "    # it's just for a cartoonish 2D manifold here. \n",
    "    raw_conc = 1.5\n",
    "    raw_ampli = 5.0\n",
    "    np_stat = np.amax  # np.mean, np.median, np.amax\n",
    "    raw_activ = np_stat(combine_fct(np.full(n_comp, raw_conc * avg_whiff), \n",
    "                                        back_comp, eps_vec, fmax=1.0))\n",
    "    osn_ampli = raw_ampli / (raw_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(osn_ampli)\n",
    "    back_pms.append(eps_vec)\n",
    "    back_pms.append(back_comp)\n",
    "\n",
    "    # In the small conc. approx, the odor vectors are (K^* - K)/2\n",
    "    s_vecs = back_comp / l2_norm(back_comp, axis=1)[:, None]\n",
    "\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comp, eps_vec, fmax=osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_list = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_comp, back_pms, init_list, s_vecs, simseed, osn_ampli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal linear manifold learning matrix $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_back(back_odors, new_odors, cser, newconc, fmax, epsils=5.0):\n",
    "    n_new = new_odors.shape[0]\n",
    "    assert n_new == cser.shape[0]  # one new odor per back sample\n",
    "    all_mixvecs = []\n",
    "    for n in range(n_new):\n",
    "        joint_concs = np.concatenate([cser[n], np.full(1, newconc)])\n",
    "        joint_components = np.concatenate(\n",
    "            [back_odors, new_odors[n:n+1]], axis=0)\n",
    "        mixvecs = combine_fct(joint_concs, \n",
    "                    joint_components, epsils, fmax=fmax)\n",
    "        all_mixvecs.append(mixvecs)\n",
    "    mixvecs = np.stack(all_mixvecs, axis=0)\n",
    "    return mixvecs\n",
    "\n",
    "def get_optimal_mat_p(bkvecser, concser, back_pms, new_concs_rel, \n",
    "                      sd=0xe20d4b26b1c7e9b943cd23f4c9d15dca):\n",
    "    \"\"\" Compute the optimal linear manifold learning matrix P, \n",
    "    using a previously simulated background\"\"\"\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_concs = avg_whiff_conc * new_concs_rel\n",
    "    n_new_concs = len(new_concs)\n",
    "    osn_ampli = back_pms[-3]\n",
    "    eps_vec = back_pms[-2]\n",
    "    back_comp = back_pms[-1]\n",
    "\n",
    "    # Compute optimal W matrix for all new odors possible\n",
    "    # Need samples from the background (use provided bkser)\n",
    "    # and samples from mixtures of background + new odor\n",
    "    # (generate from back. conc. series in nuser_ibcm)\n",
    "    dummy_rgen = np.random.default_rng(sd)\n",
    "    # New odors, each with a subset of the background samples\n",
    "    n_samp, n_dims = bkvecser.shape[0], bkvecser.shape[1]\n",
    "    new_odors_from_distrib = generate_odor_tanhcdf(\n",
    "        [n_samp, n_dims], dummy_rgen, unit_scale=kscale)\n",
    "\n",
    "    optimal_matrices = []\n",
    "    for newconc in new_concs:\n",
    "        # Mix new odors at newconc with background\n",
    "        s_new_mix = mix_new_back(back_comp, new_odors_from_distrib, \n",
    "                                 concser, newconc, osn_ampli, epsils=eps_vec)\n",
    "        mat = compute_optimal_matrix_fromsamples(bkvecser, s_new_mix)\n",
    "        optimal_matrices.append(mat)\n",
    "\n",
    "    return optimal_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D background manifold illustration\n",
    "Show also the 2D plane learned by the optimal linear manifold learning matrix $P$. \n",
    "\n",
    "We do not need to habituation IBCM, BioPCA, etc. on this manifold: we can just do an average subtraction simulation, extract the background series, compute the optimal $P$ for that background and random new odors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background initialization is specific to this simulation\n",
    "n_dimensions_toy = 25  \n",
    "n_components_toy = 2  # Number of background odors\n",
    "\n",
    "skp = 20 * int(1.0 / deltat)\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta_toy = np.random.default_rng(seed=0x220369e90599ffa80a743d99ac942f28)\n",
    "\n",
    "res = background_initialization_toy(n_dimensions_toy, n_components_toy, rgen_meta_toy, epsil=4.5)\n",
    "(back_components_toy, back_params_toy, init_bk_list_toy, \n",
    "         s_gamma_vecs_toy, simul_seed_toy, osn_ampli_toy) = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy average subtraction simulation, to get a background sample\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_avg_toy = np.zeros([1, n_dimensions_toy])\n",
    "\n",
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_avg_toy, update_fct, init_bk_list_toy, \n",
    "                [], inhib_rates, back_params_toy, duration, deltat,\n",
    "                seed=simul_seed_toy, noisetype=\"uniform\", skp=skp*2, **avg_options\n",
    ")\n",
    "tser_toy, tcser_toy, bkvecser_toy, _, _ = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal matrix P for this process\n",
    "optim_p = get_optimal_mat_p(bkvecser_toy, tcser_toy[:, :, 1], back_params_toy, np.ones(1), \n",
    "                      sd=0xe20d4b26b1c7e9b943cd23f4c9d15dca)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D manifold in a 3D slice,\n",
    "dims = (0, 2, 4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "where_01 = (tcser_toy[:, :, 1] > 0).astype(bool)\n",
    "locations = {\n",
    "    \"Both odors\": np.all(where_01, axis=1),  #all\n",
    "    \"Odor 0\": (where_01[:, 0] & ~where_01[:, 1]),  # 0\n",
    "    \"Odor 1\": (~where_01[:, 0] & where_01[:, 1])  # 1\n",
    "}\n",
    "all_colors = {\n",
    "    \"Both odors\": \"xkcd:purple\",\n",
    "    \"Odor 0\": \"xkcd:blue\",\n",
    "    \"Odor 1\": \"xkcd:red\",\n",
    "\n",
    "}\n",
    "\n",
    "# Background odors plane\n",
    "vecs = np.zeros(s_gamma_vecs_toy.shape)\n",
    "scale = 2.0\n",
    "orig = np.zeros([3, n_components_toy])\n",
    "for i in range(n_components_toy):\n",
    "    vecs[i] = s_gamma_vecs_toy[i] / np.sqrt(np.sum(s_gamma_vecs_toy[i]**2)) * scale\n",
    "    \n",
    "scale2 = 1.75\n",
    "topvecs = vecs.T\n",
    "s1, s2 = np.meshgrid(np.arange(0, 1, 0.2)*scale2, np.arange(0, 1, 0.2)*scale2)\n",
    "plane = s1[None, :, :] * topvecs[dims, 0:1, None] + s2[None, :, :] * topvecs[dims, 1:2, None]\n",
    "x, y, z = plane[0], plane[1], plane[2]\n",
    "ls = mpl.colors.LightSource(170, 45)\n",
    "rgb = ls.shade(z, cmap=mpl.cm.Greys, vert_exag=0.1, blend_mode='soft')\n",
    "#ax.plot_surface(x, y, z-0.05, color=\"grey\", alpha=0.3, linewidth=0.5, lightsource=ls)\n",
    "ax.plot_surface(x, y, z-0.05, color=\"grey\", alpha=0.3, rstride=1, cstride=1, facecolors=rgb,\n",
    "                       linewidth=0, antialiased=False, shade=True)\n",
    "\n",
    "for lbl in [\"Odor 0\", \"Odor 1\", \"Both odors\"]:\n",
    "    slc = locations[lbl]\n",
    "    ax.scatter(bkvecser_toy[slc, dims[0]], bkvecser_toy[slc, dims[1]], \n",
    "               bkvecser_toy[slc, dims[2]], s=4, lw=0.3, label=lbl, color=all_colors[lbl])\n",
    "\n",
    "ax.quiver(*orig, *(vecs[:, dims].T), color=\"k\", lw=1.5, arrow_length_ratio=0.2)\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=25)\n",
    "\n",
    "# Labeling\n",
    "for lbl, f in enumerate([ax.set_xlabel, ax.set_ylabel, ax.set_zlabel]):\n",
    "    zlbl = f(\"OSN {}\".format(lbl+1), labelpad=-17.5)\n",
    "for f in [ax.set_xticks, ax.set_yticks, ax.set_zticks]:\n",
    "    f([])\n",
    "for f in [ax.set_xticklabels, ax.set_yticklabels, ax.set_zticklabels]:\n",
    "    f([], pad=0.1)\n",
    "\n",
    "ax.view_init(azim=240, elev=35)\n",
    "\n",
    "#leg = ax.legend(loc=\"upper right\", bbox_to_anchor=(0.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Need to adjust the tightbox to remove whitespace above and below manually. \n",
    "#ax.set_aspect(\"equal\")\n",
    "fig.tight_layout()\n",
    "tightbox = fig.get_tightbbox()\n",
    "tightbox._bbox.y0 = tightbox._bbox.y0*1.6\n",
    "tightbox._bbox.y1 = tightbox._bbox.y1 - 0.8*tightbox._bbox.y0\n",
    "tightbox._bbox.x0 = tightbox._bbox.x0 * 0.7\n",
    "\n",
    "figname = \"background_manifold_2d_example.pdf\"\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(\"figures\", \"nonlin_adapt\", figname), \n",
    "                transparent=True, bbox_inches=tightbox, bbox_extra_artists=(zlbl, leg))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples and surface\n",
    "if do_save_outputs:\n",
    "    fname = pj(outputs_folder, \"2d_manifold_nonlinear_osn.npz\")\n",
    "    np.savez_compressed(fname, conc_ser=tcser_toy[:, :, 1], bkvecser=bkvecser_toy[:, dims], \n",
    "                        plane=plane, vecs=s_gamma_vecs_toy[:, dims]*scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full simulation examples at different $\\epsilon$s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global model parameters for full simulations\n",
    "\n",
    "We will run simulations for different $\\epsilon$ values, with adjusted OSN amplitudes, so place most of the background odors generation in a simulation initialization function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly number\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "# Simulation skipping, 50 is enough for plots\n",
    "skp = 50 * int(1.0 / deltat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_back_params(epsil, rgen, n_comp, n_dim):\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms = default_background_params(n_comp)\n",
    "\n",
    "    epsils_vec = np.full(n_dimensions, epsil)\n",
    "    back_comps = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Same adjustment of the OSN amplitude as in the performance recognition tests\n",
    "    raw_conc_factor = 2.5\n",
    "    raw_ampli = 2.5\n",
    "    np_statistic = np.mean  # np.mean, np.median, np.amax\n",
    "\n",
    "    raw_osn_activ = np_statistic(combine_fct(np.full(n_comp, raw_conc_factor * avg_whiff_conc), \n",
    "                                        back_comps, epsils_vec, fmax=1.0))\n",
    "    max_osn_ampli = raw_ampli / (raw_osn_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(max_osn_ampli)\n",
    "    back_pms.append(epsils_vec)\n",
    "    back_pms.append(back_comps)\n",
    "\n",
    "    # Initialization\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps, epsils_vec, fmax=max_osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_pms, init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation simulations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters, same for each tested epsilon\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00075  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.7/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation at a given $\\epsilon$\n",
    "\n",
    "Uses global IBCM parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_epsil(epsil, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing IBCM simulation for epsilon =\", epsil)\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(epsil, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "    \n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen.standard_normal(size=[n_i_ibcm, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params_local, duration, \n",
    "                deltat, seed=simseed, noisetype=\"uniform\",  \n",
    "                skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation for epsilon =\", epsil, \"in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving functions for IBCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_odors_in_manifold(back_pms, conc_ser, new_conc_rel, rgen, n_ex=2, n_samp=10):\n",
    "    \"\"\"Mix n_ex new odors with n_samp background samples each. \n",
    "    Returns a 3d-array of mixtures, indexed [n_ex, n_samp, n_dim], \n",
    "    and the new odor vectors, a 2d array indexed [n_ex, n_dim]. \n",
    "    \"\"\"\n",
    "    back_odors = back_pms[-1]\n",
    "    n_comp, n_dim = back_odors.shape[0], back_odors.shape[1]\n",
    "    max_ampli = back_pms[-3]\n",
    "    print(max_ampli)\n",
    "    new_odors = generate_odor_tanhcdf((n_ex, n_dim), rgen, unit_scale=kscale)\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_conc = avg_whiff_conc * new_conc_rel\n",
    "    non_null_concs = conc_ser[np.any(conc_ser > 0.0, axis=1)]\n",
    "    epsils_vec = np.full(n_dim, back_pms[-2])\n",
    "    back_concs = non_null_concs[rgen.choice(non_null_concs.shape[0], size=n_ex*n_samp, replace=True)]\n",
    "    back_concs = back_concs.reshape(n_ex, n_samp, n_comp)\n",
    "    all_mixed_samples = []\n",
    "    for i in range(n_ex):\n",
    "        joint_kmats = np.concatenate([back_odors, new_odors[i:i+1]], axis=0)\n",
    "        mixed_samples_i = []\n",
    "        for j in range(n_samp):\n",
    "            joint_concs = np.concatenate([back_concs[i, j:j+1], np.full((1, 1), new_conc)], axis=1)\n",
    "            mixed_samples_i.append(combine_fct(joint_concs, joint_kmats, epsils_vec, fmax=max_ampli))\n",
    "        mixed_samples_i = np.concatenate(mixed_samples_i, axis=0)\n",
    "        all_mixed_samples.append(mixed_samples_i)\n",
    "        \n",
    "    return np.stack(all_mixed_samples, axis=0), new_odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_ibcm_simul(results_raw, back_pms, rgenseed, n_ex=2, n_samp=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "    Returns:\n",
    "        cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    back_components = back_pms[-1]\n",
    "    epsil = back_pms[-2]\n",
    "    basis = back_components / l2_norm(back_components, axis=1)[:, None] \n",
    "\n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_hgammas_hbargammas(\n",
    "                                results_raw[3], coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = nuser_ibcm[:, :, 1]\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    ysernorm_ibcm = l2_norm(yser_ibcm, axis=1)\n",
    "    \n",
    "    # Examples of mixing new odors with the background\n",
    "    rgen = np.random.default_rng(np.random.SeedSequence(rgenseed).spawn(2)[1])\n",
    "    mixres = mix_new_odors_in_manifold(back_pms, conc_ser, 1.0, rgen, n_ex=n_ex, n_samp=n_samp)\n",
    "    mixed_samples, new_odors = mixres\n",
    "    results_clean = (cbars_gamma, wser_ibcm, bkvecser_ibcm, ysernorm_ibcm, moments_conc, \n",
    "                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "                     conc_ser, mixed_samples, new_odors)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_ibcm_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save cbar gamma series, background series, ynorm series\n",
    "    # Will run a separate short, non-skipped simulation to plot mixed concentrations\n",
    "    all_saved_series = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        (cbars_gamma, _, bkvecser_ibcm, ysernorm_ibcm, _, _, _, _, \n",
    "             back_comps, conc_ser, mixed_samples, new_odors) = all_results_clean[simname]\n",
    "        fullname = \"cbars_gamma_ser_\" + simname\n",
    "        all_saved_series[fullname] = cbars_gamma\n",
    "        # For habituation and manifold plots, save back series\n",
    "        # and odor components\n",
    "        fullname = \"bkvec_ser_\" + simname\n",
    "        all_saved_series[fullname] = bkvecser_ibcm\n",
    "        fullname = \"back_components_\" + simname\n",
    "        all_saved_series[fullname] = back_comps\n",
    "        fullname = \"conc_ser_\" + simname\n",
    "        all_saved_series[fullname] = conc_ser\n",
    "        # For habituation plots, save norm of y\n",
    "        fullname = \"y_norm_ser_\" + simname\n",
    "        all_saved_series[fullname] = ysernorm_ibcm\n",
    "        # Mixture of new odors with background odors, within the manifold\n",
    "        fullname = \"mixed_new_odors_\" + simname\n",
    "        all_saved_series[fullname] = mixed_samples\n",
    "        fullname = \"new_odors_\" + simname\n",
    "        all_saved_series[fullname] = new_odors\n",
    "    np.savez_compressed(fname, **all_saved_series)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, res_ibcm_clean):\n",
    "    (cbars_gamma, wser_ibcm, bkvecser_ibcm, ysernorm_ibcm, \n",
    "         moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc, \n",
    "         back_comps, conc_ser, _, _) = res_ibcm_clean\n",
    "\n",
    "    # Plot of cbars gamma series\n",
    "    fig , ax, _ = plot_hbars_gamma_series(tser_common, cbars_gamma, \n",
    "                            skp=2, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plots of neuron specificities\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(correl_c_conc.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    print(\"Odor specificities:\", specif_gammas)\n",
    "    split_val = 2.5\n",
    "    for comp in range(n_components):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_ibcm_raw[2], res_ibcm_raw[7], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA initialization and simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation_epsil(epsil, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing BioPCA simulation for epsilon =\", epsil)\n",
    "    # Initialize background parameters, give same rgenseed as IBCM to have same background\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(epsil, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "        \n",
    "    init_synapses_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back, biopca_rates, \n",
    "                inhib_rates, back_params_local, duration, deltat, \n",
    "                seed=simseed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation for epsilon =\", epsil, \"in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and saving functions for BioPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    We do not need to save odor vectors (back_components), \n",
    "    since the IBCM simulation will provide them for both models. \n",
    "    \n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, cbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "            learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, mser_pca, lser_pca, xser_pca, \n",
    "         cbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (bkvecsernorm_pca, ysernorm_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean\n",
    "\n",
    "\n",
    "def save_biopca_simuls_to_disk(fname, **all_results_clean):\n",
    "    # Save true and learnt PCA, that's all we really need\n",
    "    true_learnt_pcas = {}\n",
    "    for simname in all_results_clean.keys():\n",
    "        (bkvecsernorm_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "         learnt_pca, off_diag_l_avg_abs, align_error_ser) = all_results_clean[simname]\n",
    "        fullname = \"true_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = true_pca[0]\n",
    "        fullname = \"learnt_pca_vals_\" + simname\n",
    "        true_learnt_pcas[fullname] = learnt_pca[0]\n",
    "        fullname = \"pca_align_error_\" + simname\n",
    "        true_learnt_pcas[fullname] = align_error_ser\n",
    "        fullname = \"bkvec_norm_ser_\" + simname\n",
    "        true_learnt_pcas[fullname] = bkvecsernorm_pca\n",
    "        fullname = \"y_norm_ser_\" + simname\n",
    "        true_learnt_pcas[fullname] = ysernorm_pca\n",
    "        print(learnt_pca[1].shape)\n",
    "    np.savez_compressed(fname, **true_learnt_pcas)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_biopca_results(res_biopca_raw, res_biopca_clean):\n",
    "    (bkvecsernorm_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "    learnt_pca, off_diag_l_avg_abs, align_error_ser) = res_biopca_clean\n",
    "\n",
    "    # Plot learnt vs true PCA\n",
    "    fig, axes = plot_pca_results(tser_common/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "    axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "    axes[0].get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 2.5*plt.rcParams[\"figure.figsize\"][1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot level of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_biopca_raw[2], res_biopca_raw[8], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM simulations in the near-linear regime ($\\epsilon = 10$)\n",
    "\n",
    "BioPCA usually converges to a decent basis in these conditions, but IBCM is more sensitive to strong outlier odors and needs rates to be well adjusted care. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tser_common = np.arange(0.0, duration, deltat*skp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_low = 10.0\n",
    "# Good seeds for mean, ampli=2.5, conc_factor = 2.5  -- this seems to be a winning combination\n",
    "#seed_low = 0xe3a6a312d14ebc2e72e08619bc830375\n",
    "#simul_seed_low = 0x2ac13b6a33b7a947037ab8c07b9ecb8\n",
    "#seed_low = 0xe3a6a312d14ebc2e72e08619bc8f017e\n",
    "#simul_seed_low = 0x2ac13b6a30b7a947037ab8c37b9edb0\n",
    "#seed_low = 0x3180b82d0efbefd8131def347e3a99e6\n",
    "#simul_seed_low = 0xb4ebbf94d1f271498ccde6a094db41fe\n",
    "#seed_low = 0x3170b82d0efbefd8131def347e3a93e5\n",
    "#simul_seed_low = 0xb4e7bf94e1f271398ccde6a094db44fc\n",
    "#seed_low = 0x3270b82d0efb6fd8131def555e3a7341\n",
    "#simul_seed_low = 0x53e7bfc4e1f84139833de6a094db14fc\n",
    "#seed_low = 0x7170b82d90fb6fd8331def666e3a7343\n",
    "#simul_seed_low = 0x52e7bfc4e1f58395730de6a091db14fe\n",
    "#seed_low = 0x7170b82d905839fffa1def666e3a7343\n",
    "#simul_seed_low = 0x52e7bfc4e1f58395730de6afff855abc\n",
    "\n",
    "# Bad seeds showing sensitivity to odor asymmetry, one odor not covered\n",
    "#seed_low = 0x3170b82d0efbefd8131def347e3a93e3\n",
    "#simul_seed_low = 0xb4e7bf94e1f271398ccde6a094db14fc\n",
    "#seed_low = 0x7170b82d0efb6fd8131def555e3a7344\n",
    "#simul_seed_low = 0x52e7bfc4e1f84139833de6a094db14fe\n",
    "\n",
    "seed_low = 0x7170b82d905839fffa1def666e3a7343\n",
    "simul_seed_low = 0x52e7bfc4e1f58395730de6afff855abc\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_low, res_ibcm_low = run_ibcm_simulation_epsil(epsil_low, n_components,  \n",
    "                         n_dimensions, seed_low, simul_seed_low)\n",
    "res_ibcm_clean_low = analyze_clean_ibcm_simul(res_ibcm_low, back_ibcm_low, seed_low)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_low, res_biopca_low = run_biopca_simulation_epsil(epsil_low, n_components,\n",
    "                            n_dimensions, seed_low, simul_seed_low)\n",
    "res_biopca_clean_low = analyze_clean_biopca_simul(res_biopca_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_low, res_ibcm_clean_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_low, res_biopca_clean_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations at weak nonlinearity (moderate $\\epsilon = 6.0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_moderate = 6.0\n",
    "# Good seeds for mean, ampli=2.5, conc_factor = 2.5\n",
    "#seed_moderate = 0x3e94bc0db982c06f17bf8db6e2a0afd5\n",
    "#simul_seed_moderate = 0x95aeaedb6bca5aabb0e69e132ac9d553\n",
    "#seed_moderate = 0xb05fac69bcde76cabe7e42d06f7a9cad\n",
    "#simul_seed_moderate = 0xe3484614b86cfb179feef21cb9b5e5e2\n",
    "#seed_moderate = 0x5426f8cb35b1ad2c4bfbc0d1f9abdcee\n",
    "#simul_seed_moderate = 0x6cc1cb582e512808964f71f0edf226d\n",
    "#seed_moderate = 0x9996f8cb35b1ad2c3ee4c1d1f994dccc\n",
    "#simul_seed_moderate = 0xfff1cb582e512808964f71f0edf335d\n",
    "# The pair below is good for epsilon = 6.5 and epsilon = 6.0 too\n",
    "#seed_moderate = 0x4b10b81800bf92f1ad2bbca95f6beca8\n",
    "#simul_seed_moderate = 0x9c535fa8057f20f5381d1fb9f485c680\n",
    "\n",
    "# Bad seeds for mean, ampli=2.5, conc_factor = 2.5\n",
    "#seed_moderate = 0xb27a42505b3ee99ea8162c0d4127b11\n",
    "#simul_seed_moderate = 0x68e595fb1df3fb9c75d975a6682b9c78\n",
    "#seed_moderate = 0xb27a42505b3ee99ea8162c0d5839ffb\n",
    "#simul_seed_moderate = 0x68e595fb1df3fb9c75d975a6682b9cad\n",
    "#seed_moderate = 0x5ab03d3362deb1715e30951ebc6fbf86\n",
    "#simul_seed_moderate = 0x94b586389cc1abcafd984e13b85b001b\n",
    "#seed_moderate = 0x5ab03d3362deb1715e30951ebc90ebec\n",
    "#simul_seed_moderate = 0x94b586389cc1bccadd984e13b85b754e\n",
    "#seed_moderate = 0x5426f8cb35b1ad2c4ee4c0d1f994dcee\n",
    "#simul_seed_moderate = 0x6cc1cb582e512808964f71f0edf335f\n",
    "\n",
    "seed_moderate = 0x9996f8cb35b1ad2c3ee4c1d1f994dccc\n",
    "simul_seed_moderate = 0xfff1cb582e512808964f71f0edf335d\n",
    "\n",
    "\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_mod, res_ibcm_mod = run_ibcm_simulation_epsil(epsil_moderate, n_components,  \n",
    "                         n_dimensions, seed_moderate, simul_seed_moderate)\n",
    "res_ibcm_clean_moderate = analyze_clean_ibcm_simul(res_ibcm_mod, back_ibcm_mod, seed_moderate)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_mod, res_biopca_mod = run_biopca_simulation_epsil(epsil_moderate, n_components,\n",
    "                            n_dimensions, seed_moderate, simul_seed_moderate)\n",
    "res_biopca_clean_moderate = analyze_clean_biopca_simul(res_biopca_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_mod, res_ibcm_clean_moderate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_mod, res_biopca_clean_moderate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations at strong nonlinearity (small $\\epsilon = 3.5$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_strong = 3.5\n",
    "seed_strong = 0xb57184f5b6a9d2d998b4fb116f366ef1\n",
    "simul_seed_strong = 0x424f10cb5dce0d8c10b31bd7d818e130\n",
    "\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_strong, res_ibcm_strong = run_ibcm_simulation_epsil(epsil_strong, n_components,  \n",
    "                         n_dimensions, seed_strong, simul_seed_strong)\n",
    "res_ibcm_clean_strong = analyze_clean_ibcm_simul(res_ibcm_strong, back_ibcm_strong, seed_strong)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_strong, res_biopca_strong = run_biopca_simulation_epsil(epsil_strong, n_components,\n",
    "                            n_dimensions, seed_strong, simul_seed_strong)\n",
    "res_biopca_clean_strong = analyze_clean_biopca_simul(res_biopca_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_strong, res_ibcm_clean_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_strong, res_biopca_clean_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case at large $\\epsilon$ (low nonlinearity) where IBCM converges too fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsil_low_fail = 10.0\n",
    "#seed_low_fail = 0x7170b82d0efb6fd8131def555e3a7344\n",
    "#simul_seed_low_fail = 0x52e7bfc4e1f84139833de6a094db14fe\n",
    "seed_low_fail = 0x3170b82d0efbefd8131def347e3a93e3\n",
    "simul_seed_low_fail = 0xb4e7bf94e1f271398ccde6a094db14fc\n",
    "\n",
    "# IBCM\n",
    "back_ibcm_low_fail, res_ibcm_low_fail = run_ibcm_simulation_epsil(epsil_low_fail, n_components,  \n",
    "                         n_dimensions, seed_low_fail, simul_seed_low_fail)\n",
    "res_ibcm_clean_low_fail = analyze_clean_ibcm_simul(res_ibcm_low_fail, back_ibcm_low_fail, seed_low_fail)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca_low_fail, res_biopca_low_fail = run_biopca_simulation_epsil(epsil_low_fail, n_components,\n",
    "                            n_dimensions, seed_low_fail, simul_seed_low_fail)\n",
    "res_biopca_clean_low_fail = analyze_clean_biopca_simul(res_biopca_low_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm_low_fail, res_ibcm_clean_low_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca_low_fail, res_biopca_clean_low_fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation by optimal linear manifold learning matrix $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_back(back_pms, bkser, cser, new_odors, newconc):\n",
    "    back_odors = back_pms[-1]\n",
    "    epsils = back_pms[-2]\n",
    "    max_ampli = back_pms[-3]\n",
    "    n_new = new_odors.shape[0]\n",
    "    assert n_new == cser.shape[0]  # one new odor per back sample\n",
    "    all_mixvecs = []\n",
    "    for n in range(n_new):\n",
    "        joint_concs = np.concatenate([cser[n], np.full(1, newconc)])\n",
    "        joint_components = np.concatenate([back_odors, new_odors[n:n+1]], axis=0)\n",
    "        mixvecs = combine_fct(joint_concs, joint_components, epsils, fmax=max_ampli)\n",
    "        all_mixvecs.append(mixvecs)\n",
    "    mixvecs = np.stack(all_mixvecs, axis=0)\n",
    "    return mixvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_habituation(back_pms, bkser, cser, rgen, newconc_rel):\n",
    "    # Compute optimal W matrix for all new odors possible\n",
    "    # Need samples from the background (use bkvecser_ibcm)\n",
    "    # and samples from mixtures of background + new odor\n",
    "    # (generate from back. conc. series in nuser_ibcm)\n",
    "    # New odors, each with a subset of the background samples\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    newconc = newconc_rel * avg_whiff_conc\n",
    "    n_dim = bkser.shape[1]\n",
    "    newods = generate_odor_tanhcdf([bkser.shape[0], bkser.shape[1]], rgen, unit_scale=kscale)\n",
    "    s_new_mix = mix_new_back(back_pms, bkser, cser, newods, newconc)\n",
    "    optimal_matrix = compute_optimal_matrix_fromsamples(bkser, s_new_mix)\n",
    "\n",
    "    # Use the W matrix for the lowest concentration to inhibit the background\n",
    "    yser_optimal = bkser - bkser.dot(optimal_matrix.T)\n",
    "    return yser_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal y series for each simulation above\n",
    "rgen_optim = np.random.default_rng(0xc6ede9d7cf780703439a669234fcb8fd)\n",
    "y_optim_low = optimal_habituation(back_ibcm_low, res_ibcm_clean_low[2],\n",
    "                         res_ibcm_clean_low[9], rgen_optim, 1.0)\n",
    "y_optim_low_fail = optimal_habituation(back_ibcm_low_fail, res_ibcm_clean_low_fail[2],\n",
    "                         res_ibcm_clean_low_fail[9], rgen_optim, 1.0)\n",
    "y_optim_moderate = optimal_habituation(back_ibcm_mod, res_ibcm_clean_moderate[2], \n",
    "                        res_ibcm_clean_moderate[9], rgen_optim, 1.0)\n",
    "y_optim_strong = optimal_habituation(back_ibcm_strong, res_ibcm_clean_strong[2], \n",
    "                            res_ibcm_clean_strong[9], rgen_optim, 1.0)\n",
    "all_results_y_optimal = {\n",
    "    \"ynorm_ser_\" + str(epsil_low): l2_norm(y_optim_low, axis=1),\n",
    "    \"ynorm_ser_\" + str(epsil_low_fail): l2_norm(y_optim_low_fail, axis=1),\n",
    "    \"ynorm_ser_\" + str(epsil_moderate): l2_norm(y_optim_moderate, axis=1),\n",
    "    \"ynorm_ser_\" + str(epsil_strong): l2_norm(y_optim_strong, axis=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results for final plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_ibcm = pj(outputs_folder, \"saved_ibcm_simulations_nonlin_osn.npz\")\n",
    "fname_biopca = pj(outputs_folder, \"saved_biopca_simulations_nonlin_osn.npz\")\n",
    "fname_optimal = pj(outputs_folder, \"saved_optimal_habituation_nonlin_osn.npz\")\n",
    "# Saving examples where the models work\n",
    "all_results_clean_ibcm = {\n",
    "    str(epsil_strong): res_ibcm_clean_strong, \n",
    "    str(epsil_moderate): res_ibcm_clean_moderate,\n",
    "    str(epsil_low_fail): res_ibcm_clean_low_fail\n",
    "}\n",
    "all_results_clean_biopca = {\n",
    "    str(epsil_strong): res_biopca_clean_strong, \n",
    "    str(epsil_moderate): res_biopca_clean_moderate,\n",
    "    str(epsil_low_fail): res_biopca_clean_low_fail\n",
    "}\n",
    "\n",
    "if do_save_outputs:\n",
    "    save_ibcm_simuls_to_disk(fname_ibcm, **all_results_clean_ibcm)\n",
    "    save_biopca_simuls_to_disk(fname_biopca, **all_results_clean_biopca)\n",
    "    np.savez_compressed(fname_optimal, **all_results_y_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
