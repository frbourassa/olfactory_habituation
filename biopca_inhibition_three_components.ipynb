{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background subspace inhibition with PCA or ICA neurons\n",
    "Comparing different projection learning algorithms for olfactory habituation. In separate notebooks, IBCM is studied. Here, online versions of PCA and ICA are used at the inhibitory layer to suppress the activation of projection neurons (second layer) in response to fluctuating background odor mixtures. \n",
    "\n",
    "Synaptic weights for inhbition, from the inhibitory neurons to the projection layer, are learnt to minimize the squared norm of the projection neuron (PN) layer. In this way, the network of inhibitory neurons is like an autoencoder applying feedforward inhibition. \n",
    "\n",
    "![test](figures/feedforward_inhibitory_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biologically plausible online PCA (biopca)\n",
    "Model proposed in Minden, Pehlevan, and Chklovskii, 2018. \n",
    "I implemented their model and inserted it into my inhibitory network simulations in the module ``modelfcts.biopca``. More precisely, I will be using Algorithm 1, inverse-free Principal Subspace Projection (ifPSP). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lateral inhibition with the PCA neurons\n",
    "Use the PCA-learning neurons as lateral inhibitory neurons just like in the IBCM case. See notes in the relevant Jupyter notebook (copied below). \n",
    "\n",
    "There are weights $\\vec{w}_i$ going from inhibitory neuron $i$ to projection neurons. We can store them in the matrix $W$, where each column is a vector $\\vec{w}_i$. The total inhibition received by projection neurons is therefore $\\sum_{j} \\bar{c}_j \\vec{w}_j = W \\vec{\\bar{c}}$; if these neurons have an element-wise activation function $R$, typically a ReLU function, they take the value\n",
    "\n",
    "$$ \\vec{s} = R\\left(\\vec{x}_{in}(t) - W \\vec{\\bar{c}}  \\right) $$\n",
    "\n",
    "By calculating the gradient of the cost function\n",
    "\n",
    "$$ C(\\vec{w}_j) = \\frac12 \\mathbb{E}\\left[\\vec{s}^T \\vec{s} \\right] + \\frac{\\beta}{2\\alpha} \\mathbb{E}\\left[ \\vec{w}_j^T \\vec{w}_j \\right] $$\n",
    "\n",
    "we find that the inhibition weights leaving neuron $j$ should evolve according to\n",
    "\n",
    "$$ \\frac{d \\vec{w}_j}{dt} = -\\alpha \\nabla_{\\vec{w}_j} C(\\vec{w}_j) = \\alpha \\bar{c}_j \\vec{s} R'\\left(\\vec{s}\\right) -  \\beta \\vec{w}_j $$\n",
    "\n",
    "after assuming that $\\alpha$ is small enough to take instantaneous values but still see $\\vec{w}$ converge on average to the optimum of the cost function. $R'\\left(\\vec{s}\\right)$ is the element-wise derivative of the activation function; for instance, it is $1$ if $R$ is the identity function, or a Heaviside function if $R$ is a ReLU. In those two cases, it can simply be omitted -- in the latter case, the ReLU applied on $\\vec{s}$ itself ensures the term is zero if the difference $\\vec{x} - W\\vec{\\bar{c}}$ is negative. \n",
    "Here, I will use a ReLU, so\n",
    "\n",
    "$$ \\frac{d\\vec{w}_j}{dt} = \\alpha \\bar{c}_j \\vec{s} -  \\beta \\vec{w}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "\n",
    "from modelfcts.biopca import integrate_inhib_ifpsp_network_skip, relu_inplace\n",
    "\n",
    "# Offline PCA for comparison\n",
    "from utils.statistics import principal_component_analysis, seed_from_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input process\n",
    "To have multiple input components fluctuating realistically, we write the input as\n",
    "$$ \\vec{x}(t) = \\sum_{\\alpha=1}^K \\nu_\\alpha \\vec{x}_\\alpha $$\n",
    "where the $\\nu_\\alpha$ are random variables with some correlation, to mimick the turbulent flow that carries all odorants together. Ideally, they would follow the distributions derived in (Celani, Villermaux and Vergassola 2014), but those are a bit tricky to simulate.\n",
    "\n",
    "### General case of the Ornstein-Uhlenbeck process\n",
    "The multivariate Langevin equation for the Ornstein-Uhlenbeck process is:\n",
    "\n",
    "$$ d\\vec{x} = -A \\vec{x}(t) dt + B dW(t) $$\n",
    "\n",
    "where $\\frac{dW}{dt} = \\vec{\\eta}(t)$, a vector of gaussian white noise (independent components), $A$ and $B$ are matrices. Assume the matrix $A$ is normal and can be diagonalized as $A = U D U^\\dagger$, where $D = \\mathrm{diag}(\\lambda^1, ..., \\lambda^n)$. For a deterministic initial condition $\\vec{x}(t_0) = \\vec{x}_0$, the general solution is that $\\vec{x}(t)$ follows a multivariate normal distribution, with mean and variance given by\n",
    "\n",
    "$$ \\langle \\vec{x}(t) \\rangle = U\\mathrm{e}^{-D(t-t_0)}U^{\\dagger} \\vec{x}_0 $$\n",
    "$$ \\langle \\vec{x}(t) \\vec{x}(t)^T \\rangle = U J(t, t_0) U^\\dagger $$\n",
    "\n",
    "where the components of $J$ are \n",
    "\n",
    "$$ J^{ij}(t, t_0) = \\left(\\frac{U^\\dagger B B^T U}{\\lambda^i + \\lambda^j} \\right)^{ij} \\left(1 - e^{-(\\lambda^i + \\lambda^j)(t - t_0)}  \\right) $$\n",
    "\n",
    "\n",
    "The stationary distribution of $\\vec{x}$ is thus\n",
    "\n",
    "$$ \\vec{x}^* \\sim \\mathcal{N} \\left(\\vec{0}, \\left(\\frac{B B^T}{\\lambda^k + \\lambda^l} \\right)^{kl} \\right) \\,\\, .$$\n",
    "\n",
    "For a non-zero mean, simulate the zero-mean process and add the average afterwards, it's simpler. \n",
    "\n",
    "For details, see Gardiner, chapter 4.2.6. \n",
    "\n",
    "### Exact numerical simulation, general case\n",
    "To simulate a realization of this process exactly, we use a trick suggested by Gillespie in the univariate case (which only works for the Ornstein-Uhlenbeck process because it's linear and gaussian). We iteratively take $\\vec{x}(t)$ as the initial condition of the evolution up to $\\vec{x}(t + \\Delta t)$, the distribution of which is\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) \\sim \\mathcal{N}\\left( U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) , U J(t + \\Delta t, t) U^\\dagger \\right) $$\n",
    "\n",
    "which can be rewritten using the following property of multivariate normal distributions: if $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$, then $\\vec{x} = \\vec{\\mu} + \\Psi \\vec{n} \\sim \\mathcal{N}(\\vec{\\mu}, \\Psi \\Psi^T)$ ($\\Psi$ is the Cholesky decomposition of the desired covariance matrix). This property is easily demonstrated by computing $\\langle \\vec{x} \\rangle$ and $\\langle \\vec{x} \\vec{x}^T \\rangle$ and using the linearity of multivariate normal distributions. For our update rule, this gives\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) + \\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right] \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n}$ is a vector of standard normal(0, 1) samples. The matrices $U e^{-D \\Delta t}U^\\dagger$ and $\\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right]$ can be computed only once and applied repeatedly to the $\\vec{x}(t)$ obtained in sequence and the $\\vec{n}$ drawn at each iteration. The Cholesky decomposition of $UJU^\\dagger$ is not obviously expressed in terms of $B$, because the possibly different $\\lambda^i$ values mix up components. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple case and exact simulation of it\n",
    "If $A$ is diagonal, the $U$ matrices are just identity matrices and disappear, but the Cholesky decomposition of $J(t + \\Delta t, t)$ is still not obvious. More explicit expressions can be obtained in the simplifying case where $A$ is proportional to the identity matrix, i.e., all components of $\\vec{x}$ have the same fluctuation time scale. \n",
    "\n",
    "Let's say that $A =  \\frac{1}{\\tau} \\mathbb{1}$, where $\\tau$ is the fluctuation time scale ($\\lambda^i = \\tau \\,\\, \\forall i$). Then, the matrix $J$ simplifies to \n",
    "\n",
    "$$J(t, t_0) = \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  $$\n",
    "\n",
    "and its Cholesky decomposition is simply $\\sqrt{\\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right) } B$. Hence, the distribution of $\\vec{x}(t)$ at any time since $t_0$ (deterministic initial condition $\\vec{x}_0$) is\n",
    "\n",
    "$$ \\vec{x}(t) \\sim \\mathcal{N} \\left(e^{-(t-t_0)/\\tau} \\vec{x}_0, \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  \\right) $$\n",
    "\n",
    "The stationary distribution is simply the above with the exponential factors set to 0. The update rule from $\\vec{x}(t)$ to $\\vec{x}(t + \\Delta t)$ to simulate a realization of the process is nicer as well:\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = e^{-\\Delta t / \\tau} \\vec{x}(t) + \\sqrt{\\frac{\\tau}{2} \\left(1 - e^{-2\\Delta t/\\tau}  \\right)} B \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$ is a vector of independent standard normal samples.\n",
    "\n",
    "As before, we can compute once the (scalar) factor $e^{-\\Delta t / \\tau}$ and the . This is exact for any $\\Delta t$, there is no increase in accuracy by decreasing $\\Delta t$. You just choose the $\\Delta t$ resolution at which you want to sample the realization of the process. \n",
    "\n",
    "### Symmetric choices for correlations\n",
    "We want all pairs of $\\nu_\\gamma$ to have the same correlation. More specifically, we want to force a Pearson correlation coefficient of $0 < \\rho < 1$ between any pair of $\\nu$s. We suppose all background components have the same individual variance $\\sigma^2$. The corresponding covariance matrix we want for the steady-state distribution is\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & \\rho & \\ldots & \\rho \\\\\n",
    "    \\rho & 1 & \\ldots & \\rho \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    \\rho & \\rho & \\ldots & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we apply Cholesky decomposition to get $\\Sigma = \\Psi \\Psi^T$, then $\\sqrt{\\tau/2} B = \\Psi$, since the steady-state covariance of the Ornstein-Uhlenbeck process is, in this simplified case, $\\frac{\\tau}{2} BB^T$. The $M_B$ coefficient in the update rule is then\n",
    "\n",
    "$$ M_B = \\sqrt{\\tau/2(1 - e^{-2 \\Delta t/\\tau})}B = \\sqrt{(1 - e^{-2 \\Delta t/\\tau})} \\Psi $$\n",
    "\n",
    "The other coefficient is just\n",
    "\n",
    "$$ M_A = e^{-\\Delta t / \\tau} \\mathbb{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to update the fluctuating background variable\n",
    "from modelfcts.backgrounds import update_ou_kinputs, update_ou_2inputs, decompose_nonorthogonal_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simulation with gaussian background\n",
    "Select parameters, background components below to integrate the PCA and inhibitory neuron equations while the background fluctuates. One simulation runs in ~10 s on my laptop (not very efficient Python code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 4\n",
    "n_components = 3\n",
    "n_neurons = 3\n",
    "\n",
    "# Simulation time scales\n",
    "duration = 50000.0\n",
    "deltat = 1.0\n",
    "tau_nu = 2.0  # Correlation time scale of the background nu_gammas (same for all)\n",
    "learnrate = 0.001  # Learning rate of M\n",
    "rel_lrate = 2.0  # Learning rate of L, relative to learnrate\n",
    "lambda_range = 0.5\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "lambda_mat_diag = np.asarray([1.0 - lambda_range*k / (n_neurons - 1) for k in range(n_neurons)])\n",
    "biopca_rates = [learnrate, rel_lrate, lambda_range]\n",
    "\n",
    "inhib_rates = [25e-5, 5e-5]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: as advised in Minden et al., 2018 \n",
    "rgen_meta = np.random.default_rng(seed=0x8496f883e85163519eb26fb84733ebad)\n",
    "init_mmat = rgen_meta.standard_normal(size=[n_neurons, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat = np.eye(n_neurons, n_neurons)  # Supposed to be near-identity, start as identity\n",
    "ml_inits = [init_mmat, init_lmat]\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant: [0.8, 0.1, 0.1], [0.1, 0.8, 0.1], etc.\n",
    "back_components = 0.2*np.ones([n_components, n_dimensions])\n",
    "# Symmetric background components are an issue for PCA, it causes degeneracy and fluctuations\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8 - 0.0*i\n",
    "    else:  # If there are more components than there are dimensions (ORNs)\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "# Initial background params, ordered with nu first for the update_ou_kinputs function\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0  # Set to zero for comparison with analytical prediction\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_init, update_bk, bk_init, ibcm_params, inhib_params, bk_params, tmax, dt, seed=None, noisetype=\"normal\"\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(ml_inits, update_ou_kinputs, \n",
    "                        init_back_list, biopca_rates, inhib_rates, back_params, duration, \n",
    "                        deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser, nuser, bkvecser, mser, lser, _, cbarser, wser, sser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the solution\n",
    "\n",
    "### TODO: upgrade plots\n",
    "We should look at $\\vec{m}$ (the PCA neurons's input weights, to be compared to the analytical prediction), $\\vec{w}$ (the inhibitory neurons), and $\\vec{s}$, the olfactory background after inhibition, which we expect to have less variance than the original background $\\vec{x}$.\n",
    "\n",
    "Also look at $\\vec{c}$, this one should look like PCA projections. And look at the combined matrix $L^{-1}M^T$, which is supposed to be like a projection matrix on principal components. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import (plot_3d_series, plot_w_matrix, plot_m_matrix, plot_pca_results,\n",
    "                            plot_background_norm_inhibition, plot_background_neurons_inhibition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check PCA learning\n",
    "Let $X$ be a matrix with each input sample in a column. According to Lemma 3 from Minden et al., 2018:\n",
    " - $L$ is diagonal with the $K$ first principal values, that is, the $K$ first eigenvalues of $XX^T$, on its diagonal\n",
    " - $\\hat{U}_K = \\Lambda^{-1} L^{-1} M$ is the learnt projector on the $K$ subspace: rows of $\\hat{U}_K$ are the first $K$ principal components. \n",
    " - Note that in the ifPSP algorithm, the Taylor series approximation $L^{-1} = L_d^{-1} - L_d^{-1} L_o L_d^{-1}$ is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca_meankept(samp, do_proj=False, vari_thresh=1.0, force_svd=False):\n",
    "    \"\"\" Given an array of samples, compute the empirical covariance and\n",
    "    diagonalize it to obtain the principal components and principal values,\n",
    "    which are what is returned.\n",
    "\n",
    "    If less than 10*d samples, take SVD of the sample matrix directly\n",
    "    divided by 1/sqrt(N-1), because this amounts to eigendecomposition of\n",
    "    the covariance matrix, but with better numerical stability and accuracy\n",
    "    (but it's a lot slower).\n",
    "\n",
    "    Args:\n",
    "        samp (np.array): nxp matrix for n samples of p dimensions each.\n",
    "            Pass the values of a dataframe for proper slicing.\n",
    "        do_proj (bool): if True, also project the sample points\n",
    "        vari_thresh (float in [0., 1.]): include principal components until\n",
    "            a fraction vari_thresh of the total variance is explained.\n",
    "        force_svd (bool): if True, use SVD of the data matrix directly.\n",
    "    Returns:\n",
    "        p_values (np.ndarray): 1d array of principal values, descending order.\n",
    "        p_components (np.ndarray): 2d array of principal components.\n",
    "            p_components[:, i] is the vector for p_values[i]\n",
    "        samp_proj (np.ndarray): of shape (samp.shape[0], n_comp) where n_comp\n",
    "            is the number of principal components needed to explain\n",
    "            vari_thresh of the total variance.\n",
    "    \"\"\"\n",
    "    # Few samples: use SVD on the de-meaned data directly.\n",
    "    if force_svd or samp.shape[0] <= 10*samp.shape[1]:\n",
    "        svd_res = np.linalg.svd(samp.T / np.sqrt(samp.shape[0] - 1))\n",
    "        # U, Sigma, V. Better use transpose so small first dimension,\n",
    "        # because higher accuracy in eigenvectors in U\n",
    "        # Each column of U is an eigenvector of samp^T*samp/(N-1)\n",
    "        p_components = svd_res[0]\n",
    "        p_values = svd_res[1]**2  # Singular values are sqrt of eigenvalues\n",
    "\n",
    "    # Many samples are available; use covariance then eigen decomposition\n",
    "    else:\n",
    "        covmat = np.dot(samp.T, samp) / (samp.shape[0] - 1)\n",
    "        p_values, p_components = np.linalg.eigh(covmat)\n",
    "        # Sort in decreasing order; eigh returns increasing order\n",
    "        p_components = p_components[:, ::-1]\n",
    "        p_values = p_values[::-1]\n",
    "\n",
    "    if do_proj:\n",
    "        vari_explained = 0.\n",
    "        total_variance = np.sum(p_values)\n",
    "        n_comp = 0\n",
    "        while vari_explained < total_variance*vari_thresh:\n",
    "            vari_explained += p_values[n_comp]\n",
    "            n_comp += 1\n",
    "            if n_comp > p_values.shape[0]: break\n",
    "        samp_proj = samp.dot(p_components[:, :n_comp])\n",
    "\n",
    "    else:\n",
    "        samp_proj = None\n",
    "\n",
    "    return p_values, p_components, samp_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mat):\n",
    "    \"\"\" Compute Frobenius norm of matrix A, \n",
    "    ||A||^2 = Tr(A^T A). \"\"\"\n",
    "    return np.trace(mat.T.dot(mat))\n",
    "\n",
    "def subspace_align_error(mat, target):\n",
    "    \"\"\" Compute min_Q ||Q.dot(mat) - target||^2 / ||target||^2. \n",
    "    The solution to that orthogonal Procrustes problem is \n",
    "        Q = U V^T, where USV^T is the SVD of target.dot(mat.T)\n",
    "    according to Wikipedia, citing the solution of Schönemann, 1966. \n",
    "    (https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)\n",
    "    \"\"\"\n",
    "    # Solve Procrustes problem\n",
    "    u, s, vh = np.linalg.svd(target.dot(mat.T))\n",
    "    q = u.dot(vh)\n",
    "    # Compute alignment error\n",
    "    return frobnorm(q.dot(mat) - target) / frobnorm(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projector_series(mser, lser):\n",
    "    nt = mser.shape[0]\n",
    "    nk = lser.shape[1]\n",
    "    linvdiag = 1.0 / np.diagonal(lser, axis1=1, axis2=2)\n",
    "    loffd = lser.copy()\n",
    "    loffd[:, np.arange(nk), np.arange(nk)] = 0.0\n",
    "    \n",
    "    linvser = linvdiag[:, :, None] * (np.tile(np.eye(nk), (nt, 1, 1)) - loffd * linvdiag[:, None, :])\n",
    "    fser = np.einsum(\"...ij,...jk\", linvser, mser)  # ... allows broadcasting\n",
    "    return fser\n",
    "    \n",
    "def analyze_pca_learning(xser, mser, lser, lambda_diag):\n",
    "    # Exact PCA: eigenvalue decomposition of xx^T / (n_samples-1)\n",
    "    nk = lser.shape[1]\n",
    "    nt = xser.shape[0]\n",
    "    eigvals, eigvecs, _ = compute_pca_meankept(xser, do_proj=False)\n",
    "    \n",
    "    # Determine basis learnt by algorithm and return\n",
    "    fser = compute_projector_series(mser, lser)\n",
    "    learntvecs = ((1.0/lambda_diag[None, :, None]) * fser)\n",
    "    # Each row of learntvecs[t] is an eigenvector learnt at time t\n",
    "    \n",
    "    # Values on the diagonal of L are supposed to be eigenvalues\n",
    "    learntvals = np.diagonal(lser, axis1=1, axis2=2)\n",
    "    # Sort them in decreasing order\n",
    "    sort_arg = np.argsort(np.mean(learntvals[nt//2:], axis=0))[::-1]\n",
    "    learntvals = learntvals[:, sort_arg]\n",
    "    \n",
    "    # Off-diagonal values are supposed to tend to zero\n",
    "    loffd = lser.copy()\n",
    "    loffd[:, np.arange(nk), np.arange(nk)] = 0.0\n",
    "    offd_avg_abs = np.mean(np.abs(loffd), axis=(1, 2))\n",
    "    \n",
    "    # Subspace alignment\n",
    "    learnt_pca_ser = [learntvecs[i].T for i in range(nt)]\n",
    "    error_series = np.asarray([subspace_align_error(eigvecs[:, :nk], v) for v in learnt_pca_ser])\n",
    "    \n",
    "    return [eigvals, eigvecs], [learntvals, learntvecs], fser, offd_avg_abs, error_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser, mser, lser, lambda_mat_diag)\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fluctuations of the projector\n",
    "fig, axes = plt.subplots(n_neurons)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_neurons):\n",
    "    for j in range(n_dimensions):\n",
    "        #axes[i].plot(tser, learnt_pca[1][:, i, j])\n",
    "        axes[i].plot(tser, fser[:, i, j])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA neurons\n",
    "Dynamics, comparison to analytical fixed points, and plot of the time course of some selected components of $\\vec{m}$ of a few vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_series(mser, dim_idx=[0, 1, 2], transient=1000, skp=100)\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([3, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 0.3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=120, elev=30)\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes_mat = plot_m_matrix(tser, mser, skp=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The L matrix should be nearly diagonal, it does not seem to be the case here. \n",
    "fig, axes_mat = plot_m_matrix(tser, lser, skp=100)\n",
    "for i in range(axes_mat.shape[0]):\n",
    "    axes_mat[i, 0].set_ylabel(r\"$L^{ij}$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot of the original and inhibited odors, sampled sparsely in time\n",
    "# NB: due to our choice of components, there is symmetry and degeneracy \n",
    "# in the two lower eigenvalues, so vectors may not be the same, up to\n",
    "# a rotation around the largest, unique PC. \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Raw background\n",
    "transient = 30000\n",
    "skp = 100\n",
    "tslice = slice(transient, None, skp)\n",
    "dimslice = [0, 1, 2]\n",
    "ax.scatter(bkvecser[tslice, 0], bkvecser[tslice, 1], bkvecser[tslice, 2], color=\"r\", label=\"Background\")\n",
    "ax.scatter(sser[tslice, 0], sser[tslice, 1], sser[tslice, 2], \n",
    "           color=\"b\", label=\"PCA inhibition\")\n",
    "# Compare to inhibition of the mean\n",
    "mean_inhibition = bkvecser - np.mean(bkvecser[transient:], axis=0)*inhib_rates[0]/sum(inhib_rates)\n",
    "ax.scatter(mean_inhibition[tslice, 0], mean_inhibition[tslice, 1], mean_inhibition[tslice, 2], \n",
    "           color=\"xkcd:light blue\", label=\"Average subtraction\")\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=200, alpha=1)\n",
    "\n",
    "# Each row is the mean bkvec\n",
    "orig = np.zeros([3, 3])\n",
    "orig = np.zeros([3, n_components]) + np.mean(bkvecser[transient:, dimslice, None], axis=0) \n",
    "ax.scatter(*orig, c=\"k\", s=9, zorder=1)\n",
    "# Need to unravel dimensions, which is the last axis of stacks of vectors\n",
    "# However in true_pca[1], each column is an eigenvector: p_components[:, i] is the vector for p_values[i]\n",
    "# so there we don't need to transpose. \n",
    "ax.quiver(*orig, *(true_pca[1][dimslice, :n_components]), \n",
    "          color=\"k\", lw=2.0, zorder=100, label=\"True PCs\")\n",
    "print(fser.shape)\n",
    "# In fser, each row is an eigenvector, each column a dimension\n",
    "vecs = np.mean(fser[transient:, :, dimslice], axis=0)  # Each row is a vector\n",
    "#vecs = vecs / np.sqrt(np.sum(vecs**2, axis=1, keepdims=True))\n",
    "# Remove scale; rows of F have norm F.F^T = Lambda^2\n",
    "vecs = 1.0 / lambda_mat_diag[:, None] * vecs\n",
    "ax.quiver(*orig, *(vecs[:n_components].T), color=\"b\", lw=2.0, zorder=101, label=\"Learnt PCs\")\n",
    "ax.view_init(azim=140, elev=30)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.85))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot products of learnt vectors with themselves. \n",
    "print(np.dot(vecs, vecs.T))  # Should be orthogonal or nearly. \n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the inhibitory neurons' weights $\\vec{w}_i$\n",
    "Analytically, I find that, on average, $\\vec{w}_i$ converges to $\\vec{x}(\\pm \\sigma)$, i.e. to either input vector one standard deviation away from the mean input. So, here, I compare the numerical results for $\\vec{w}$ to the possible fixed points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat = plot_w_matrix(tser, wser, skp=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background before and after inhibition\n",
    "\n",
    "### Analytical prediction of $\\vec{w}$ for gaussian $\\vec{x}$\n",
    "\n",
    "TODO\n",
    "\n",
    "### Analytical prediction of $\\vec{s}$ for gaussian $\\vec{x}$ and many neurons\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(tser, bkvecser, sser, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 30000\n",
    "avg_bknorm = np.mean(bknorm_ser[transient:])\n",
    "avg_snorm = np.mean(snorm_ser[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser[transient:])\n",
    "std_snorm = np.std(snorm_ser[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser, bkvecser, sser, skp=10)\n",
    "axes[-1].legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.6), fontsize=8, handlelength=1.5)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 30000\n",
    "avg_bknorm = np.mean(bkvecser[transient:])\n",
    "avg_snorm = np.mean(sser[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bkvecser[transient:])\n",
    "std_snorm = np.std(sser[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity of a projection neuron reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of a projection neuron's activity reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard deviations\n",
    "# TODO: determine analytical expression\n",
    "stdev_inhib_comps = np.std(sser[transient:], axis=0)\n",
    "stdev_bk_comps = np.std(bkvecser[transient:], axis=0)\n",
    "print(\"Standard deviation of components, no inhibition:\", stdev_bk_comps)\n",
    "print(\"Standard deviation of components, after inhibition:\", stdev_inhib_comps)\n",
    "print(\"The noise is reduced on average to\", (stdev_inhib_comps[:3]/stdev_bk_comps[:3]).mean()*100, \"% of original\")\n",
    "#print(\"Theoretical prediction: not determined yet, intuitively 1-a/(a+b) =\", 100*(1 - inhib_rates[0]/sum(inhib_rates)), \"%\")\n",
    "print()\n",
    "\n",
    "# Averages\n",
    "avg_inhib_comps = np.mean(sser[transient:], axis=0)\n",
    "avg_bk_comps = averages_nu.dot(back_components)\n",
    "print(\"Average background after inhibition:\", avg_inhib_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response to a new odor\n",
    "This part of the code only runs if the simulation above had ``n_dimensions > n_components``. \n",
    "\n",
    "The goal is to see whether a new odor, not linearly dependent of the ones in the background, also gets repressed close to zero, or produces an inhibited output noticeably different from the inhibited background, and still similar to the new odor vector, at least its component perpendicular to the background subspace.\n",
    "\n",
    "I should test this property more carefully, over a statistical ensemble of new odors, once I have a more realistic model of odors themselves. \n",
    "\n",
    "If the background, at the moment where the new odor is presented, is significantly different from the average background, the simple inhibition by average background subtraction will not work: the subtracted background (the average) will be very different from the current background, and the new odor will not be isolated. The IBCM model should work better for this by being able to subtract components of the background separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_new_odor_ifpsp(odor, typical_f, typical_w):\n",
    "    # Compute activation of neurons with this new odor (new+background)\n",
    "    # Given the ifPSP projector's current state \n",
    "    # (either latest or some average state of the neurons)\n",
    "    cbar = typical_f.dot(odor)\n",
    "\n",
    "    # New odor after inhibition by the network, ReLU activation on s\n",
    "    # Inhibit with the mean cbar*wser, to see how on average the new odor will show\n",
    "    new_output = relu_inplace(odor - typical_w.dot(cbar))  # s = x - Wc\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_odor = np.roll(back_components[0], shift=-1)  # Should be a new vector\n",
    "full_basis = np.hstack([back_components.T, new_odor.reshape(-1, 1)])\n",
    "\n",
    "# Mix new odor with background\n",
    "# new_odor = 0.5*new_odor + 0.5*np.sum(back_components, axis=0) / n_components  # Combine with mean background\n",
    "new_odor = 0.5*new_odor + 0.5*back_components[1]  # Combine with one component\n",
    "# Inhibit with average m synapses and w\n",
    "new_odor_after_inhibition_average = respond_new_odor_ifpsp(new_odor, np.mean(fser[transient:], axis=0), \n",
    "                                                     np.mean(wser[transient:], axis=0))\n",
    "# Inhibit with latest m and w\n",
    "new_odor_after_inhibition_latest = respond_new_odor_ifpsp(new_odor, mser[-1], wser[-1])\n",
    "\n",
    "# Response to new odor after inhibition by removal of average background\n",
    "# s = x - alpha/(alpha+beta)*mean_background\n",
    "new_odor_after_average_subtract = new_odor - (inhib_rates[0]/(sum(inhib_rates))\n",
    "                                              * np.mean(bkvecser[transient:], axis=0))\n",
    "\n",
    "# Show components along the three background vectors plus the new odor vector (full_basis)\n",
    "print(\"Decomposition on the basis of x_gamma\")\n",
    "print(\"Unhinibited new odor:\", decompose_nonorthogonal_basis(new_odor, full_basis))\n",
    "print(\"Average after ifPSP inhibition:\", decompose_nonorthogonal_basis(\n",
    "                        new_odor_after_inhibition_average, full_basis))\n",
    "print(\"Inhibition by average subtraction:\", decompose_nonorthogonal_basis(\n",
    "                        new_odor_after_average_subtract, full_basis))\n",
    "\n",
    "# We indeed detect the new odor in the plane perpendicular to the inhibition. \n",
    "# Hence the more components of the new odor are not spanned by the old odors, the more\n",
    "# we can detect it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhibition of an alternating background\n",
    "\n",
    "This works very well with the IBCM model. What about PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alternating_inputs(idx_bk, params_bk, noises, dt):\n",
    "    \"\"\" Select randomly the next background input. \n",
    "    Args:\n",
    "        nu_bk (np.ndarray): array of length k-1, containing proportions nu_i of odorants\n",
    "        params_bk (list):  Contains the following parameters       \n",
    "            cumul_probs (np.ndarray): cumulative probabilities up to the kth input vector. \n",
    "            vecs (np.ndarray): 2d array where each row is one of the possible input vectors\n",
    "        noises (np.1darray): pre-generated uniform(0, 1) samples, in an array of length 1, \n",
    "            to choose next input vector. \n",
    "        \"\"\"\n",
    "    # Index of the next input\n",
    "    cumul_probs, vecs = params_bk\n",
    "    idx = np.argmax(cumul_probs > noises[0])\n",
    "    return vecs[idx], np.asarray([idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]\n",
    "\n",
    "sim_res = integrate_inhib_ifpsp_network_skip(ml_inits, update_alternating_inputs, \n",
    "                        init_back_altern, biopca_rates, inhib_rates, back_params_altern, duration, \n",
    "                        deltat, seed=seed_from_gen(rgen_meta), noisetype=\"uniform\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser_alt, nuser_alt, bkvecser_alt, mser_alt, lser_alt, cbarser_alt, wser_alt, sser_alt = sim_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time evolution of the learnt PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_alt, mser_alt, lser_alt, lambda_mat_diag)\n",
    "true_pca_alt, learnt_pca_alt, fser_alt, off_diag_l_avg_abs_alt, align_error_ser_alt = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_alt, true_pca_alt, learnt_pca_alt, align_error_ser_alt, off_diag_l_avg_abs_alt)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a sample of points for each neuron\n",
    "transient = 30000\n",
    "skp = 500\n",
    "tslice = slice(transient, None, skp)\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(mser_alt[tslice, i, 0], mser_alt[tslice, i, 1], mser_alt[tslice, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 0.3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "# ax.view_init(azim=0, elev=30)\n",
    "ax.view_init(azim=30, elev=30)\n",
    "# ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background components after inhibition\n",
    "And their standard deviation: it is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser_alt, bkvecser_alt, sser_alt, skp=10)\n",
    "axes[-1].legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.6), fontsize=8, handlelength=1.5)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 20000\n",
    "avg_bknorm = np.mean(bkvecser_alt[transient:])\n",
    "avg_snorm = np.mean(sser_alt[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bkvecser_alt[transient:])\n",
    "std_snorm = np.std(sser_alt[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity of a projection neuron reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of a projection neuron's activity reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-gaussian unimodal distribution\n",
    "If there is a discrete number of fixed points when the $\\nu_{\\alpha}$ have a distribution with non-zero third moment, there should be a transition from a continuum of fixed points to this discrete case as we increase a parameter $\\epsilon = \\langle (\\nu - \\langle\\nu\\rangle)^3 \\rangle$ above zero. \n",
    "\n",
    "To interpolate with a parameter $\\epsilon$ from a gaussian distribution to a distribution with non-zero central third moment, one trick is to simulate $x$ as an Ornstein-Uhlenbeck process with zero mean, then take\n",
    "$$ \\nu = s + x + \\epsilon x^2 $$\n",
    "or, in the multivariate case, \n",
    "$$ \\vec{\\nu} = \\vec{s} + \\vec{x} + \\epsilon \\mathrm{diag}(\\vec{x}) \\vec{x} $$\n",
    "\n",
    "If there are no correlations, we can treat each component $\\nu_{\\alpha}$ as a univariate case, and we then have a third moment of order $\\epsilon$, with only lower-order corrections to the second moment and order $\\epsilon$ corrections to the desired mean $s$:\n",
    "\n",
    "$$ \\langle \\nu \\rangle = s + \\epsilon \\sigma^2 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^2 \\rangle = \\sigma^2 + 2 \\epsilon^2 \\sigma^4 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^3 \\rangle = 6 \\epsilon \\sigma^4 + 8 \\epsilon^3 \\sigma^6 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import update_thirdmoment_kinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset some simulations parameters, others stay as before\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=0xeefeb6e5f101c07cf9e80e95d5e8ecfd)\n",
    "init_mmat = rgen_meta.standard_normal(size=[n_neurons, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat = np.eye(n_neurons, n_neurons)  # Supposed to be near-identity, start as identity\n",
    "ml_inits3 = [init_mmat, init_lmat]\n",
    "\n",
    "# biopca parameters\n",
    "learnrate3 = 0.003  # Learning rate of M\n",
    "rel_lrate3 = 2.0  # Learning rate of L, relative to learnrate\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "lambda_range3 = 0.5\n",
    "lambda_mat_diag3 = np.asarray([1.0 - lambda_range3*k / (n_neurons - 1) for k in range(n_neurons)])\n",
    "biopca_rates3 = [learnrate3, rel_lrate3, lambda_range3]\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2  # Fluctuation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "epsilon_nu = 0.2\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params_3 = [update_mat_A, update_mat_B, back_components, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_init, update_bk, bk_init, ibcm_params, inhib_params, bk_params, tmax, dt, seed=None, noisetype=\"normal\"\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(ml_inits3, update_thirdmoment_kinputs, \n",
    "                        init_back_list, biopca_rates3, inhib_rates, back_params_3, duration, \n",
    "                        deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser3, nuser3, bkvecser3, mser3, lser3, cbarser3, wser3, sser3 = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check PCA learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser3, mser3, lser3, lambda_mat_diag3)\n",
    "true_pca3, learnt_pca3, fser3, off_diag_l_avg_abs3, align_error_ser3 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser3, true_pca3, learnt_pca3, align_error_ser3, off_diag_l_avg_abs3)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time evolution of IBCM neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# Plot a sample of points for each neuron\n",
    "transient = 20000\n",
    "skp = 100\n",
    "tslice = slice(transient, None, skp)\n",
    "ax.plot(0, 0, 0, color=\"k\", marker=\"o\", ls=\"none\", ms=12)\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_neurons)\n",
    "for i in range(n_neurons):\n",
    "    ax.scatter(mser3[tslice, i, 0], mser3[tslice, i, 1], mser3[tslice, i, 2], \n",
    "               alpha=0.5, color=colors[i], label=\"Neuron {}\".format(i))\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "ax.plot(orig[0, 0], orig[0, 1], orig[0, 2], mfc=\"k\", marker=\"o\", mec=\"k\", ls=\"none\")\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 0.3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "#ax.view_init(azim=45, elev=30)\n",
    "# ax.view_init(azim=45, elev=140)\n",
    "# ax.legend()\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "#fig.savefig(\"figures/three_odors/neurones_ifpsp_3e_moment_epsilon_2e-1.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background components after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser3, snorm_ser3 = plot_background_norm_inhibition(tser3, bkvecser3, sser3, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 30000\n",
    "avg_bknorm3 = np.mean(bknorm_ser3[transient:])\n",
    "avg_snorm3 = np.mean(snorm_ser3[transient:])\n",
    "avg_reduction_factor3 = avg_snorm3 / avg_bknorm3\n",
    "std_bknorm3 = np.std(bknorm_ser3[transient:])\n",
    "std_snorm3 = np.std(snorm_ser3[transient:])\n",
    "std_reduction_factor3 = std_snorm3 / std_bknorm3\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor3 * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor3 * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor3 * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot of the original and inhibited odors, sampled sparsely in time\n",
    "# NB: due to our choice of components, there is symmetry and degeneracy \n",
    "# in the two lower eigenvalues, so vectors may not be the same, up to\n",
    "# a rotation around the largest, unique PC. \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Raw background\n",
    "transient = 30000\n",
    "skp = 100\n",
    "tslice = slice(transient, None, skp)\n",
    "dimslice = [0, 1, 2]\n",
    "ax.scatter(bkvecser3[tslice, 0], bkvecser3[tslice, 1], bkvecser3[tslice, 2], color=\"r\", \n",
    "           label=\"Background\")\n",
    "ax.scatter(sser3[tslice, 0], sser3[tslice, 1], sser3[tslice, 2], \n",
    "           color=\"b\", label=\"PCA inhibition\")\n",
    "# Compare to inhibition of the mean\n",
    "mean_inhibition = bkvecser3 - np.mean(bkvecser3[transient:], axis=0)*inhib_rates[0]/sum(inhib_rates)\n",
    "ax.scatter(mean_inhibition[tslice, 0], mean_inhibition[tslice, 1], mean_inhibition[tslice, 2], \n",
    "           color=\"xkcd:light blue\", label=\"Average subtraction\")\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=200, alpha=1)\n",
    "\n",
    "# Each row is the mean bkvec\n",
    "orig = np.zeros([3, 3])\n",
    "orig = np.zeros([3, n_components]) + np.mean(bkvecser3[transient:, dimslice, None], axis=0) \n",
    "ax.scatter(*orig, c=\"k\", s=9)\n",
    "# Need to unravel dimensions, which is the last axis of stacks of vectors\n",
    "# However in true_pca[1], each column is an eigenvector: p_components[:, i] is the vector for p_values[i]\n",
    "# so there we don't need to transpose. \n",
    "ax.quiver(*orig, *(true_pca3[1][dimslice, :n_components]), \n",
    "          color=\"k\", lw=2.0, label=\"True PCs\")\n",
    "# In fser, each row is an eigenvector, each column a dimension\n",
    "vecs = np.mean(fser3[transient:, :, dimslice], axis=0)  # Each row is a vector\n",
    "#vecs = vecs / np.sqrt(np.sum(vecs**2, axis=1, keepdims=True))\n",
    "# Remove scale; rows of F have norm F.F^T = Lambda^2\n",
    "vecs = 1.0 / lambda_mat_diag3[:, None] * vecs\n",
    "ax.quiver(*orig, *(vecs[:n_components].T), color=\"b\", lw=2.0, label=\"Learnt PCs\")\n",
    "ax.view_init(azim=330, elev=10)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.85))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
