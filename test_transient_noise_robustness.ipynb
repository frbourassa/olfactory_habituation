{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise robustness of BioPCA and IBCM to transient background odors\n",
    "Consider a simple three-odor case with turbulent statistics, add and remove new background odors on a slow time scale. \n",
    "\n",
    "This requires defining a new background function where, on top of the regular background odors, we add an extra, slow, turbulent odor which, every time it reappears, has a new orientation. Make its time scale much slower and its concentration much smaller. \n",
    "Make it a unit norm vector along one of the $n_R$ ORNs. Hence, it can be chosen with one uniform random sample, to choose the ORN identity. In total, we need two extra pairs of random samples per step, at most: one pair to update t, c of that transient odor, and one pair for ORN selection. \n",
    "\n",
    "(Next step of complexity: generate a new odor each time. Would need to pass a random generator as a parameter? OK for this notebook but would fail for any future multiprocessing. Anyways, first try the simpler step here where the new odor = one ORN is activated. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from math import floor as floor_int\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    fixedpoint_thirdmoment_perturbtheory,\n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_ifpsp_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    check_conc_samples_powerlaw_exp1,\n",
    "    compute_pca_meankept, \n",
    "    compute_projector_series, \n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_background_powerlaw,\n",
    "    sample_ss_conc_powerlaw, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    update_alternating_inputs, \n",
    "    generate_odorant, \n",
    "    update_tc_odor\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    "    tags_list_to_csr_matrix\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_inverse_transform, \n",
    "    truncexp1_density, \n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transient_odor(tc_iu, dt, unif, n_orn, *args):\n",
    "    \"\"\" Update the transient odor. It has a turbulent process for its\n",
    "    whiff/blank time t and concentration c, and a random uniform\n",
    "    uncorrelated sampling of the identity i of the active ORN. \n",
    "    \n",
    "    Args:\n",
    "        tc_iu (np.ndarray): 2x2 array. First row is (t, c) of the\n",
    "            transient odor, second row is the active ORN index \n",
    "            and a dummy number. \n",
    "        dt (float): time step\n",
    "        unif (np.ndarray): 2x2 array of U(0, 1) samples\n",
    "            for t, c update and ORN selection.\n",
    "        n_orn (int): number of ORNs to select from. \n",
    "        parameters: t_w_lo, t_w_hi, t_b_lo, t_b_hi, c0, alpha\n",
    "    \"\"\"\n",
    "    twlo, twhi, tblo, tbhi, c0, alpha = args\n",
    "    # Update needed if dt or less left to the wait time.\n",
    "    if (tc_iu[0, 0] - dt) <= 0:\n",
    "        # Determine whether we were in a whiff or a blank\n",
    "        if tc_iu[0, 1] > 0 :  # we were in a whiff\n",
    "            # Pull t_b, duration of new blank\n",
    "            tc_iu[0, 0:1] = powerlaw_cutoff_inverse_transform(unif[0, 0:1], tblo, tbhi)\n",
    "            # Set c to zero\n",
    "            tc_iu[0, 1] = 0.0\n",
    "        else:  # we were in a blank\n",
    "            # Pull t_w, duration of new whiff\n",
    "            tc_iu[0, 0:1] = powerlaw_cutoff_inverse_transform(unif[0, 0:1], twlo, twhi)\n",
    "            # Set conc c of the whiff\n",
    "            tc_iu[0, 1:2] = truncexp1_inverse_transform(unif[0, 1:], c0, alpha)\n",
    "            # Change active ORN\n",
    "            tc_iu[1, 0] = floor_int(unif[1, 0] * n_orn) + 1e-12  # ensure conversion to float and back is OK\n",
    "            tc_iu[1, 1] = unif[1, 1]\n",
    "    else:\n",
    "        tc_iu[0, 0] = tc_iu[0, 0] - dt\n",
    "    return tc_iu\n",
    "\n",
    "def update_powerlaw_one_orn_noise(tc_bk, params_bk, noises, dt):\n",
    "    \"\"\"\n",
    "    Simulate turbulent odors by pulling wait times until the end of a whiff\n",
    "        or until the next blank, and a concentration of the whiff.\n",
    "        For each odor, check whether the time left until switch is <= zero;\n",
    "        if so, pull either\n",
    "            - another wait time t_w if current c=0, and pull the new c > 0\n",
    "              (we were in a blank and are starting a whiff)\n",
    "            - another wait time t_b if current c > 0, and set c = 0\n",
    "              (we were in a whiff and are starting a blank)\n",
    "        Otherwise, decrement t by dt and don't change c.\n",
    "    Then simulate a turbulent concentration of an extra transient odor vector, \n",
    "    which will be activation of a single ORN, selected by the last pair of\n",
    "    uniform random samples. \n",
    "\n",
    "    Args:\n",
    "        tc_bk (np.ndarray): array of t, c for each odor in the background,\n",
    "            where t = time left until next change, c = current concentration\n",
    "            of the odor. \n",
    "            Next row is a (t, c) pair for the transient odor\n",
    "            Last row is the identity of the active ORN and a dummy number. \n",
    "            Shaped [n_odors + 2, 2]\n",
    "        params_bk (list): contains the following elements (a lot needed!):\n",
    "            whiff_tmins (np.ndarray): lower cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            whiff_tmaxs (np.ndarray): upper cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            blank_tmins (np.ndarray): same as whiff_tmins but for blanks\n",
    "            blank_tmaxs (np.ndarray): same as whiff_tmaxs but for blanks\n",
    "            c0s (np.ndarray): c0 concentration scale for each odor\n",
    "            alphas (np.ndarray): alpha*c0 is the lower cutoff of p_c\n",
    "            noise_ampli (1-element array): amplitude (standard dev.) \n",
    "                of Gaussian white noise added to each ORN on top of the background. \n",
    "            vecs (np.ndarray): 2d array where each row is one of the\n",
    "                possible input vectors\n",
    "        noises (np.ndarray): fresh U(0, 1) samples, shaped [n_odors + [n_R//2], 2],\n",
    "            in case we need to pull a new t and/or c.\n",
    "            TODO: most noises are wasted; for now memory isn't an issue\n",
    "            but this is a place where the code can be optimized a lot.\n",
    "        dt (float): time step duration, in simulation units\n",
    "    \"\"\"\n",
    "    # Update one odor's t and c at a time, if necessary\n",
    "    tc_bk_new = np.zeros(tc_bk.shape)\n",
    "    # infer number of odors based on number of dimensions\n",
    "    vecs_nu = params_bk[-1]\n",
    "    n_odors, n_orn = vecs_nu.shape\n",
    "    for i in range(n_odors):\n",
    "        tc_bk_new[i] = update_tc_odor(tc_bk[i], dt, noises[i, :2],\n",
    "                                *[p[i] for p in params_bk[:6]])\n",
    "    # Update the transient odor concentration too:\n",
    "    tc_bk_new[n_odors:] = update_transient_odor(tc_bk[n_odors:], dt, \n",
    "                                noises[n_odors:, :2], n_orn, \n",
    "                                *[p[n_odors] for p in params_bk[:6]])\n",
    "    # Compute backgound vector (even if it didn't change)\n",
    "    # TODO: this could be optimized too by giving the current back vec\n",
    "    # as an input, but this requires editing the ibcm simulation functions\n",
    "    new_bk_vec = np.dot(tc_bk_new[:n_odors, 1], vecs_nu)\n",
    "    # Add transient ORN activation\n",
    "    orn_index = int(tc_bk_new[n_odors+1, 0])\n",
    "    new_bk_vec[orn_index] += tc_bk_new[n_odors, 1]  # concentration\n",
    "    return new_bk_vec, tc_bk_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 25  # Half the real number for faster simulations\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "inhib_rates = [0.0001, 0.00002]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 1\n",
    "skp = 10 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_one_orn_noise\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x8896ce0154295ba29df7e93dc277af2e)\n",
    "#rgen_meta = np.random.default_rng(seed=0x85dfce01542492a29df7e93dc277ad2d)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components + [1.0]),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components + [500.0]),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components + [2.0]),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components + [800.0]),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components + [0.2]),        # c0s and noise amplitude\n",
    "    np.asarray([0.5] * (n_components + 1)),        # alphas\n",
    "]\n",
    "back_params.append(back_components)\n",
    "\n",
    "# Initial values of background process variables (t, c for each variable)\n",
    "init_concs = sample_ss_conc_powerlaw(*back_params[:6], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components+1), *back_params[2:4])\n",
    "init_concs[n_components:] = 0.0\n",
    "tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "tc_init = np.concatenate([tc_init, np.zeros([1, 2])], axis=0)\n",
    "\n",
    "# Initial background vector \n",
    "init_bkvec = tc_init[:n_components, 1].dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise similarity between background odors\n",
    "Determines how well-posed the PCA is and how easy it is for the IBCM model to disentangle odors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00125  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.3*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])*lambd_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_theta_series(cbser, tau, dt):\n",
    "    theta = np.zeros([cbser.shape[0], cbser.shape[1]])\n",
    "    theta[0] = cbser[0]**2\n",
    "    for i in range(cbser.shape[0]-1):\n",
    "        theta[i+1] = theta[i] + dt/tau*(cbser[i]*cbser[i] - theta[i])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "# Perform successive shorter runs/restarts for memory efficiency\n",
    "tser_ibcm = []\n",
    "nuser_ibcm = []\n",
    "bkvecser_ibcm = []\n",
    "mser_ibcm = []\n",
    "cbarser_ibcm = []\n",
    "wser_ibcm = []\n",
    "sser_ibcm = []\n",
    "thetaser_ibcm = []\n",
    "if n_chunks > 1:\n",
    "    seed_spawns = np.random.SeedSequence(simul_seed).spawn(10)\n",
    "else:\n",
    "    seed_spawns = [simul_seed]\n",
    "for i in range(n_chunks):\n",
    "    tstart = perf_counter()\n",
    "    if i == 0:\n",
    "        init_vari = init_synapses_ibcm\n",
    "        init_back = init_back_list\n",
    "    else:\n",
    "        init_vari = [mser_ibcm[i-1][-1], thetaser_ibcm[i-1][-1], wser_ibcm[i-1][-1]]\n",
    "        init_back = [nuser_ibcm[i-1][-1], bkvecser_ibcm[i-1][-1]]\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_vari, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params, duration/n_chunks, \n",
    "                deltat, seed=seed_spawns[i], noisetype=\"uniform\",  \n",
    "                skp=skp, **ibcm_options\n",
    "    )\n",
    "    tser_ibcm.append(sim_results[0] + i/n_chunks*duration)\n",
    "    nuser_ibcm.append(sim_results[1])\n",
    "    bkvecser_ibcm.append(sim_results[2])\n",
    "    mser_ibcm.append(sim_results[3]) \n",
    "    cbarser_ibcm.append(sim_results[4]) \n",
    "    thetaser_ibcm.append(sim_results[5])\n",
    "    wser_ibcm.append(sim_results[6])\n",
    "    sser_ibcm.append(sim_results[7])\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished chunk\", i, \"in {:.2f} s\".format(tend - tstart))\n",
    "\n",
    "# Concatenate\n",
    "tser_ibcm = np.concatenate(tser_ibcm, axis=0)\n",
    "nuser_ibcm = np.concatenate(nuser_ibcm)\n",
    "bkvecser_ibcm = np.concatenate(bkvecser_ibcm)\n",
    "mser_ibcm = np.concatenate(mser_ibcm)\n",
    "cbarser_ibcm = np.concatenate(cbarser_ibcm)\n",
    "thetaser_ibcm = np.concatenate(thetaser_ibcm)\n",
    "wser_ibcm = np.concatenate(wser_ibcm)\n",
    "sser_ibcm = np.concatenate(sser_ibcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = int(5/6*duration / deltat) // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, back_components)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. Easiest to compute numerically. \n",
    "conc_ser = nuser_ibcm[:, :n_components, 1]\n",
    "# Odors are all iid so we can average over all odors\n",
    "mean_conc = np.mean(conc_ser)\n",
    "sigma2_conc = np.var(conc_ser)\n",
    "thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "moments_conc = [mean_conc, sigma2_conc, thirdmom_conc]\n",
    "\n",
    "# Analytical prediction\n",
    "res = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1)\n",
    "c_specif, c_nonspecif = res[:2]\n",
    "cs_cn = res[:2]\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 2.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, back_components, cs_cn, specif_gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(tser_ibcm[:300], nuser_ibcm[:300, :, 1])\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm/1000, thetaser_ibcm[:, i], lw=0.5, color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"$\\bar{\\Theta} = \\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=320000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "#ax.set_xlim([350, 360])\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "fig.tight_layout()\n",
    "leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "\n",
    "#fig.savefig(\"figures/powerlaw/cbargammas_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = (nuser_ibcm[:, :n_components, 1] \n",
    "                     - np.mean(nuser_ibcm[transient:, :n_components, 1], axis=0))\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/specificities_turbulent_background_example.pdf\", \n",
    "#           transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm[1000:1500], \n",
    "                        bkvecser_ibcm[1000:1500], sser_ibcm[1000:1500], skp=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/pn_activity_norm_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/w_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of average fixed points\n",
    "Check the eigenvalues of the jacobian for one neuron, for every possible specificity. There are $2^{n_B}$ possibilities: choosing specific or not for each odor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_eigenvalues = ibcm_all_largest_eigenvalues(\n",
    "    moments_conc, ibcm_rates, back_components, m3=1.0, cut=1e-16, options=ibcm_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ibcm_specif_keys = list(all_max_eigenvalues.keys())\n",
    "ibcm_eig_values = np.asarray([all_max_eigenvalues[a] for a in ibcm_specif_keys])\n",
    "reals, imags = np.real(ibcm_eig_values), np.imag(ibcm_eig_values)\n",
    "ibcm_eig_values_specif1 = np.asarray([len(s) == 1 for s in ibcm_specif_keys], dtype=bool)\n",
    "highlights = ibcm_eig_values_specif1\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "scaleup = 1e3\n",
    "ax.plot(reals[highlights]*scaleup, imags[highlights]*scaleup, marker=\"*\", mfc=\"b\", mec=\"b\", \n",
    "        ls=\"none\", label=\"One odor\", ms=8)\n",
    "ax.plot(reals[~highlights]*scaleup, imags[~highlights]*scaleup, marker=\"o\", mfc=\"k\", mec=\"k\", \n",
    "       ls=\"none\", label=\"0 or 2+ odors\", ms=6)\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.legend(title=\"Specificity\")\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{max}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{max}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand what happens when $W$ blows up"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(5, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "fig.set_size_inches(6, 12)\n",
    "#t_window = slice(116400, 116800)\n",
    "t_window = slice(20300, 21500)\n",
    "for j in range(n_components):\n",
    "    axes[0].plot(tser_ibcm[t_window]/100, nuser_ibcm[t_window, j, 1])\n",
    "for i in range(n_i_ibcm):\n",
    "    axes[1].plot(tser_ibcm[t_window]/100, l2_norm(mser_ibcm[t_window, i], axis=1))\n",
    "    axes[2].plot(tser_ibcm[t_window]/100, cbarser_ibcm[t_window, i])\n",
    "\n",
    "for i in range(n_i_ibcm):\n",
    "    axes[3].plot(tser_ibcm[t_window]/100, l2_norm(wser_ibcm[t_window, :, i], axis=1))\n",
    "\n",
    "axes[4].plot(tser_ibcm[t_window]/100, l2_norm(sser_ibcm[t_window], axis=1))\n",
    "axes[0].set_ylabel(r\"$\\nu_{\\gamma}(t)$\")\n",
    "axes[1].set_ylabel(r\"$\\| \\vec{m}(t)\\|$\")\n",
    "axes[2].set_ylabel(r\"$\\bar{c}(t)$\")\n",
    "axes[3].set_ylabel(r\"$\\| \\vec{w}_j \\|$\")\n",
    "axes[3].set_yscale(\"log\")\n",
    "axes[4].set_ylabel(r\"$\\| \\vec{s} \\|$\")\n",
    "axes[4].set_yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check the value of different terms in W's equation when things are blowing up\n",
    "def check_derivative(t, bkvecser, mser, cbarser, cbar2_avg, sser, wser, ibcm_params, inhib_params, options={}):\n",
    "    saturation = options.get(\"saturation\", \"linear\")\n",
    "    variant = options.get(\"variant\", \"intrator\")\n",
    "    activ_fct = str(options.get(\"activ_fct\", \"ReLU\")).lower()\n",
    "    decay = options.get(\"decay\", False)\n",
    "    alpha, beta = inhib_params\n",
    "    learnrate, tavg, coupling, sat, ktheta, decay_relative = ibcm_params\n",
    "    \n",
    "    # W derivative\n",
    "    w_deriv1 = alpha*cbarser[t, np.newaxis, :]*sser[t, :, np.newaxis]\n",
    "    w_deriv2 = -beta*wser[t]\n",
    "    \n",
    "    if variant == \"intrator\":\n",
    "        phiterms_vec = cbarser[t] * (cbarser[t] - cbar2_avg)\n",
    "    #  Law and Cooper modification for faster convergence.\n",
    "    elif variant == \"law\":\n",
    "        phiterms_vec = cbarser[t] * (cbarser[t] - cbar2_avg) / (cbar2_avg + ktheta)\n",
    "    if saturation == \"tanh\":\n",
    "        phiterms_vec *=  1.0 - (cbarser[t]/sat)**2\n",
    "    # Now, careful with broadcast: for each neuron (dimension 0 of m and cbar), we need a scalar element\n",
    "    # of phiterms_vec times the whole bkvec, for dimension 1 of m.\n",
    "    # This can be done vectorially with a dot product (n_neu, 1)x(1, n_components)\n",
    "    rhs_scalar = phiterms_vec - coupling*(np.sum(phiterms_vec) - phiterms_vec)\n",
    "    rhs_scalar *= learnrate\n",
    "    \n",
    "    if decay and variant == \"law\":\n",
    "        decay_term = decay_relative * learnrate / (ktheta + cbar2_avg[:, np.newaxis]) * mser[t]\n",
    "    elif decay and variant == \"intrator\":\n",
    "        decay_term = decay_relative * learnrate * mser[t]\n",
    "    \n",
    "    cbar2 = cbarser[t]*cbarser[t] / tavg\n",
    "    cbar2_decay = -cbar2_avg /tavg\n",
    "    \n",
    "    ret_dict = {\n",
    "        \"w_deriv_alpha\": w_deriv1, \n",
    "        \"w_deriv_beta\": w_deriv2, \n",
    "        \"phiterms_vec\": phiterms_vec, \n",
    "        \"rhs_scalar\": rhs_scalar, \n",
    "        \"decay_m\": decay_term,\n",
    "        \"cbar2\": cbar2, \n",
    "        \"cbar2_decay\": cbar2_decay\n",
    "    }\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t_blow = 20380\n",
    "theta_ser = recompute_theta_series(cbarser_ibcm, tau_avg_ibcm, deltat)\n",
    "check_derivative(t_blow, bkvecser_ibcm, mser_ibcm, cbarser_ibcm, theta_ser[t_blow], \n",
    "                 sser_ibcm, wser_ibcm, ibcm_rates, inhib_rates, options=ibcm_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = min(n_components * 2, n_dimensions)  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 11.0\n",
    "#lambda_max_pca = 1.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "# We selected a seed (out of 40+ tested) giving initial conditions leading to correct PCA\n",
    "# The model has trouble converging on this background, we're giving as many chances as possible here. \n",
    "rgen_pca = np.random.default_rng(seed=0x8b6664612cfeda4a121436fcfbbca449)\n",
    "init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compute_pca_gap(true, learnt, n_comp, transient_p):\n",
    "    s = np.sum(np.log10(true[0][:n_comp]) - np.mean(np.log10(learnt[0][transient_p:, :n_comp]), axis=0))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Look for a good seed automatically... really trying to help the PCA model here\n",
    "logpc_gaps_sum = 1.0\n",
    "log_align_error = 1.0\n",
    "transient_pca = 300000 // skp\n",
    "n_trials = 0\n",
    "good_seed = 0x8b6664612cfeda4a121436fcfbbca43e\n",
    "\n",
    "# Record all PCA results, in case one comes close\n",
    "all_pca_gap_results = {}\n",
    "all_pca_align_errors = {}\n",
    "while logpc_gaps_sum > 0.25 and n_trials < 20 and log_align_error > 0.025:\n",
    "    # Try a new seed...\n",
    "    good_seed += 1\n",
    "    rgen_pca = np.random.default_rng(seed=good_seed)\n",
    "    init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run simulation\n",
    "    sim_results = integrate_inhib_ifpsp_network_skip(\n",
    "                    ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                    inhib_rates, back_params, duration, deltat, \n",
    "                    seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "    (tser_pca, \n",
    "     nuser_pca, \n",
    "     bkvecser_pca, \n",
    "     mser_pca, \n",
    "     lser_pca, \n",
    "     xser_pca, \n",
    "     cbarser_pca, \n",
    "     wser_pca, \n",
    "     sser_pca) = sim_results\n",
    "    \n",
    "    # Analyze simulation PCA\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res\n",
    "    log_align_error = np.mean(np.log10(align_error_ser[transient_pca:]))\n",
    "    \n",
    "    # Update convergence criteria\n",
    "    n_trials += 1\n",
    "    logpc_gaps_sum = compute_pca_gap(true_pca, learnt_pca, n_components, transient_pca)\n",
    "    all_pca_gap_results[good_seed] = logpc_gaps_sum\n",
    "    all_pca_align_errors[good_seed] = log_align_error\n",
    "    print(\"Finished trial {}\".format(n_trials-1))\n",
    "\n",
    "best_gap = 1e12\n",
    "best_align_error = 1e12\n",
    "best_pca_seed = good_seed\n",
    "for k in all_pca_gap_results:\n",
    "    if all_pca_gap_results[k] < best_gap:\n",
    "        best_gap = all_pca_gap_results[k]\n",
    "        best_pca_seed = k\n",
    "    if all_pca_align_errors[k] < best_align_error:\n",
    "        best_align_error = all_pca_align_errors[k]\n",
    "print(\"Finished after {} trials\".format(n_trials))\n",
    "print(\"Last gap was {}\".format(logpc_gaps_sum))\n",
    "print(\"Best gap was {} for seed {}\".format(best_gap, best_pca_seed))\n",
    "print(\"With alignment error {}\".format(all_pca_align_errors[best_pca_seed]))\n",
    "print(\"Best align error overall was {}\".format(best_align_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " sser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to de-noised PCA series\n",
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res\n",
    "denoised_bkvecser_pca = nuser_pca[:, :n_components, 1].dot(back_components)\n",
    "res2 = analyze_pca_learning(denoised_bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(true_pca[0] - res2[0][0]) / true_pca[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.statistics import principal_component_analysis\n",
    "from modelfcts.checktools import compute_pca_meankept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "axes[0].get_legend().remove()\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, sser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "# Only consider the subspace spanned by odors, not noise\n",
    "# So need to project on the subspace. \n",
    "sser_ideal = bkvecser_ibcm - bkvecser_ibcm.dot(back_projector.T) * (1.0 - ideal_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "snorm_series = {\n",
    "    \"ibcm\": l2_norm(sser_ibcm), \n",
    "    \"biopca\": l2_norm(sser_pca), \n",
    "    \"avgsub\": l2_norm(sser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(sser_ideal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(snorm_series[a], **std_options)) for a in snorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(snorm_series[a], **mean_options) for a in snorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "snorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + snorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + snorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background tagging after habituation\n",
    "We create a projection matrix, then compute the tag assigned to the background after inhibition by each habituation model, over time. Hopefully, only IBCM inhibits enough to see tags go to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)\n",
    "sser_dict = {\n",
    "    \"ibcm\": sser_ibcm, \n",
    "    \"biopca\": sser_pca, \n",
    "    \"avgsub\": sser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": sser_ideal\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Computing background tag lengths at various time points for each model\n",
    "\n",
    "tag_length_series = {a: np.zeros(tser_ibcm.shape[0]) for a in sser_dict.keys()}\n",
    "for a in sser_dict.keys():\n",
    "    for i in range(0, tag_length_series[a].shape[0]):\n",
    "        if bkvecser_ibcm[i].max() > 0:\n",
    "            tag = project_neural_tag(sser_dict[a][i], bkvecser_ibcm[i], \n",
    "                                 proj_mat, **projection_arguments)\n",
    "        else:\n",
    "            tag = (1,)*int(projection_arguments[\"kc_sparsity\"]*n_kc)\n",
    "        tag_length_series[a][i] = len(tag)\n",
    "tag_length_series_smooth = {a: moving_average(tag_length_series[a], **mean_options)\n",
    "                            for a in tag_length_series}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "for model in sser_dict.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    ax.plot(tser_ibcm / 1000, tag_length_series_smooth[model], **props)\n",
    "snorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "ax.set_ylabel(r\"Tag length, $\\mathrm{card}(z)$\")\n",
    "ax.set_xlabel(\"Time (x1000 steps)\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 100\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "real_back_params = [p[:n_components] for p in back_params[:6]]\n",
    "\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*real_back_params[4:6]))\n",
    "print(\"Average whiff concentration: {:.4f}\".format(avg_whiff_conc))\n",
    "new_test_concs *= avg_whiff_conc\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "conc_samples = sample_ss_conc_powerlaw(\n",
    "                    *real_back_params, size=n_test_times*(n_back_samples-1), rgen=rgen_meta\n",
    "                )  # Shaped [sample, component]\n",
    "back_samples = conc_samples.dot(back_components)\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# TODO: add random ORN activity? for each sample, pull one conc. and one orn choice\n",
    "extra_back_params = [p[n_components:] for p in back_params[:6]]\n",
    "extra_concs = sample_ss_conc_powerlaw(\n",
    "                    *extra_back_params, size=n_test_times*(n_back_samples-1), rgen=rgen_meta\n",
    "                ).flatten()\n",
    "orn_choices = rgen_meta.choice(n_dimensions, size=n_test_times*(n_back_samples-1), replace=True)\n",
    "add_vecs = np.zeros([extra_concs.shape[0], n_dimensions])\n",
    "add_vecs[np.arange(orn_choices.size), orn_choices] = extra_concs\n",
    "add_vecs = add_vecs.reshape(n_test_times, n_back_samples-1, n_dimensions)\n",
    "back_samples[:, 1:] += add_vecs\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_svecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in sser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in sser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in sser_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ideal reduction factor for each concentration\n",
    "dummy_rgen = np.random.default_rng(0x6e3e2886c30163741daaaf7c8b8a00e6)\n",
    "ideal_factors = [compute_ideal_factor(c, moments_conc[:2], [n_components, n_dimensions], \n",
    "                    generate_odorant, (dummy_rgen,)) for c in new_test_concs]\n",
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_svecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_svecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_svecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_svecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_svecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, mixtures, new_test_concs[k], \n",
    "                ideal_factors[k], **avg_options\n",
    "            )\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_svecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_svecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_svecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_svecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/noise_struct/model_comparison_jaccard_transient_noise.pdf\",\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_svecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_snorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
