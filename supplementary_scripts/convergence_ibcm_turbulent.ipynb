{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to turbulent backgrounds as a function of model rates\n",
    "\n",
    "For IBCM, measure the robustness of alignment to one odor using, for each neuron, the difference between its maximum and second largest (or minimum?) alignments, or some similar metric. Then average across neurons to measure the full network convergence. \n",
    "\n",
    "We do not check new odor recognition performance for the sake of simplicity; we can infer whether the IBCM model would perform well or not based on its alignment specificity and noise. \n",
    "\n",
    "Things to check:\n",
    " - Convergence of IBCM as as function of $\\mu$ (or equivalently the OSN input amplitude? Or some moment of the background? not sure) and $\\tau_\\Theta$. \n",
    " - Convergence as a function of the number of odors\n",
    " - Convergence as a function of the strength of turbulence, whatever that means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse, special\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_hgammas_hbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw, \n",
    "    generate_odorant,\n",
    "    mean_turbulent_concs\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"convergence\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"convergence\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background generation and initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combi(concs, backs):\n",
    "    \"\"\" concs: shaped [..., n_odors]\n",
    "        backs: 2D array, shaped [n_odors, n_osn]\n",
    "    \"\"\"\n",
    "    return concs.dot(backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global choice of background and odor mixing functions\n",
    "update_fct = update_powerlaw_times_concs\n",
    "combine_fct = linear_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will later explore the effect of varying these parameters on the convergence, \n",
    "# but put the default ones in a function\n",
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background initialization, given parameters and a seeded random generator\n",
    "def initialize_given_background(back_pms, rgen, n_comp, n_dim):\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-1], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    back_comps = back_pms[-1]\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps)\n",
    "    # background random variables are first in the list of initial values\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze to establish convergence to specific fixed points. \n",
    "def analyze_ibcm_simulation(sim_results, ibcm_rates_loc, back_pms, \n",
    "                            skp_loc=20, dt=1.0, duration_loc=360000.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sim_results = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            hbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "        ibcm_rates_loc: learnrate_ibcm, tau_avg_ibcm, coupling_eta_ibcm, ...\n",
    "            \n",
    "    Returns:\n",
    "        alignment_gaps, indexed [neuron]\n",
    "        specif_gammas, indexed [neuron]\n",
    "        gamma_vari, indexed [neuron, component]\n",
    "    \"\"\"\n",
    "    coupling = ibcm_rates_loc[2]\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "        hbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = sim_results\n",
    "    # Calculate hgammas_bar and mbars\n",
    "    transient = int(5/6*duration_loc / dt) // skp_loc\n",
    "    basis = back_pms[-1]\n",
    "    \n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, hbars_gamma = compute_mbars_hgammas_hbargammas(mser_ibcm, coupling, basis)\n",
    "    hbars_gamma_mean = np.mean(hbars_gamma[transient:], axis=0)\n",
    "    # Sorted odor indices, from min to max, of odor alignments for each neuron\n",
    "    aligns_idx_sorted = np.argsort(hbars_gamma_mean, axis=1) \n",
    "    specif_gammas = np.argmax(hbars_gamma_mean, axis=1)\n",
    "    assert np.all(specif_gammas == aligns_idx_sorted[:, -1])\n",
    "    \n",
    "    \n",
    "    # Gap between first and second largest alignments for each neuron\n",
    "    n_i = hbars_gamma_mean.shape[0]\n",
    "    alignment_gaps = (hbars_gamma_mean[np.arange(n_i), specif_gammas]\n",
    "                     - hbars_gamma_mean[np.arange(n_i), aligns_idx_sorted[:, -2]])\n",
    "    \n",
    "    # Variance (fluctuations) of hbars gamma in the last 20 minutes of the simul\n",
    "    # Increases when the learning rate increases\n",
    "    last_steps = int(2.0*duration_loc/3.0 / dt) // skp_loc\n",
    "    hbars_gamma_vari = np.var(hbars_gamma[last_steps:], axis=0)\n",
    "    \n",
    "    return hbars_gamma, alignment_gaps, specif_gammas, hbars_gamma_vari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analyze_ibcm_one_back_seed(\n",
    "        ibcm_rates_loc, back_rates, inhib_rates_loc, \n",
    "        options_loc, dimensions, seedseq, minit_scale=0.2,\n",
    "        duration_loc=360000.0, dt_loc=1.0, skp_loc=20, full_returns=False\n",
    "    ):\n",
    "    \"\"\" Given IBCM model rates and background parameters except\n",
    "    background odors (but incl. number odors, c0), and a main seed sequence, \n",
    "    run and analyze convergence of IBCM on the background generated from that seed. \n",
    "    The seedseq should itself have been spawned from a root seed to have a distinct\n",
    "    one per run; this still makes seeds reproducible yet distinct for different runs. \n",
    "    The seedseq here is spawned again for a background gen. seed and a simul. seed. \n",
    "    \n",
    "    Args:\n",
    "        dimensions: gives [n_components, n_dimensions, n_i_ibcm]\n",
    "    \n",
    "    Returns:\n",
    "        iff full_return:\n",
    "            gaps, specifs, hgamvari, hgammas_ser, sim_results\n",
    "        else:\n",
    "            gaps, specifs, hgamvari, None, None\n",
    "        alignment_gaps: indexed [neuron]\n",
    "        specif_gammas: indexed [neuron]\n",
    "        gamma_vari: indexed [neuron, component]\n",
    "    \"\"\"\n",
    "    #print(\"Initializing IBCM simulation...\")\n",
    "    # Get dimensions\n",
    "    n_comp, n_dim, n_i = dimensions\n",
    "    \n",
    "    # Spawn back. generation seed and simul seed\n",
    "    initseed, simseed = seedseq.spawn(2)\n",
    "    \n",
    "    # Duplicate back params before appending locally-generated odor vectors to them\n",
    "    back_pms_loc = list(back_rates)\n",
    "    \n",
    "    # Create background\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    back_comps_loc = generate_odorant((n_comp, n_dim), rgen_init)\n",
    "    back_comps_loc = back_comps_loc / l2_norm(back_comps_loc, axis=1)[:, None]\n",
    "\n",
    "    # Add odors to the list of background parameters\n",
    "    back_pms_loc.append(back_comps_loc)\n",
    "\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen_init = np.random.default_rng(initseed)\n",
    "    init_back = initialize_given_background(back_pms_loc, rgen_init, n_comp, n_dim)\n",
    "\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    lambd_loc = ibcm_rates_loc[3]\n",
    "    init_synapses_ibcm = minit_scale*rgen_init.standard_normal(size=[n_i, n_dim])*lambd_loc\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    #print(\"Running IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates_loc, inhib_rates_loc, back_pms_loc, \n",
    "                duration_loc, dt_loc, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_loc, **options_loc\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    #print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    # Now analyze IBCM simul for convergence\n",
    "    #print(\"Starting to analyze IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    hgammas_ser, gaps, specifs, hgamvari = analyze_ibcm_simulation(sim_results, \n",
    "                        ibcm_rates_loc, back_pms_loc, skp_loc=skp_loc, duration_loc=duration_loc)\n",
    "    tend = perf_counter()\n",
    "    #print(\"Finished analyzing IBCM simulation\")\n",
    "    \n",
    "    # Doesn't return full c gamma series, only the summary statistics of convergence\n",
    "    if full_returns:\n",
    "        hgammas_ser_ret = hgammas_ser\n",
    "        sim_results_ret = sim_results\n",
    "    else:\n",
    "        hgammas_ser_ret = None\n",
    "        sim_results_ret = None\n",
    "    \n",
    "    return gaps, specifs, hgamvari, hgammas_ser_ret, sim_results_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, hbars_gamma, skp=20):\n",
    "    # tseries, bk_series, bkvec_series, m_series,\n",
    "    # hbar_series, theta_series, w_series, y_series\n",
    "    tser, bkser, bkvecser, _, _, _, _, yser = res_ibcm_raw\n",
    "    tser_scaled = tser *  10.0 / 60.0  # in min\n",
    "    # Plot of hbars gamma series\n",
    "    fig1 , ax1, _ = plot_hbars_gamma_series(tser_scaled, hbars_gamma, \n",
    "                            skp=skp, transient=320000 // skp)\n",
    "    fig1.tight_layout()\n",
    "    leg = ax1.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    ax1.set_xlabel(\"Time (min)\")\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig2, ax2, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser, bkvecser, yser, skp=skp)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax2.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax2.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig2.tight_layout()\n",
    "    \n",
    "    return fig1, ax1, fig2, ax2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBCM simulation with proper rates \n",
    "To illustrate convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for all simulations\n",
    "# Dimensions: 25 is enough?\n",
    "n_dimensions = 25\n",
    "n_components = 3  # try with 3 for simplicity by default\n",
    "\n",
    "# Inhibition W learning and decay rates\n",
    "inhib_rates_default = [0.0001, 0.00002]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration and integration time step\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Saving every skp simulation point, 50 is enough for plots, \n",
    "# here use 20 to get convergence time accurately\n",
    "skp_default = 20 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp_default)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Default model rates\n",
    "learnrate_ibcm = 0.00125 #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates_default = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "meta_seedseq = np.random.SeedSequence(0x36d8487b210fe0277493149915aba4f3)\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "back_rates_default = default_background_params(n_components)\n",
    "# Try changing background rates here if desired\n",
    "# Concentration scale c0: multiply up-down to change convergence dynamics\n",
    "# just as well as the learning rate, although with a different scaling. \n",
    "#back_rates_default[4][:] = 0.6\n",
    "#back_rates_default[1][:] = 500.0  # whiff duration\n",
    "#back_rates_default[3][:] = 800.0  # blank duration\n",
    "dimensions_ibcm = [n_components, n_dimensions, n_i_ibcm]\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "all_res = run_analyze_ibcm_one_back_seed(ibcm_rates_default, back_rates_default, inhib_rates_default, \n",
    "                        ibcm_options, dimensions_ibcm, meta_seedseq,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)\n",
    "\n",
    "gaps_def, specifs_def, hgamvari_def, hgammas_ser_def, ibcm_results_def = all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "plot_ibcm_results(ibcm_results_def, hgammas_ser_def, skp=skp_default)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print some other convergence analysis results\n",
    "print(\"{} out of {} odors covered\".format(np.unique(specifs_def).size, n_components))\n",
    "print(\"Variance of the largest hbar_gamma for each neuron:\\n\", hgamvari_def[np.arange(n_i_ibcm), specifs_def])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(n_i_ibcm), gaps_def, marker=\"o\", mfc=\"w\", ms=8)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.annotate(str(specifs_def[i]), xy=(i, gaps_def[i]), ha=\"center\", va=\"center\")\n",
    "ax.set_ylim([0.0, gaps_def.max()*1.1])\n",
    "ax.set(ylabel=\"Alignment gap\", xlabel=\"Neuron\", xticks=np.arange(0, n_i_ibcm, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation with learning rate too high\n",
    "\n",
    "The neurons still select one odor, but have large oscillations around the specific fixed point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with excessively high learning rate\n",
    "learnrate_ibcm_hi = 0.005\n",
    "tau_avg_ibcm_hi = 2000\n",
    "ibcm_rates_hi = [\n",
    "    learnrate_ibcm_hi, \n",
    "    tau_avg_ibcm_hi, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "\n",
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "meta_seedseq_hi = np.random.SeedSequence(0xa5ad7857493a6fc471b0283b8db69268)\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "back_rates_default = default_background_params(n_components)\n",
    "dimensions_ibcm = [n_components, n_dimensions, n_i_ibcm]\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "all_res = run_analyze_ibcm_one_back_seed(ibcm_rates_hi, back_rates_default, inhib_rates_default, \n",
    "                        ibcm_options, dimensions_ibcm, meta_seedseq_hi,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)\n",
    "\n",
    "gaps_hi, specifs_hi, hgamvari_hi, hgammas_ser_hi, ibcm_results_hi = all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "plot_ibcm_results(ibcm_results_hi, hgammas_ser_hi, skp=skp_default)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation example with learning rate too slow\n",
    "Most neurons get stuck at zero and do not converge to a selective fixed point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnrate_ibcm_low = 2e-4\n",
    "tau_avg_ibcm_low = 1600  # 2000\n",
    "ibcm_rates_low = [\n",
    "    learnrate_ibcm_low, \n",
    "    tau_avg_ibcm_low, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "\n",
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "meta_seedseq_low = np.random.SeedSequence(0x65be6823817996324d42794007b45e89)\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "back_rates_default = default_background_params(n_components)\n",
    "dimensions_ibcm = [n_components, n_dimensions, n_i_ibcm]\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "all_res = run_analyze_ibcm_one_back_seed(ibcm_rates_low, back_rates_default, inhib_rates_default, \n",
    "                        ibcm_options, dimensions_ibcm, meta_seedseq_low,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)\n",
    "\n",
    "gaps_low, specifs_low, hgamvari_low, hgammas_ser_low, ibcm_results_low = all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "plot_ibcm_results(ibcm_results_low, hgammas_ser_low, skp=skp_default)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = pj(outputs_folder, \"ibcm_convergence_hgammas_examples.npz\")\n",
    "if do_save_outputs:\n",
    "    np.savez_compressed(fname,\n",
    "        hgammas_ser_hi=hgammas_ser_hi,\n",
    "        hgammas_ser_low=hgammas_ser_low,\n",
    "        hgammas_ser_def=hgammas_ser_def\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop main simulations\n",
    "\n",
    "Simulations are run in the script ``supplementary_scripts/run_ibcm_convergence_analysis.py`` and results saved in ``results/for_plots/convergence/``. \n",
    "\n",
    "We run 32 simulation seeds for each combination of rates on a grid of $\\mu, \\tau_\\Theta$ choices, and we do it for $N_\\mathrm{B} = 3, 4, 5, 6, 8$ background odors. \n",
    "\n",
    "Later, might do simulations at fixed $N_\\mathrm{B}$, and perhaps fixed model rates too, for different whiff and blank durations (scale both up or down?)\n",
    "\n",
    "And do the same for the concentration amplitude, controlled by $c_0$, to show it is equivalent to scaling $\\mu$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_odor_coverage(specifs):\n",
    "    nmu, ntau = specifs.shape[:2]\n",
    "    cov = np.zeros(specifs.shape[:3])\n",
    "    for i in range(nmu):\n",
    "        for j in range(ntau):\n",
    "            for k in range(specifs.shape[2]):\n",
    "                cov[i, j, k] = np.unique(specifs[i, j, k]).size\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vari_specifs(allhvaris, specifs):\n",
    "    vari_specifs = np.zeros(allhvaris.shape[:4])\n",
    "    n_mu, n_tau, n_seed, n_i = specifs.shape[:4]\n",
    "    for i in range(n_mu):\n",
    "        for j in range(n_tau):\n",
    "            for k in range(n_seed):\n",
    "                vari_specifs[i, j, k] = allhvaris[i, j, k][np.arange(n_i), specifs[i, j, k]]\n",
    "    return vari_specifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "conv_results = np.load(pj(outputs_folder, \"convergence_vs_ibcm_rates_results_3odors.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutau_grid = conv_results[\"mutau_grid\"]\n",
    "align_gaps = conv_results[\"align_gaps\"]\n",
    "gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "hgamma_varis = conv_results[\"hgamma_varis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$\", \n",
    "          loc=\"upper left\", frameon=False, fontsize=5, bbox_to_anchor=(0.0, 1.05))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "tau_range = mutau_grid[1, 0, :]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_tau)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    gap_mean_line_mu = np.mean(align_gaps[i, :, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_mu = np.std(np.mean(align_gaps[i, :, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(tau_range, gap_mean_line_mu, color=colors[i], label=mutau_grid[0, i, 0])\n",
    "    ax.fill_between(tau_range, gap_mean_line_mu-gap_std_line_mu, \n",
    "                   gap_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"Averaging time scale $\\tau_{\\Theta}$\", \n",
    "       ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\mu$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_gaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage of odors: fraction of seeds where all 3 odors are covered?\n",
    "fig, ax = plt.subplots()\n",
    "# extent: left, right, bottom, top\n",
    "#im = ax.imshow(np.mean(align_gaps, axis=(2, 3)).T, \n",
    "#          extent=(mu_range.min(), mu_range.max(), tau_range.min()*0.01, tau_range.max()*0.01), \n",
    "#         cmap=\"viridis\", aspect=\"auto\")\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], np.mean(align_gaps, axis=(2, 3)), cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "fig.colorbar(im, label=\"Alignment gap\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "vari_specifs = get_vari_specifs(hgamma_varis, gamma_specifs)\n",
    "\n",
    "# Square root from variance to standard deviation\n",
    "# and normalize by the average alignment gap\n",
    "stdev_specifs = np.sqrt(vari_specifs) / np.mean(align_gaps, axis=(2,3), keepdims=True)\n",
    "\n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Alignment noise\", \n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", ncol=2, frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "n_i = gamma_specifs.shape[3]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    std_mean_line_mu = np.mean(stdev_specifs[i, :, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    std_std_line_mu = np.std(stdev_specifs[i, :, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(tau_range, std_mean_line_mu, color=colors[i], label=mutau_grid[0, i, 0])\n",
    "    #ax.fill_between(tau_range, std_mean_line_mu-std_std_line_mu, \n",
    "    #               std_mean_line_mu+std_std_line_mu, color=colors[i], alpha=0.15)\n",
    "ax.set(xlabel=r\"Averaging time $\\tau_\\Theta$\", \n",
    "       ylabel=r\"Standard dev. of $h_{\\gamma,sp}$\" \"\\n(smaller is better)\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\mu$\", frameon=False, bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage of odors: fraction of seeds where all 3 odors are covered?\n",
    "odor_coverage = compute_odor_coverage(gamma_specifs)\n",
    "odor_coverage_mean = np.mean(odor_coverage, axis=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# extent: left, right, bottom, top\n",
    "#im = ax.imshow(odor_coverage_mean.T, \n",
    "#          extent=(mu_range.min(), mu_range.max(), tau_range.min()*0.01, tau_range.max()*0.01), \n",
    "#         cmap=\"viridis\", aspect=\"auto\")\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], odor_coverage_mean, cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\", yscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "ax.set_yticks(tau_range)\n",
    "fig.colorbar(im, label=\"Average # odors covered\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_line_tau = odor_coverage_mean[:, j]\n",
    "    coverage_std_tau = np.std(odor_coverage[:, j], axis=1)  # std across seed\n",
    "    ax.plot(mu_range, coverage_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, coverage_line_tau-coverage_std_tau, \n",
    "    #               coverage_line_tau+coverage_std_tau, color=colors[j], alpha=0.0)\n",
    "ax.axhline(3.0, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Average # odors covered\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"lower left\",  frameon=False, ncol=2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage of odors, but now the fraction of seeds where all 3 odors are covered\n",
    "n_seeds = odor_coverage.shape[2]\n",
    "odor_coverage_frac_seeds = (odor_coverage == 3).sum(axis=2) / n_seeds\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_frac_line_tau = odor_coverage_frac_seeds[:, j]\n",
    "    ax.plot(mu_range, coverage_frac_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "ax.axhline(1.0, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Prob. of full coverage\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for different odor numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "n_odors = 8\n",
    "conv_results = np.load(pj(outputs_folder, f\"convergence_vs_ibcm_rates_results_{n_odors}odors.npz\"))\n",
    "\n",
    "mutau_grid = conv_results[\"mutau_grid\"]\n",
    "align_gaps = conv_results[\"align_gaps\"]\n",
    "gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "hgamma_varis = conv_results[\"hgamma_varis\"]\n",
    "\n",
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], np.mean(align_gaps, axis=(2, 3)), cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "fig.colorbar(im, label=\"Alignment gap\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors: average number of odors covered. \n",
    "# Or fraction of seeds where all 3 odors are covered? TODO\n",
    "odor_coverage = compute_odor_coverage(gamma_specifs)\n",
    "odor_coverage_mean = np.mean(odor_coverage, axis=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_line_tau = odor_coverage_mean[:, j]\n",
    "    coverage_std_tau = np.std(odor_coverage[:, j], axis=1)  # std across seed\n",
    "    #ax.plot(mu_range, coverage_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.plot(mu_range, np.mean(odor_coverage[:, j], axis=1), color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, coverage_line_tau-coverage_std_tau, \n",
    "                   coverage_line_tau+coverage_std_tau, color=colors[j], alpha=0.1)\n",
    "ax.axhline(n_odors, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Average # odors covered\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors, but now the fraction of seeds where all 3 odors are covered\n",
    "n_seeds = odor_coverage.shape[2]\n",
    "odor_coverage_frac_seeds = (odor_coverage == n_odors).sum(axis=2) / n_seeds\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_frac_line_tau = odor_coverage_frac_seeds[:, j]\n",
    "    ax.plot(mu_range, coverage_frac_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "ax.axhline(1.0, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Prob. of full coverage\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "n_i = gamma_specifs.shape[3]\n",
    "vari_specifs = get_vari_specifs(hgamma_varis, gamma_specifs)\n",
    "\n",
    "# Square root from variance to standard deviation\n",
    "# and normalize by the average alignment gap\n",
    "stdev_specifs = np.sqrt(vari_specifs) / np.mean(align_gaps, axis=(2,3), keepdims=True)\n",
    "\n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Alignment noise\", \n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", ncol=2, frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots as a function of the number of odors\n",
    "\n",
    " Alignment gap, average fraction odors covered as a function of number of odors. \n",
    " \n",
    " Pick one $\\tau$, show lines for different $\\mu$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_align_gaps = []\n",
    "concatenated_odor_coverages = []\n",
    "concatenated_hgams_varis = []\n",
    "nodors_range = np.asarray([3, 4, 5, 6, 8])\n",
    "f_name_prefix = \"convergence_vs_ibcm_rates_results_\"\n",
    "for n in nodors_range:\n",
    "    try:\n",
    "        fp = np.load(pj(outputs_folder, f_name_prefix + f\"{n}odors.npz\"))\n",
    "    except FileNotFoundError: \n",
    "        continue\n",
    "    else:\n",
    "        concatenated_align_gaps.append(fp[\"align_gaps\"])\n",
    "        specifs_n = fp[\"gamma_specifs\"]\n",
    "        concatenated_odor_coverages.append(compute_odor_coverage(specifs_n))\n",
    "        vari_specifs = get_vari_specifs(fp[\"hgamma_varis\"], fp[\"gamma_specifs\"])\n",
    "        concatenated_hgams_varis.append(vari_specifs)\n",
    "concatenated_align_gaps = np.stack(concatenated_align_gaps)\n",
    "concatenated_odor_coverages = np.stack(concatenated_odor_coverages)\n",
    "concatenated_hgams_varis = np.stack(concatenated_hgams_varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chosen_tau_j = 5  # 1600\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"ocean\", n_colors=n_mu)\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    gap_mean_line_mu = np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_mu = np.std(np.mean(concatenated_align_gaps[:, i, chosen_tau_j], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, gap_mean_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=\"o\", ms=3)\n",
    "    ax.fill_between(nodors_range, gap_mean_line_mu-gap_std_line_mu, \n",
    "                   gap_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"Number of odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Alignment gap\\n(larger is better)\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$, ($\\tau_{\\Theta} = \" + \"{:d}$)\".format(int(mutau_grid[1, 0, chosen_tau_j])), \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    coverage_line_mu = np.mean(concatenated_odor_coverages[:, i, chosen_tau_j], axis=1) / nodors_range\n",
    "    coverage_std_mu = np.std(concatenated_odor_coverages[:, i, chosen_tau_j], axis=1) / nodors_range  # std across seed\n",
    "    ax.plot(nodors_range, coverage_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=\"o\", ms=3)\n",
    "    ax.fill_between(nodors_range, coverage_line_mu-coverage_std_mu, \n",
    "                   coverage_line_mu+coverage_std_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"Number of odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Average fraction odors covered\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\tau_{\\Theta} = \" + \"{:d}$)\".format(int(mutau_grid[1, 0, chosen_tau_j])), \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the standard deviation across seeds and neurons, divide by the alignment gap average\n",
    "# Axes are indexed [n_odor, n_mu, n_tau, n_seeds, n_neurons, ...]\n",
    "align_noises_vs_nodor = (np.sqrt(concatenated_hgams_varis)\n",
    "                         / np.mean(concatenated_align_gaps, axis=3, keepdims=True))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "for i in range(n_mu-1, -1, -1):\n",
    "    std_mean_line_mu = np.mean(align_noises_vs_nodor[:, i, chosen_tau_j], axis=(1, 2))\n",
    "    std_std_line_mu = np.std(np.mean(align_noises_vs_nodor[:, i, chosen_tau_j], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(nodors_range, std_mean_line_mu, color=colors[i], \n",
    "            label=str(mutau_grid[0, i, 0]), marker=\"o\", ms=3)\n",
    "    ax.fill_between(nodors_range, std_mean_line_mu-std_std_line_mu, \n",
    "                   std_mean_line_mu+gap_std_line_mu, color=colors[i], alpha=0.1)\n",
    "ax.set(xlabel=r\"Number of odors $N_\\mathrm{B}$ (in $N_\\mathrm{S}=25$ dim.)\", \n",
    "       ylabel=\"Alignment noise\", #,\" + r\"$\\sigma[h_\\mathrm{sp}] / \\langle \\Delta h \\rangle$\",\n",
    "      yscale=\"log\")\n",
    "ax.legend(handles=handles, labels=labels, \n",
    "          title=r\"$\\mu$ ($\\mathrm{s^{-1}}$)\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(0.98, 1), frameon=False)\n",
    "\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(panels_folder, \"supfig_convergence_turbulent_alignment_noise_vs_nodors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the effect of the strength of turbulence\n",
    "\n",
    "Note that for a concentration scale $c_0$ (for unit-normed odor vectors), the IBCM convergence time scales as $1/\\mu c_0^2$ (see analytical results and notebook ``non-gaussian_convergence.ipynb``) while the $h_\\mathrm{sp}$, $h_\\mathrm{ns}$ fixed points have scale $1/c_0$ (see appendix, section 4.E, analytical IBCM solutions for $y_1, y_2$). So, we need to adjust the learning rate to keep $\\mu c_0^2$ constant. \n",
    "\n",
    "We also need to adjust the $\\alpha$ rate for $W$ to prevent numerical instabilities arising from larger $h$ values: so, keep $\\alpha / c_0$ constant, $\\alpha' =\\alpha c_0'/c_0$, $\\alpha$ scaled proportional to $c_0$.  \n",
    "\n",
    "## Initial value correction\n",
    "\n",
    "The other factor influencing initial convergence time is $1/h_0 - 1/h_{\\mathrm{saddle}}$, which we want to keep constant. Since $h_{\\mathrm{saddle}} \\sim 1/c_0$ while $ h_0 \\sim m_0 c_0$, we want to make $m_0 \\sim 1/c_0^2$ to ensure $h_0 \\sim 1/c_0$ as well. \n",
    "\n",
    "\n",
    "### Other issue with the Law variant\n",
    "\n",
    "$\\mu$ is divided, in the Law variant of the IBCM model, by $k_{\\Theta} + \\Theta$, which scales as $h^2$ and thus as $1/c_0^2$. So, for small $c_0$, $\\mu$ is divided by a large $\\Theta$ after the initial phase, so the learning rate is effectively too low and convergence to the specific fixed point cannot be completed. Likewise, for large $c_0$, the learning rate is not diminished much by the denominator near the fixed point, so there are more fluctuations. We do not compensate this effect, so it is another contribution to the IBCM network's sensitivity to concentration scale. \n",
    "\n",
    "### Bottom line issue\n",
    "\n",
    "The IBCM model becomes specific in response to the difference between the second and third moments of the background; when $c_0$ is too small, the difference between $m_3 \\sim c_0^3$ and $\\sigma^2 \\sim c_0^2$ is of order $c_0^3 - c_0^2$, so a factor of $c_0-1$ remains even when dividing the learning rate by $c_0^2$; this remains small. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learnrate(back_pms, back_pms_ref, mu_ref):\n",
    "    \"\"\" Return mu to keep mu * <c>**2 constant compared\n",
    "    to the average concentration entailed by back_pms_ref\n",
    "    and the learning rate reference mu_ref. \"\"\"\n",
    "    avg_conc_ref = mean_turbulent_concs(back_pms_ref).mean()\n",
    "    avg_conc_sim = mean_turbulent_concs(back_pms).mean()\n",
    "    return mu_ref * (avg_conc_ref / avg_conc_sim) ** 2\n",
    "\n",
    "def adjust_minit_scale(back_pms, back_pms_ref, mscale_ref=0.2):\n",
    "    c0ref = back_pms_ref[4].mean()\n",
    "    c0 = back_pms[4].mean()\n",
    "    mscale = mscale_ref * (c0ref / c0)**2\n",
    "    return mscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnrate_ibcm_turb_base = 0.00125\n",
    "tau_avg_ibcm_turb = 2000  # 2000\n",
    "do_adjust_rate = True\n",
    "ibcm_rates_turb = [\n",
    "    learnrate_ibcm_turb_base, \n",
    "    tau_avg_ibcm_turb, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm\n",
    "]\n",
    "inhib_rates_turb = [5e-5, 1e-5]\n",
    "\n",
    "# Test run for now\n",
    "# Create a default background for testing purposes\n",
    "#meta_seedseq_turb = np.random.SeedSequence(0xbaf3feeb1391885d09d21eed643ee07f)\n",
    "meta_seedseq_turb = np.random.SeedSequence(0x8f380db086f6b8c843bd7b9969a7a2c0).spawn(2)[1]\n",
    "\n",
    "# Package dimensions and back. parameters\n",
    "n_components_turb = 3\n",
    "back_rates_turb = default_background_params(n_components_turb)\n",
    "\n",
    "# Reference average conc.\n",
    "back_rates_turb_base = default_background_params(n_components_turb)\n",
    "\n",
    "# Try changing background rates here if desired\n",
    "# Concentration scale c0: multiply up-down to change convergence dynamics\n",
    "# just as well as the learning rate, although with a different scaling. \n",
    "back_rates_turb[4][:] = 0.6  # concentration scale, default 0.6\n",
    "back_rates_turb[5][:] = 0.5  # concentration cutoff relative to c0, default 0.5\n",
    "\n",
    "back_rates_turb[1][:] = 20.0  # whiff duration, default 500.0  (steps of 10 ms)\n",
    "back_rates_turb[2][:] = 1.0  # minimum blank duration, default 1.0  (step of 10 ms)\n",
    "back_rates_turb[3][:] = 200.0  # blank duration, default 800 (steps of 10 ms)\n",
    "\n",
    "# adjust mu to compensate average conc. constant\n",
    "if do_adjust_rate:\n",
    "    learnrate_ibcm_turb = adjust_learnrate(back_rates_turb, \n",
    "            back_rates_turb_base, learnrate_ibcm_turb_base)\n",
    "    print(\"Adjusted learning rate:\", learnrate_ibcm_turb)\n",
    "    minit_turb = adjust_minit_scale(back_rates_turb, \n",
    "            back_rates_turb_base, 0.2)\n",
    "else:\n",
    "    learnrate_ibcm_turb = learnrate_ibcm_turb_base\n",
    "    print(\"Not adjusted learning rate:\", learnrate_ibcm_turb)\n",
    "    minit_turb = 0.2\n",
    "ibcm_rates_turb[0] = learnrate_ibcm_turb\n",
    "\n",
    "dimensions_ibcm = [n_components_turb, n_dimensions, n_i_ibcm]\n",
    "\n",
    "# Run and analyze simulation derived from the meta seedsequence\n",
    "all_res = run_analyze_ibcm_one_back_seed(ibcm_rates_turb, back_rates_turb, inhib_rates_turb, \n",
    "                        ibcm_options, dimensions_ibcm, meta_seedseq_turb, minit_scale=minit_turb,\n",
    "                        duration_loc=duration, dt_loc=deltat, skp_loc=skp_default, full_returns=True)\n",
    "\n",
    "gaps_turb, specifs_turb, hgamvari_turb, hgammas_ser_turb, ibcm_results_turb = all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical predictions?\n",
    "conc_ser = ibcm_results_turb[1][:, :, 1]\n",
    "mean_conc = np.mean(conc_ser)\n",
    "moments_conc = [\n",
    "    mean_conc, \n",
    "    np.mean((conc_ser - mean_conc)**2),\n",
    "    np.mean((conc_ser - mean_conc)**3)\n",
    "]\n",
    "hspecif, hnonsp, hdpred, u2pred = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components_turb-1)\n",
    "print(moments_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence dynamics first\n",
    "fig1, ax1, _, _ = plot_ibcm_results(ibcm_results_turb, hgammas_ser_turb, skp=skp_default)\n",
    "# Plot analytical predictions, because when the time scale is shorter, the average hgammas are\n",
    "# very close to them; long time scales and ensuing correlations between s, theta, m cause deviations. \n",
    "ax1.axhline(hspecif, ls=\"--\", color=\"k\")\n",
    "ax1.axhline(hnonsp, ls=\"--\", color=\"grey\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_turb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results of the full simulation scripts\n",
    "\n",
    "Added main functions to test convergence as a function of $c_0$ vs $\\mu$ and of $t_w$ and $t_b$ upper limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "n_odors = 3\n",
    "conv_results = np.load(pj(outputs_folder, \"convergence_vs_turbulence_strength_results_3odors.npz\"))\n",
    "\n",
    "mutau_grid = conv_results[\"mutau_grid\"]\n",
    "align_gaps = conv_results[\"align_gaps\"]\n",
    "gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "hgamma_varis = conv_results[\"hgamma_varis\"]\n",
    "\n",
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], np.mean(align_gaps, axis=(2, 3)), cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\", yscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "ax.set_ylim(tau_range[0], tau_range[-1])\n",
    "fig.colorbar(im, label=\"Alignment gap\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors: average number of odors covered. \n",
    "# Or fraction of seeds where all 3 odors are covered? TODO\n",
    "odor_coverage = compute_odor_coverage(gamma_specifs)\n",
    "odor_coverage_mean = np.mean(odor_coverage, axis=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_line_tau = odor_coverage_mean[:, j]\n",
    "    coverage_std_tau = np.std(odor_coverage[:, j], axis=1)  # std across seed\n",
    "    #ax.plot(mu_range, coverage_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.plot(mu_range, np.mean(odor_coverage[:, j], axis=1), color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, coverage_line_tau-coverage_std_tau, \n",
    "                   coverage_line_tau+coverage_std_tau, color=colors[j], alpha=0.1)\n",
    "ax.axhline(n_odors, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Average # odors covered\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors, but now the fraction of seeds where all 3 odors are covered\n",
    "n_seeds = odor_coverage.shape[2]\n",
    "odor_coverage_frac_seeds = (odor_coverage == n_odors).sum(axis=2) / n_seeds\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_frac_line_tau = odor_coverage_frac_seeds[:, j]\n",
    "    ax.plot(mu_range, coverage_frac_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "ax.axhline(1.0, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Prob. of full coverage\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "n_i = gamma_specifs.shape[3]\n",
    "vari_specifs = get_vari_specifs(hgamma_varis, gamma_specifs)\n",
    "\n",
    "# Square root from variance to standard deviation\n",
    "# and normalize by the average alignment gap\n",
    "stdev_specifs = np.sqrt(vari_specifs) / np.mean(align_gaps, axis=(2,3), keepdims=True)\n",
    "\n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Alignment noise\", \n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", ncol=2, frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_gaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete simulations run on the cluster\n",
    "n_odors = 3\n",
    "conv_results = np.load(pj(outputs_folder, f\"convergence_vs_background_ampli_results_3odors.npz\"))\n",
    "\n",
    "mutau_grid = conv_results[\"mutau_grid\"]\n",
    "align_gaps = conv_results[\"align_gaps\"]\n",
    "gamma_specifs = conv_results[\"gamma_specifs\"]\n",
    "hgamma_varis = conv_results[\"hgamma_varis\"]\n",
    "\n",
    "n_mu, n_tau = mutau_grid.shape[1:3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau-1, -1, -1):\n",
    "    gap_mean_line_tau = np.mean(align_gaps[:, j, :], axis=(1, 2))\n",
    "    # Variance across seed of the mean alignment in a simulation. \n",
    "    # We don't wan't the intra-simulation variance, neurons selecting different \n",
    "    # odors may converge to different alignment values due to background fluctuations\n",
    "    gap_std_line_tau = np.std(np.mean(align_gaps[:, j, :], axis=2), axis=1, ddof=1)\n",
    "    ax.plot(mu_range, gap_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, gap_mean_line_tau-gap_std_line_tau, \n",
    "                   gap_mean_line_tau+gap_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Alignment gap\\n(larger is better)\", xscale=\"log\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, title=r\"$\\tau_{\\Theta}$\", \n",
    "          loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(mutau_grid[0], mutau_grid[1], np.mean(align_gaps, axis=(2, 3)), cmap=\"viridis\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Averaging time $\\tau_{\\Theta}$ (s)\", xscale=\"log\", yscale=\"log\")\n",
    "ax.set_xlim(mu_range[0], mu_range[-1])\n",
    "ax.set_ylim(tau_range[0], tau_range[-1])\n",
    "fig.colorbar(im, label=\"Alignment gap\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors: average number of odors covered. \n",
    "# Or fraction of seeds where all 3 odors are covered? TODO\n",
    "odor_coverage = compute_odor_coverage(gamma_specifs)\n",
    "odor_coverage_mean = np.mean(odor_coverage, axis=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_line_tau = odor_coverage_mean[:, j]\n",
    "    coverage_std_tau = np.std(odor_coverage[:, j], axis=1)  # std across seed\n",
    "    #ax.plot(mu_range, coverage_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.plot(mu_range, np.mean(odor_coverage[:, j], axis=1), color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    ax.fill_between(mu_range, coverage_line_tau-coverage_std_tau, \n",
    "                   coverage_line_tau+coverage_std_tau, color=colors[j], alpha=0.1)\n",
    "ax.axhline(n_odors, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Average # odors covered\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Coverage of odors, but now the fraction of seeds where all 3 odors are covered\n",
    "n_seeds = odor_coverage.shape[2]\n",
    "odor_coverage_frac_seeds = (odor_coverage == n_odors).sum(axis=2) / n_seeds\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mu_range = mutau_grid[0, :, 0]\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "for j in range(n_tau):\n",
    "    coverage_frac_line_tau = odor_coverage_frac_seeds[:, j]\n",
    "    ax.plot(mu_range, coverage_frac_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "ax.axhline(1.0, ls=\"--\", color=\"k\")\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=\"Prob. of full coverage\", xscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot the variance of the specific c_gamma, mean across neurons and seeds\n",
    "fig, ax = plt.subplots()\n",
    "colors = sns.color_palette(\"magma\", n_colors=n_tau)\n",
    "n_i = gamma_specifs.shape[3]\n",
    "vari_specifs = get_vari_specifs(hgamma_varis, gamma_specifs)\n",
    "\n",
    "# Square root from variance to standard deviation\n",
    "# and normalize by the average alignment gap\n",
    "stdev_specifs = np.sqrt(vari_specifs) / np.mean(align_gaps, axis=(2,3), keepdims=True)\n",
    "\n",
    "for j in range(n_tau):\n",
    "    std_mean_line_tau = np.mean(stdev_specifs[:, j, :], axis=(1, 2))\n",
    "    std_std_line_tau = np.std(stdev_specifs[:, j, :], axis=(1, 2), ddof=1)\n",
    "    ax.plot(mu_range, std_mean_line_tau, color=colors[j], label=int(mutau_grid[1, 0, j]))\n",
    "    #ax.fill_between(mu_range, std_mean_line_tau-std_std_line_tau, \n",
    "    #               std_mean_line_tau+std_std_line_tau, color=colors[j], alpha=0.15)\n",
    "ax.set(xlabel=r\"Learning rate $\\mu$\", ylabel=r\"Alignment noise\", \n",
    "       xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(title=r\"$\\tau_{\\Theta}$\", ncol=2, frameon=False)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
