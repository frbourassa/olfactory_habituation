{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-orthogonality of odor vectors\n",
    "\n",
    "Illustrate that the background vectors and resulting mixture are non-negative and non-orthogonal; also compute the average dot product between two vectors with exponentially-distributed elements afterwise unit-normed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "import os, json, sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "from os.path import join as pj\n",
    "\n",
    "from modelfcts.average_sub import integrate_inhib_average_sub_skip\n",
    "\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs,\n",
    "    sample_ss_conc_powerlaw,\n",
    "    sample_ss_mixed_concs_powerlaw,\n",
    "    generate_odorant, \n",
    "    generate_gamma_odorant\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.metrics import jaccard, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_save_plots = True\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"powerlaw_turbulent\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)\n",
    "    \n",
    "models = list(model_colors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly dimensions  # I also tested 25\n",
    "n_components = 4  # Number of background odors\n",
    "\n",
    "inhib_rates = [5e-5, 1e-5]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 1\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  # \"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_times_concs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "# This seed gave nicely spread out odors easier to learn 0xe329714605b83365e67b44ed7e001ec\n",
    "# Another random seed: 0xb7bf767bbad297aeeee19d0ccdc3647e\n",
    "rgen_meta = np.random.default_rng(seed=0x47cf767aaab807aeeee19d0cfdc3629c)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "\n",
    "# Compute mean of independent underlying variables, \n",
    "# to determine the mean and target covariance of mixed variables\n",
    "tblo, tbhi, twlo, twhi = back_params[2], back_params[3], back_params[0], back_params[1]\n",
    "whiffprob = np.mean(1.0 / (1.0 + np.sqrt(tblo*tbhi/twlo/twhi)))\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*back_params[4:6]))\n",
    "mean_conc = whiffprob * avg_whiff_conc  # average time in whiffs vs blanks * average whiff conc\n",
    "print(\"Analytical mean conc:\", mean_conc)\n",
    "#print(\"Numerical mean conc:\", mean_conc_empirical)\n",
    "\n",
    "# Then add background odor vectors last to that list\n",
    "back_params.append(back_components)\n",
    "\n",
    "# Initial values of background process variables (t, c for each variable)\n",
    "init_concs = sample_ss_conc_powerlaw(*back_params[:-1], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components), *back_params[2:4])\n",
    "tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector \n",
    "init_bkvec = tc_init[:, 1].dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background process example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a dense simulation to extract mixed concentrations for\n",
    "# global correl_rho chosen above (0.7)\n",
    "# Dummy initialization\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])\n",
    "\n",
    "sim_avg_res = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=1, **avg_options\n",
    ")\n",
    "\n",
    "_, bkser_avg, bkvecser_avg, _, _ = sim_avg_res\n",
    "del sim_avg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background vectors time series with mixed concentrations\n",
    "tslice = slice(0, 50000, 200)\n",
    "n_cols = 6\n",
    "n_plots = n_dimensions // 4  # Only show first 24 OSNs\n",
    "n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_cols*1.25, n_rows*1.25)\n",
    "for i in range(n_plots):\n",
    "    ax = axes.flat[i]\n",
    "    ax.scatter(bkvecser_avg[tslice, 2*i+1], bkvecser_avg[tslice, 2*i], \n",
    "               s=9, alpha=0.5, color=\"k\")\n",
    "    for j in range(n_components):\n",
    "        ax.plot(*zip([0.0, 0.0], 3.0*back_components[j, 2*i:2*i+2:][::-1]), lw=2.0)\n",
    "    ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "for i in range(n_plots, n_rows*n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "fig.tight_layout()\n",
    "if do_save_plots:\n",
    "    fig.savefig(pj(\"..\", \"figures\", \"correlation\", \"osn_background_vectors.pdf\"), \n",
    "               transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average dot product between random odors\n",
    "Generate a bunch of odors and bootstrap the average dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap by sampling s_i and s_j with replacement, computing s_i.dot(s_j)\n",
    "n_samp = int(1e5)\n",
    "samp_size = int(1e4)\n",
    "n_boot = 10000\n",
    "boot_dots = np.zeros(n_boot)\n",
    "\n",
    "start_t = perf_counter()\n",
    "odors = generate_odorant((n_samp, n_dimensions), rgen_meta)\n",
    "odors = odors / l2_norm(odors, axis=1)[:, None]\n",
    "for i in range(n_boot):\n",
    "    od_choice_i = rgen_meta.choice(n_samp, size=samp_size, replace=True)\n",
    "    od_choice_j = rgen_meta.choice(n_samp, size=samp_size, replace=True)\n",
    "    odors_sample_i = odors[od_choice_i]\n",
    "    odors_sample_j = odors[od_choice_j]\n",
    "    # Since vectors are unit normed, this is the cosine similarity\n",
    "    dotprods = np.sum(odors_sample_i * odors_sample_j, axis=1)\n",
    "    boot_dots[i] = np.mean(dotprods)\n",
    "end_t = perf_counter()\n",
    "print(\"Finished {} bootstrap repeats with {} samples each in {:.2f} s\".format(\n",
    "    n_boot, samp_size, end_t - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dot = np.mean(boot_dots)\n",
    "vari_dot = np.var(boot_dots, ddof=1)  # unbiased estimator\n",
    "print(\"Mean cosine similarity:\", mean_dot)\n",
    "print(\"Standard dev.:\", np.sqrt(vari_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to generating new odors every time\n",
    "n_boot2 = n_boot // 2\n",
    "samp_size2 = samp_size // 2\n",
    "boot_dots2 = np.zeros(n_boot2)\n",
    "start_t = perf_counter()\n",
    "for i in range(n_boot2):\n",
    "    odors_sample_i = generate_odorant((samp_size2, n_dimensions), rgen_meta)\n",
    "    odors_sample_i = odors_sample_i / l2_norm(odors_sample_i, axis=1)[:, None]\n",
    "    odors_sample_j = generate_odorant((samp_size2, n_dimensions), rgen_meta)\n",
    "    odors_sample_j = odors_sample_j / l2_norm(odors_sample_j, axis=1)[:, None]\n",
    "    dotprods = np.sum(odors_sample_i * odors_sample_j, axis=1)\n",
    "    boot_dots2[i] = np.mean(dotprods)\n",
    "end_t = perf_counter()\n",
    "print(\"Finished {} bootstrap repeats with {} samples each in {:.2f} s\".format(\n",
    "    n_boot2, samp_size2, end_t - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dot2 = np.mean(boot_dots2)\n",
    "vari_dot2 = np.var(boot_dots2, ddof=1)  # unbiased estimator\n",
    "print(\"Mean cosine similarity:\", mean_dot2)\n",
    "print(\"Standard dev. of estimate:\", np.sqrt(vari_dot2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same but for $N_S = 25$ dimensions, which I sometimes use in simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to generating new odors every time\n",
    "boot_dots2_25 = np.zeros(n_boot2)\n",
    "start_t = perf_counter()\n",
    "for i in range(n_boot2):\n",
    "    odors_sample_i = generate_odorant((samp_size2, 25), rgen_meta)\n",
    "    odors_sample_i = odors_sample_i / l2_norm(odors_sample_i, axis=1)[:, None]\n",
    "    odors_sample_j = generate_odorant((samp_size2, 25), rgen_meta)\n",
    "    odors_sample_j = odors_sample_j / l2_norm(odors_sample_j, axis=1)[:, None]\n",
    "    dotprods = np.sum(odors_sample_i * odors_sample_j, axis=1)\n",
    "    boot_dots2_25[i] = np.mean(dotprods)\n",
    "end_t = perf_counter()\n",
    "print(\"Finished {} bootstrap repeats with {} samples each in {:.2f} s\".format(\n",
    "    n_boot2, samp_size2, end_t - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dot2_25 = np.mean(boot_dots2_25)\n",
    "vari_dot2_25 = np.var(boot_dots2_25, ddof=1)  # unbiased estimator\n",
    "print(\"Mean cosine similarity in N_S=25 dimensions:\", mean_dot2_25)\n",
    "print(\"Standard dev. of estimate:\", np.sqrt(vari_dot2_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same drill, but for gamma-distributed vector elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to generating new odors every time\n",
    "start_t = perf_counter()\n",
    "\n",
    "boot_dots_gam = np.zeros(n_boot2)\n",
    "samp_size3 = samp_size2 // 2\n",
    "for i in range(n_boot2):\n",
    "    odors_sample_i = generate_gamma_odorant((samp_size3, n_dimensions), rgen_meta)\n",
    "    odors_sample_j = generate_gamma_odorant((samp_size3, n_dimensions), rgen_meta)\n",
    "    dotprods = np.sum(odors_sample_i * odors_sample_j, axis=1)\n",
    "    norms_ij = [l2_norm(odors_sample_i, axis=1), l2_norm(odors_sample_j, axis=1)]\n",
    "    # Normalize to get cosine similarity\n",
    "    dotprods = dotprods / (norms_ij[0] * norms_ij[1])\n",
    "    boot_dots_gam[i] = np.mean(dotprods)\n",
    "end_t = perf_counter()\n",
    "print(\"Finished {} bootstrap repeats with {} samples each in {:.2f} s\".format(\n",
    "    n_boot2, samp_size3, end_t - start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dot_gam = np.mean(boot_dots_gam)\n",
    "vari_dot_gam = np.var(boot_dots_gam, ddof=1)  # unbiased estimator\n",
    "print(\"Mean cosine similarity:\", mean_dot_gam)\n",
    "print(\"Standard dev. of estimate:\", np.sqrt(vari_dot_gam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
