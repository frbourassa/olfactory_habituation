{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations with OSN adaptation\n",
    "\n",
    "Same model of OSN saturation as in ``nonlinear_osn_turbulent_illustration.ipynb``, \n",
    "\n",
    "$$ s_i(t) = F_\\mathrm{max} \\frac{\\sum_\\gamma K_{i \\gamma} c_\\gamma}{\\exp{(\\epsilon_i(t))} + \\sum_\\gamma K_{i \\gamma} c_\\gamma} $$\n",
    "\n",
    "but with modified IBCM and BioPCA integration functions that promote $\\epsilon_i(t)$ to dynamical variables with feedback from OSN activity $s_i(t)$ and a target amplitude\n",
    "\n",
    "$$ \\frac{\\mathrm{d} \\epsilon_i(t)}{\\mathrm{d} t} = \\frac{1}{\\tau_\\mathrm{a}} \\left( s_i(t) - s_{i, 0} \\right) $$\n",
    "\n",
    "where we also clip (i.e. stop updating beyond this range) $\\epsilon \\in [\\epsilon_L, \\epsilon_H]$ to prevent divergences arising from a continued excess or deficit of OSN activity. In other words, adaptation only occurs on a finite range.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import json\n",
    "from os.path import join as pj\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    ibcm_respond_new_odors,  # Unchanged if we give inputs nonlinearized with correct epsilon\n",
    "    compute_mbars_hgammas_hbargammas,  # Unchanged\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    build_lambda_matrix,  \n",
    "    biopca_respond_new_odors  # Unchanged if we give nonlinearized inputs with correct epsilon\n",
    ")\n",
    "# Do not consider average or idealized subtraction here\n",
    "from modelfcts.ideal import (\n",
    "    compute_optimal_matrix_fromsamples\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning  # Unchanged if give pre-computed nonlinear inputs\n",
    ")\n",
    "from utils.metrics import jaccard, l2_norm\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform,\n",
    "    inverse_transform_tanhcdf\n",
    ")\n",
    "# re-use functions for nonlinear OSNs, will need to put \n",
    "# updated epsilon in back_params at each step\n",
    "from modelfcts.nonlin_adapt_osn import (  \n",
    "    generate_odor_tanhcdf, \n",
    "    combine_odors_affinities, \n",
    "    update_powerlaw_times_concs_affinities,\n",
    ")\n",
    "from modelfcts.backgrounds import (  #\n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw,   # unchanged\n",
    ")\n",
    "from modelfcts.tagging import (  # unchanged\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import moving_average\n",
    "\n",
    "from simulfcts.plotting import (\n",
    "    plot_hbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_plots = False\n",
    "do_save_outputs = False\n",
    "\n",
    "root_dir = pj(\"..\")\n",
    "outputs_folder = pj(root_dir, \"results\", \"for_plots\", \"nonlin_adapt\")\n",
    "panels_folder = pj(root_dir, \"figures\", \"nonlin_adapt\")\n",
    "params_folder = pj(root_dir, \"results\", \"common_params\")\n",
    "\n",
    "# rcParams\n",
    "with open(pj(params_folder, \"olfaction_rcparams.json\"), \"r\") as f:\n",
    "    new_rcParams = json.load(f)\n",
    "plt.rcParams.update(new_rcParams)\n",
    "\n",
    "# color maps\n",
    "with open(pj(params_folder, \"back_colors.json\"), \"r\") as f:\n",
    "    all_back_colors = json.load(f)\n",
    "back_color = all_back_colors[\"back_color\"]\n",
    "back_color_samples = all_back_colors[\"back_color_samples\"]\n",
    "back_palette = all_back_colors[\"back_palette\"]\n",
    "\n",
    "with open(pj(params_folder, \"orn_colors.json\"), \"r\") as f:\n",
    "    orn_colors = json.load(f)\n",
    "    \n",
    "with open(pj(params_folder, \"inhibitory_neuron_two_colors.json\"), \"r\") as f:\n",
    "    neuron_colors = np.asarray(json.load(f))\n",
    "with open(pj(params_folder, \"inhibitory_neuron_full_colors.json\"), \"r\") as f:\n",
    "    neuron_colors_full24 = np.asarray(json.load(f))\n",
    "# Here, 32 neurons, need to make a new palette with same parameters\n",
    "neuron_colors_full = np.asarray(sns.husl_palette(n_colors=32, h=0.01, s=0.9, l=0.4, as_cmap=False))\n",
    "\n",
    "with open(pj(params_folder, \"model_colors.json\"), \"r\") as f:\n",
    "    model_colors = json.load(f)\n",
    "with open(pj(params_folder, \"model_nice_names.json\"), \"r\") as f:\n",
    "    model_nice_names = json.load(f)\n",
    "\n",
    "models = list(model_colors.keys())\n",
    "print(models)\n",
    "    \n",
    "models = list(model_colors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main new simulation functions\n",
    "\n",
    "See functions ``integrate_ibcm_adaptation`` in ``modelfcts.ibcm`` and ``integrate_biopca_adaptation`` in ``modelfcts.biopca``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import integrate_ibcm_adaptation\n",
    "from modelfcts.biopca import integrate_biopca_adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Fly number\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "# Common parameters for toy and full simulations\n",
    "inhib_rates = [0.00005, 0.00001]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "\n",
    "# Simulation skipping, 50 is enough for plots\n",
    "skp = 50 * int(1.0 / deltat)\n",
    "tser_common = np.arange(0.0, duration, deltat*skp)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "combine_fct = combine_odors_affinities\n",
    "update_fct = update_powerlaw_times_concs_affinities\n",
    "\n",
    "# Scale of affinity vectors: default\n",
    "kscale = 5e-4  # default is 5e-4\n",
    "\n",
    "# OSN target activity and epsilon ranges\n",
    "# TODO; maybe adjust given the odor vectors, f_max scale, etc. \n",
    "target_osn_activ = np.full(n_dimensions, 1.0 / np.sqrt(n_dimensions))\n",
    "adaptation_params = [\n",
    "    25.0,  # tau_adapt = 250 ms\n",
    "    1.0,  # eps_min, allow quite low\n",
    "    10.0,  # eps_max\n",
    "    target_osn_activ \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_background_params(n_comp):\n",
    "    \"\"\" Default time and concentration parameters for the turbulent process\"\"\"\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms_turbulent = [\n",
    "        np.asarray([1.0] * n_comp),        # whiff_tmins\n",
    "        np.asarray([500.] * n_comp),       # whiff_tmaxs\n",
    "        np.asarray([1.0] * n_comp),        # blank_tmins\n",
    "        np.asarray([800.0] * n_comp),      # blank_tmaxs\n",
    "        np.asarray([0.6] * n_comp),        # c0s\n",
    "        np.asarray([0.5] * n_comp),        # alphas\n",
    "    ]\n",
    "    return back_pms_turbulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_back_params(adapt_params, rgen, n_comp, n_dim):\n",
    "    # Turbulent background parameters: same rates and constants for all odors\n",
    "    back_pms = default_background_params(n_comp)\n",
    "    \n",
    "    tau_eps, eps_min, eps_max, osn_targets = adapt_params\n",
    "    epsils_vec = np.full(n_dim, 0.5 * (eps_min + eps_max))\n",
    "    back_comps = generate_odor_tanhcdf((n_comp, n_dim), rgen, unit_scale=kscale)\n",
    "\n",
    "    # To keep OSN amplitudes comparable to usual simulations, scale down OSN max. ampli\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Same adjustment of the OSN amplitude as in the performance recognition tests\n",
    "    raw_conc_factor = 2.5\n",
    "    raw_ampli = 2.5\n",
    "    np_statistic = np.mean  # np.mean, np.median, np.amax\n",
    "\n",
    "    raw_osn_activ = np_statistic(combine_fct(np.full(n_comp, raw_conc_factor * avg_whiff_conc), \n",
    "                                        back_comps, epsils_vec, fmax=1.0))\n",
    "    max_osn_ampli = raw_ampli / (raw_osn_activ * np.sqrt(n_dim))\n",
    "\n",
    "    # Add these extra parameters to the list of background params\n",
    "    back_pms.append(max_osn_ampli)\n",
    "    back_pms.append(epsils_vec)\n",
    "    back_pms.append(back_comps)\n",
    "\n",
    "    # Initialization\n",
    "    # Initial values of background process variables (t, c for each variable)\n",
    "    init_concs = sample_ss_conc_powerlaw(*back_pms[:-3], size=1, rgen=rgen)\n",
    "    init_times = powerlaw_cutoff_inverse_transform(\n",
    "                    rgen.random(size=n_comp), *back_pms[2:4])\n",
    "    tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "    # Initial background vector: combine odors with the tc_init concentrations\n",
    "    init_bkvec = combine_fct(tc_init[:, 1], back_comps, epsils_vec, fmax=max_osn_ampli)\n",
    "    # nus are first in the list of initial background params\n",
    "    init_back = [tc_init, init_bkvec]\n",
    "    \n",
    "    return back_pms, init_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global IBCM parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ibcm_simulation_adapt(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp, n_i=None):\n",
    "    print(\"Initializing IBCM simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background with the random generator with seed rgenseed\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "    if n_i is None:\n",
    "        n_i = n_comp * 4\n",
    "    # Initial synaptic weights: small positive noise\n",
    "    init_synapses_ibcm = 0.2*rgen.standard_normal(size=[n_i, n_dim])*lambd_ibcm\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting IBCM simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_ibcm_adaptation(\n",
    "                init_synapses_ibcm, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params_local, \n",
    "                adapt_params, duration, deltat, seed=simseed, \n",
    "                noisetype=\"uniform\",  skp=skp_local, **ibcm_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished IBCM simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_odors_in_manifold(back_pms, conc_ser, new_conc_rel, rgen, n_ex=2, n_samp=10):\n",
    "    \"\"\"Mix n_ex new odors with n_samp background samples each. \n",
    "    Returns a 3d-array of mixtures, indexed [n_ex, n_samp, n_dim], \n",
    "    and the new odor vectors, a 2d array indexed [n_ex, n_dim]. \n",
    "    \"\"\"\n",
    "    back_odors = back_pms[-1]\n",
    "    n_comp, n_dim = back_odors.shape[0], back_odors.shape[1]\n",
    "    max_ampli = back_pms[-3]\n",
    "    new_odors = generate_odor_tanhcdf((n_ex, n_dim), rgen, unit_scale=kscale)\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_conc = avg_whiff_conc * new_conc_rel\n",
    "    non_null_concs = conc_ser[np.any(conc_ser > 0.0, axis=1)]\n",
    "    epsils_vec = back_pms[-2]\n",
    "    back_concs = non_null_concs[rgen.choice(non_null_concs.shape[0], size=n_ex*n_samp, replace=True)]\n",
    "    back_concs = back_concs.reshape(n_ex, n_samp, n_comp)\n",
    "    all_mixed_samples = []\n",
    "    for i in range(n_ex):\n",
    "        joint_kmats = np.concatenate([back_odors, new_odors[i:i+1]], axis=0)\n",
    "        mixed_samples_i = []\n",
    "        for j in range(n_samp):\n",
    "            joint_concs = np.concatenate([back_concs[i, j:j+1], np.full((1, 1), new_conc)], axis=1)\n",
    "            mixed_samples_i.append(combine_fct(joint_concs, joint_kmats, epsils_vec, fmax=max_ampli))\n",
    "        mixed_samples_i = np.concatenate(mixed_samples_i, axis=0)\n",
    "        all_mixed_samples.append(mixed_samples_i)\n",
    "        \n",
    "    return np.stack(all_mixed_samples, axis=0), new_odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def analyze_clean_ibcm_simul(results_raw, back_pms, rgenseed, n_ex=2, n_samp=10, t_mix=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_raw = (tser_ibcm, nuser_ibcm, bkvecser_ibcm, mser_ibcm, \n",
    "            cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm)\n",
    "    Returns:\n",
    "        cbars_gamma, wser_ibcm, bkvecser_ibcm, \n",
    "            yser_ibcm, moments_conc, cgammas_bar_counts, specif_gammas, correl_c_conc\n",
    "    \"\"\"\n",
    "    (tser_ibcm, nuser_ibcm, bkvecser_ibcm, eps_ser, mser_ibcm, \n",
    "        cbarser_ibcm, thetaser_ibcm, wser_ibcm, yser_ibcm) = results_raw\n",
    "    # Calculate cgammas_bar and mbars\n",
    "    transient = int(5/6*duration / deltat) // skp\n",
    "    back_components = back_pms[-1]\n",
    "    basis = back_components / l2_norm(back_components, axis=1)[:, None] \n",
    "    \n",
    "    # Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "    mbarser, c_gammas, cbars_gamma = compute_mbars_hgammas_hbargammas(\n",
    "                                mser_ibcm, coupling_eta_ibcm, basis)\n",
    "    \n",
    "    # Moments of concentrations\n",
    "    conc_ser = nuser_ibcm[:, :, 1]\n",
    "    mean_conc = np.mean(conc_ser)\n",
    "    sigma2_conc = np.var(conc_ser)\n",
    "    thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "    moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "\n",
    "    # Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "    cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "    specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "    \n",
    "    cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "    conc_ser_centered = conc_ser - np.mean(conc_ser[transient:], axis=0)\n",
    "    correl_c_conc = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "    \n",
    "    ysernorm_ibcm = l2_norm(yser_ibcm, axis=1)\n",
    "    \n",
    "    # Examples of mixing new odors with the background\n",
    "    rgen = np.random.default_rng(np.random.SeedSequence(rgenseed).spawn(2)[1])\n",
    "    back_pms_local = list(back_pms)\n",
    "    back_pms_local[-2] = eps_ser[t_mix]\n",
    "    mixres = mix_new_odors_in_manifold(back_pms_local, conc_ser, 1.0, rgen, n_ex=n_ex, n_samp=n_samp)\n",
    "    mixed_samples, new_odors = mixres\n",
    "    results_clean = (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, moments_conc, \n",
    "                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "                     conc_ser, mixed_samples, new_odors)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "# Plotting functions for IBCM\n",
    "def plot_ibcm_results(res_ibcm_raw, res_ibcm_clean):\n",
    "    (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, \n",
    "         moments_conc, cbars_gamma_mean, specif_gammas, correl_c_conc, \n",
    "         back_comps, conc_ser, _, _) = res_ibcm_clean\n",
    "\n",
    "    # Plot of cbars gamma series\n",
    "    fig , ax, _ = plot_hbars_gamma_series(tser_common, cbars_gamma, \n",
    "                            skp=2, transient=320000 // skp)\n",
    "    fig.tight_layout()\n",
    "    leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plots of neuron specificities\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(correl_c_conc.T)\n",
    "    ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "    fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "                 r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "                location=\"top\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Check if each component has at least one neuron\n",
    "    print(\"Odor specificities:\", specif_gammas)\n",
    "    split_val = 2.5\n",
    "    for comp in range(n_components):\n",
    "        print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))\n",
    "\n",
    "    # Plot of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_ibcm_raw[2], res_ibcm_raw[8], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # TODO: plot epsilon vector series?\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run and clean a simulation\n",
    "\n",
    "Uses global BioPCA parameters defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_biopca_simulation_adapt(adapt_params, n_comp, n_dim, rgenseed, simseed, skp_local=skp):\n",
    "    print(\"Initializing BioPCA simulation for adapt_params[:3] =\", adapt_params[:3])\n",
    "    # Initialize background parameters, give same rgenseed as IBCM to have same background\n",
    "    rgen = np.random.default_rng(rgenseed)\n",
    "    res = initialize_back_params(adapt_params, rgen, n_comp, n_dim)\n",
    "    back_params_local, init_back = res\n",
    "        \n",
    "    init_synapses_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen.standard_normal(size=[n_i_pca, n_dim]) / np.sqrt(n_dim)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run the IBCM simulation\n",
    "    print(\"Starting BioPCA simulation...\")\n",
    "    tstart = perf_counter()\n",
    "    sim_results = integrate_biopca_adaptation(\n",
    "                ml_inits_pca, update_fct, init_back, biopca_rates, \n",
    "                inhib_rates, back_params_local, adapt_params, duration, deltat, \n",
    "                seed=simseed, noisetype=\"uniform\", skp=skp_local, **pca_options\n",
    "    )\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished BioPCA simulation in {:.2f} s\".format(tend - tstart))\n",
    "    \n",
    "    return back_params_local, sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clean_biopca_simul(results_raw):\n",
    "    \"\"\"\n",
    "    We do not need to save odor vectors (back_components), \n",
    "    since the IBCM simulation will provide them for both models. \n",
    "    \n",
    "    Args:\n",
    "        results_raw = (tser_pca, nuser_pca, bkvecser_pca, mser_pca, \n",
    "            lser_pca, xser_pca, cbarser_pca, wser_pca, yser_pca)\n",
    "    Returns:\n",
    "        bkvecser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "            learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    \"\"\"\n",
    "    (tser_pca, nuser_pca, bkvecser_pca, eps_ser, mser_pca, lser_pca, xser_pca, \n",
    "         cbarser_pca, wser_pca, yser_pca) = results_raw\n",
    "    \n",
    "    # Analyze versus true offline PCA of the background samples\n",
    "    print(\"Starting analysis of BioPCA vs true PCA\")\n",
    "    tstart = perf_counter()\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, _, off_diag_l_avg_abs, align_error_ser = res\n",
    "    tend = perf_counter()\n",
    "    print(\"Completed analysis in {:.1f} s\".format(tend - tstart))\n",
    "    \n",
    "    ysernorm_pca = l2_norm(yser_pca, axis=1)\n",
    "    bkvecsernorm_pca = l2_norm(bkvecser_pca, axis=1)\n",
    "    \n",
    "    # Also save info about background vs yser_pca\n",
    "    results_clean = (bkvecsernorm_pca, eps_ser, ysernorm_pca, wser_pca,\n",
    "                     true_pca, learnt_pca, off_diag_l_avg_abs, align_error_ser)\n",
    "    return results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_biopca_results(res_biopca_raw, res_biopca_clean):\n",
    "    (bkvecsernorm_pca, epser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "    learnt_pca, off_diag_l_avg_abs, align_error_ser) = res_biopca_clean\n",
    "\n",
    "    # Plot learnt vs true PCA\n",
    "    fig, axes = plot_pca_results(tser_common/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "    axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "    axes[0].get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(fig.get_size_inches()[0], 2.5*plt.rcParams[\"figure.figsize\"][1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot level of background inhibition\n",
    "    fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                    tser_common, res_biopca_raw[2], res_biopca_raw[9], skp=2)\n",
    "\n",
    "    # Compute noise reduction factor, annotate\n",
    "    transient = 250000 // skp\n",
    "    norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "    print(\"Mean activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "    print(\"Standard deviation of activity norm reduced to \"\n",
    "          + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "    ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "               xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "    ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal $P$ linear manifold learning matrix\n",
    "Compute average across a background time series, using the adapted $\\epsilon(t)$ value going with each background sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_new_back_adapt(back_odors, new_odors, cser, newconc, fmax, eps_ser):\n",
    "    n_new = new_odors.shape[0]\n",
    "    assert n_new == cser.shape[0]  # one new odor per back sample\n",
    "    all_mixvecs = []\n",
    "    for n in range(n_new):\n",
    "        joint_concs = np.concatenate([cser[n], np.full(1, newconc)])\n",
    "        joint_components = np.concatenate(\n",
    "            [back_odors, new_odors[n:n+1]], axis=0)\n",
    "        mixvecs = combine_fct(joint_concs, \n",
    "                    joint_components, eps_ser[n], fmax=fmax)\n",
    "        all_mixvecs.append(mixvecs)\n",
    "    mixvecs = np.stack(all_mixvecs, axis=0)\n",
    "    return mixvecs\n",
    "\n",
    "\n",
    "def get_optimal_mat_p(bkvecser, concser, eps_ser, back_pms, new_concs_rel, \n",
    "                      sd=0xdf8cc55ff9195d82ed83ae87ba4e10fc):\n",
    "    \"\"\" Compute the optimal linear manifold learning matrix P, \n",
    "    using a previously simulated background\"\"\"\n",
    "    avg_whiff_conc = np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    new_concs = avg_whiff_conc * new_concs_rel\n",
    "    osn_ampli = back_pms[-3]\n",
    "    back_comp = back_pms[-1]\n",
    "\n",
    "    # Compute optimal W matrix for all new odors possible\n",
    "    # Need samples from the background (use provided bkser)\n",
    "    # and samples from mixtures of background + new odor\n",
    "    # (generate from back. conc. series in nuser_ibcm)\n",
    "    dummy_rgen = np.random.default_rng(sd)\n",
    "    # New odors, each with a subset of the background samples\n",
    "    n_samp, n_dims = bkvecser.shape[0], bkvecser.shape[1]\n",
    "    new_odors_from_distrib = generate_odor_tanhcdf(\n",
    "        [n_samp, n_dims], dummy_rgen, unit_scale=kscale)\n",
    "\n",
    "    optimal_matrices = []\n",
    "    for newconc in new_concs:\n",
    "        # Mix new odors at newconc with background\n",
    "        s_new_mix = mix_new_back_adapt(back_comp, new_odors_from_distrib, \n",
    "                                 concser, newconc, osn_ampli, eps_ser)\n",
    "        mat = compute_optimal_matrix_fromsamples(bkvecser, s_new_mix)\n",
    "        optimal_matrices.append(mat)\n",
    "\n",
    "    return optimal_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New odor recognition functions\n",
    "For now, forget average subtraction, optimal $P$ learning, and orthogonal habituation. Just try to answer the question: when OSNs adapt, is it still necessary to habituate with manifold learning to improve new odor recognition? \n",
    "\n",
    "In other words, the question is not so much \"does manifold learning still work with OSN adaptation?\", but rather \"is manifold learning still needed with OSN adaptation?\". \n",
    "\n",
    "Compare no habituation without adaptation (setting $\\epsilon$ to the average in each OSN during the simulation), no habituation with adaptation, and the IBCM / BioPCA models with adaptation. \n",
    "\n",
    "### Technical remarks\n",
    "\n",
    "Since OSN response adapts over time, we test odor recognition at multiple time points, using only the background sample from the simulated instance (to which OSNs have adapted) but not extra samples (to which OSNs would not have adapted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IBCM and BioPCA\n",
    "def test_odor_recognition_adaptation(\n",
    "        back_pms, new_od_kmats, ibcm_res, biopca_res, \n",
    "        opts_ibcm, opts_biopca, proj_mat, proj_args,\n",
    "        rates_ibcm, rates_pca, optim_mat, \n",
    "        new_conc_rel=1.0, n_test_t=100, n_new=100):\n",
    "    # Select test times, etc.\n",
    "    bk_comp = back_pms[-1]\n",
    "    n_comp, n_dim = bk_comp.shape[0], bk_comp.shape[1]\n",
    "    n_kc = proj_mat.shape[0]\n",
    "    \n",
    "    # New odors tested\n",
    "    new_conc = new_conc_rel * np.mean(truncexp1_average(*back_pms[4:6]))\n",
    "    \n",
    "    # Load background and epsilon series, assert it's the same background for both models\n",
    "    conc_ser = ibcm_res[1][:, :, 1]  # concentrations\n",
    "    conc_ser_pca = biopca_res[1][:, :, 1]\n",
    "    assert np.allclose(conc_ser, conc_ser_pca)\n",
    "    eps_ser = ibcm_res[3]\n",
    "    eps_ser_pca = biopca_res[3]\n",
    "    assert np.allclose(eps_ser, eps_ser_pca)\n",
    "    \n",
    "    # Time series of relevant weights\n",
    "    mser_ibcm = ibcm_res[4]\n",
    "    wser_ibcm = ibcm_res[7]\n",
    "    mser_pca = biopca_res[4]\n",
    "    lser_pca = biopca_res[5]\n",
    "    xser_pca = biopca_res[6]\n",
    "    wser_pca = biopca_res[8]\n",
    "    \n",
    "    # Reference tags are computed for the average epsilon of each OSN\n",
    "    # TODO: consider recomputing them at each instantaneous tested epsilon, \n",
    "    # since epsilon adaptation changes the tag that the new odor alone would generate; \n",
    "    # see which is better later. \n",
    "    default_eps = np.mean(eps_ser, axis=0)  # average epsilon for each OSN\n",
    "    single_mean_eps = np.full(n_dim, np.mean(default_eps))\n",
    "    osn_ampli = back_pms[-3]\n",
    "\n",
    "    # OSN response to new odors and back odors at average conc.\n",
    "    new_odor_responses = np.stack([combine_fct(np.asarray([new_conc]), new_od_kmats[i:i+1], \n",
    "            default_eps, fmax=osn_ampli) for i in range(n_new)])\n",
    "    back_odor_responses = np.stack([combine_fct(np.asarray([new_conc]), bk_comp[i:i+1], \n",
    "            default_eps, fmax=osn_ampli) for i in range(n_comp)])\n",
    "\n",
    "    # Test times, based on global parameters (duration, deltat, skp)\n",
    "    start_test_t = duration - 30000.0\n",
    "    test_times = np.linspace(start_test_t, duration, n_test_t)\n",
    "    test_times -= deltat*skp\n",
    "    test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "    # Containers for y vectors of each model in response to the mixture\n",
    "    models = [\"none\", \"adapt\", \"optimal_adapt\", \"biopca_adapt\", \"ibcm_adapt\"]\n",
    "    mixture_yvecs = {a: np.zeros([n_new, n_test_t, n_dim]) \n",
    "                    for a in models}\n",
    "    mixture_tags = {a: SparseNDArray((n_new, n_test_t, n_kc), dtype=bool) \n",
    "                    for a in models}\n",
    "    new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "    jaccard_scores = {a: np.zeros([n_new, n_test_t]) \n",
    "                      for a in models}\n",
    "    jaccard_backs = {a: np.zeros([n_new, n_test_t]) \n",
    "                      for a in models}\n",
    "    back_tags = [project_neural_tag(b, b, proj_mat, **proj_args) \n",
    "                 for b in back_odor_responses]\n",
    "    \n",
    "    # Assess recognition of new odors mixed non-linearly\n",
    "    for i in range(n_new):\n",
    "        # Compute neural tag of the new odor alone, without inhibition\n",
    "        new_tag = project_neural_tag(\n",
    "                        new_odor_responses[i], new_odor_responses[i],\n",
    "                        proj_mat, **proj_args\n",
    "                    )\n",
    "        new_odor_tags[i, list(new_tag)] = True\n",
    "\n",
    "        # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "        # compute the PN response at each test concentration,\n",
    "        # compute tags too, and save results\n",
    "        # Combine background and new odor i's parameters into one joint K matrix\n",
    "        # to use in the combine_fct. \n",
    "        joint_odor_kmats = np.concatenate([bk_comp, new_od_kmats[i:i+1]], axis=0)\n",
    "        for j in range(n_test_t):\n",
    "            current_eps = eps_ser[j]\n",
    "            jj = test_idx[j]\n",
    "            joint_conc_samples = np.concatenate(\n",
    "                [conc_ser[j], np.full(1, new_conc)], axis=0)\n",
    "            mixture = combine_fct(joint_conc_samples, joint_odor_kmats, \n",
    "                                   current_eps, fmax=osn_ampli)\n",
    "            mixture_noadapt = combine_fct(joint_conc_samples, joint_odor_kmats, \n",
    "                                   single_mean_eps, fmax=osn_ampli)\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_yvecs[\"ibcm_adapt\"][i, j] = ibcm_respond_new_odors(\n",
    "                mixture, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                rates_ibcm, options=opts_ibcm\n",
    "            )\n",
    "            mixture_yvecs[\"biopca_adapt\"][i, j] = biopca_respond_new_odors(\n",
    "                mixture, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], rates_pca, options=opts_biopca\n",
    "            )\n",
    "            mixture_yvecs[\"adapt\"][i, j] = mixture\n",
    "            mixture_yvecs[\"none\"][i, j] = mixture_noadapt\n",
    "            mixture_yvecs[\"optimal_adapt\"][i, j] = mixture - optim_mat.dot(mixture)\n",
    "            #mixture_yvecs[\"orthogonal\"][i, j] = mixtures - mixtures.dot(back_projector.T)\n",
    "            for mod in mixture_yvecs.keys():\n",
    "                mix_tag = project_neural_tag(\n",
    "                    mixture_yvecs[mod][i, j], mixture,\n",
    "                    proj_mat, **proj_args\n",
    "                )\n",
    "                try:\n",
    "                    mixture_tags[mod][i, j, list(mix_tag)] = True\n",
    "                except ValueError as e:\n",
    "                    print(mix_tag)\n",
    "                    print(mixture_yvecs[mod][i, j])\n",
    "                    print(proj_mat.dot(mixture_yvecs[mod][i, j]))\n",
    "                    raise e\n",
    "                jaccard_scores[mod][i, j] = jaccard(mix_tag, new_tag)\n",
    "                jaccard_backs[mod][i, j] = max((jaccard(mix_tag, b) for b in back_tags))\n",
    "    return jaccard_scores, jaccard_backs, mixture_tags, new_odor_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional plotting functions, to visualize the manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will make similar plots, define functions\n",
    "def plot_manifold(bkser, bkvecs, conc_ser, view_params, \n",
    "                  mixed_new_odors=None, new_odor_vec=None, dims=(0, 1, 2)):\n",
    "    # Plot 2D manifold in a 3D slice,\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    # Too many combinations for 6 odors, maybe just highlight\n",
    "    # single-odor axes\n",
    "    where_each = (conc_ser > 0).astype(bool)\n",
    "    n_odors = where_each.shape[1]\n",
    "    locations = {}\n",
    "    # Track places with 0 or 1 odor\n",
    "    any_single_odor = np.all(where_each == False, axis=1)  # start with places with 0 odor\n",
    "    for i in range(n_odors):\n",
    "        mask = np.zeros((1, n_odors), dtype=bool)\n",
    "        mask[0, i] = True\n",
    "        locations[\"Odor {}\".format(i)] = np.all(where_each == mask, axis=1)\n",
    "        any_single_odor += locations[\"Odor {}\".format(i)]  # add places with odor i only\n",
    "    locations[\"2+ odors\"] = np.logical_not(any_single_odor)  # 2+ odors anywhere else\n",
    "    single_odor_colors = sns.color_palette(\"colorblind\", n_colors=n_odors)\n",
    "    all_colors = {\"Odor {}\".format(i): single_odor_colors[i] for i in range(n_odors)}\n",
    "    all_colors[\"2+ odors\"] = \"grey\"\n",
    "    \n",
    "    orig = np.zeros([3, 6])\n",
    "    locations_order = [\"2+ odors\"] + [\"Odor {}\".format(i) for i in range(n_odors)]\n",
    "    for lbl in locations_order:\n",
    "        alpha = 0.3 if lbl.startswith(\"2+\") else 1.0\n",
    "        slc = locations[lbl]\n",
    "        tskp = 5 if lbl.startswith(\"2+\") else 1\n",
    "        zshift = 0.03 if lbl.startswith(\"2+\") else 0.0\n",
    "        shift = 0.03 if lbl.startswith(\"2+\") else 0.0\n",
    "        lbl_append = \"\"# if lbl.startswith(\"2+\") else \" alone\"\n",
    "        bk_subset = [bkser[slc, d].copy() + shift for d in dims]\n",
    "        bk_subset[2] -= zshift\n",
    "        ax.scatter(bk_subset[0][::tskp], bk_subset[1][::tskp], bk_subset[2][::tskp], \n",
    "                   s=4, lw=0.3, label=lbl+lbl_append, color=all_colors[lbl], alpha=alpha)\n",
    "    vecs = bkvecs / l2_norm(bkvecs, axis=1)[:, None]\n",
    "    print(vecs.shape)\n",
    "    ax.quiver(*orig, *(vecs[:, dims].T), color=\"k\", lw=1.5, arrow_length_ratio=0.2)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=25)\n",
    "    \n",
    "    # Also show what adding a new odor can do -- out of the manifold?\n",
    "    new_odor_lbl = \"+ new odor\"\n",
    "    if mixed_new_odors is not None:\n",
    "        n_new_odors = mixed_new_odors.shape[0]\n",
    "        new_odors_palette = sns.dark_palette(\"r\", n_colors=n_new_odors+1)[1:]\n",
    "        for i in range(n_new_odors):\n",
    "            lbl = new_odor_lbl + \" {}\".format(\"abcdefghijklmnop\".upper()[i])\n",
    "            all_colors[lbl] = new_odors_palette[i]\n",
    "            ax.scatter(mixed_new_odors[i, :, dims[0]], mixed_new_odors[i, :, dims[1]], \n",
    "                        mixed_new_odors[i, :, dims[2]], s=6, lw=0.3, \n",
    "                        label=lbl+lbl_append, color=all_colors[lbl], alpha=1.0)\n",
    "            if new_odor_vec is not None:\n",
    "                vec = new_odor_vec[i] / l2_norm(new_odor_vec[i])\n",
    "                ax.quiver(*orig, *(vec[list(dims)]), color=all_colors[lbl], \n",
    "                          lw=1.5, arrow_length_ratio=0.2)\n",
    "    # No new odors shown\n",
    "    else:\n",
    "        n_new_odors = 0\n",
    "        \n",
    "\n",
    "    # Labeling\n",
    "    for lbl, f in enumerate([ax.set_xlabel, ax.set_ylabel, ax.set_zlabel]):\n",
    "        # z label gets caught in the zlbl variable at the last iteration\n",
    "        zlbl = f(\"OSN {} (of {})\".format(lbl+1, bkser.shape[1]), labelpad=-17.5)\n",
    "    for f in [ax.set_xticks, ax.set_yticks, ax.set_zticks]:\n",
    "        f([])\n",
    "    for f in [ax.set_xticklabels, ax.set_yticklabels, ax.set_zticklabels]:\n",
    "        f([], pad=0.1)\n",
    "    view_params.setdefault(\"azim\", 240)\n",
    "    view_params.setdefault(\"elev\", 3)\n",
    "    ax.view_init(**view_params)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # Move the label for 2+ odors to before the new odors\n",
    "    if n_new_odors > 0:\n",
    "        handles.insert(-n_new_odors-1, handles[0])\n",
    "        labels.insert(-n_new_odors-1, labels[0])\n",
    "    else:\n",
    "        handles.append(handles[0])\n",
    "        labels.append(labels[0])\n",
    "    handles.pop(0)\n",
    "    labels.pop(0)\n",
    "    leg = ax.legend(handles=handles, labels=labels, \n",
    "        frameon=True, ncol=1, loc=\"upper left\", bbox_to_anchor=(0.85, 1.0), \n",
    "        title=\"Odor presence\", title_fontsize=6)\n",
    "    #loc=\"upper right\", bbox_to_anchor=(0.0, 1.0), frameon=False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Need to adjust the tightbox to remove whitespace above and below manually. \n",
    "    #ax.set_aspect(\"equal\")\n",
    "    fig.tight_layout()\n",
    "    tightbox = fig.get_tightbbox()\n",
    "    tightbox._bbox.y0 = tightbox._bbox.y0*1.1   #bottom\n",
    "    tightbox._bbox.y1 = tightbox._bbox.y1 + 0.7*tightbox._bbox.y0  # top\n",
    "    tightbox._bbox.x0 = tightbox._bbox.x0 * 0.6  # position of left side\n",
    "\n",
    "    return fig, ax, tightbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the background process\n",
    "def pairplots_background(bkser, bkvecs, epsser=None, mixed_new_odors=None, new_odor_vec=None):\n",
    "    # Background vectors time series with mixed concentrations\n",
    "    tslice = slice(0, None, 30)\n",
    "    \n",
    "    # Scale odor affinities K_{i \\gamma} by the average saturation threshold \n",
    "    # of the OSN, exp(\\epsilon_i), to get the effective affinity scale, \n",
    "    # before normalizing each odor vector K_\\gamma\n",
    "    vecs = bkvecs / l2_norm(bkvecs, axis=1)[:, None]\n",
    "    if epsser is not None:\n",
    "        mean_eps = np.mean(epsser, axis=0)\n",
    "        vecs_eff = bkvecs / np.exp(mean_eps[None, :])\n",
    "        vecs_eff = vecs_eff / l2_norm(vecs_eff, axis=1)[:, None]\n",
    "    else:\n",
    "        vecs_eff = None\n",
    "        \n",
    "    n_comp = bkvecs.shape[0]\n",
    "    n_cols = 6\n",
    "    n_plots = 48 // 2\n",
    "    n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "    fig.set_size_inches(n_cols*1.0, n_rows*1.0)\n",
    "    single_odor_colors = sns.color_palette(\"colorblind\", n_colors=n_comp)\n",
    "    all_colors = {\"Odor {}\".format(i): single_odor_colors[i] for i in range(n_comp)}\n",
    "    if mixed_new_odors is not None:\n",
    "        n_new_odors = mixed_new_odors.shape[0]\n",
    "        new_odors_palette = sns.dark_palette(\"r\", n_colors=n_new_odors+1)[1:]\n",
    "    for i in range(n_plots):\n",
    "        ax = axes.flat[i]\n",
    "        ax.scatter(bkser[tslice, 2*i+1], bkser[tslice, 2*i], \n",
    "                   s=9, alpha=0.5, color=\"k\")\n",
    "        for j in range(n_comp):\n",
    "            ax.plot(*zip([0.0, 0.0], vecs[j, 2*i:2*i+2][::-1]), lw=1.5, \n",
    "                    color=single_odor_colors[j])\n",
    "            if epsser is not None:\n",
    "                ax.plot(*zip([0.0, 0.0], vecs_eff[j, 2*i:2*i+2][::-1]), lw=1.5, ls=\"--\",\n",
    "                    color=single_odor_colors[j])\n",
    "        if mixed_new_odors is not None:\n",
    "            for j in range(n_new_odors):\n",
    "                clr = new_odors_palette[j]\n",
    "                ax.scatter(mixed_new_odors[j, :, 2*i+1], mixed_new_odors[j, :, 2*i], \n",
    "                       s=6, alpha=1.0, color=clr)\n",
    "        if new_odor_vec is not None:\n",
    "            for j in range(n_new_odors):\n",
    "                vec = new_odor_vec[j] / l2_norm(new_odor_vec[j])\n",
    "                ax.plot(*zip([0.0, 0.0], vec[2*i:2*i+2][::-1]), lw=2.0, \n",
    "                    color=new_odors_palette[j])\n",
    "        ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        axes.flat[i].set_axis_off()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axes zoom effect from Matplotlib documentation\n",
    "from matplotlib.transforms import (Bbox, TransformedBbox,\n",
    "                                   blended_transform_factory)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (BboxConnector,\n",
    "                                                   BboxConnectorPatch,\n",
    "                                                   BboxPatch)\n",
    "def connect_bbox(bbox1, bbox2,\n",
    "                 loc1a, loc2a, loc1b, loc2b,\n",
    "                 prop_lines, prop_patches=None):\n",
    "    if prop_patches is None:\n",
    "        prop_patches = {\n",
    "            **prop_lines,\n",
    "            \"alpha\": prop_lines.get(\"alpha\", 1) * 0.2,\n",
    "            \"clip_on\": False,\n",
    "        }\n",
    "\n",
    "    c1 = BboxConnector(\n",
    "        bbox1, bbox2, loc1=loc1a, loc2=loc2a, clip_on=False, **prop_lines)\n",
    "    c2 = BboxConnector(\n",
    "        bbox1, bbox2, loc1=loc1b, loc2=loc2b, clip_on=False, **prop_lines)\n",
    "\n",
    "    bbox_patch1 = BboxPatch(bbox1, **prop_patches, color=\"grey\")\n",
    "    bbox_patch2 = BboxPatch(bbox2, **prop_patches, color=\"grey\")\n",
    "\n",
    "    p = BboxConnectorPatch(bbox1, bbox2,\n",
    "                           loc1a=loc1a, loc2a=loc2a, loc1b=loc1b, loc2b=loc2b,\n",
    "                           clip_on=False,\n",
    "                           **prop_patches)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p\n",
    "\n",
    "def zoom_effect01(ax1, ax2, xmin, xmax, **kwargs):\n",
    "    \"\"\"\n",
    "    Connect *ax1* and *ax2*. The *xmin*-to-*xmax* range in both Axes will\n",
    "    be marked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax1\n",
    "        The main Axes.\n",
    "    ax2\n",
    "        The zoomed Axes.\n",
    "    xmin, xmax\n",
    "        The limits of the colored area in both plot Axes.\n",
    "    **kwargs\n",
    "        Arguments passed to the patch constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    bbox = Bbox.from_extents(xmin, 0, xmax, 1)\n",
    "\n",
    "    mybbox1 = TransformedBbox(bbox, ax1.get_xaxis_transform())\n",
    "    mybbox2 = TransformedBbox(bbox, ax2.get_xaxis_transform())\n",
    "\n",
    "    prop_patches = {**kwargs, \"ec\": \"none\", \"alpha\": 0.2}\n",
    "\n",
    "    c1, c2, bbox_patch1, bbox_patch2, p = connect_bbox(\n",
    "        mybbox1, mybbox2,\n",
    "        loc1a=3, loc2a=2, loc1b=4, loc2b=1,\n",
    "        prop_lines=kwargs, prop_patches=prop_patches)\n",
    "\n",
    "    #ax1.add_patch(bbox_patch1)\n",
    "    ax2.add_patch(bbox_patch2)\n",
    "    ax2.add_patch(c1)\n",
    "    ax2.add_patch(c2)\n",
    "    ax2.add_patch(p)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what individual OSN epsilons are doing on fast and long time scales\n",
    "def plot_eps_series(eps_ser, n_shown, tzoom_interv, skp_n=1, smoothsize=201):\n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    axes = [fig.add_subplot(gs[0, :3])]\n",
    "    axes.append(fig.add_subplot(gs[1, :3], sharey=axes[0]))\n",
    "    axleg = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])\n",
    "    eps_ser_smooth = moving_average(eps_ser, kernelsize=smoothsize, boundary=\"free\")\n",
    "    tscale = deltat * 10.0 / 1000.0 / 60.0  # minutes\n",
    "\n",
    "    osn_colors = sns.cubehelix_palette(n_colors=n_shown, \n",
    "            start=0.0, rot=1.0, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=True)\n",
    "\n",
    "    tsl_local = slice(*tzoom_interv, 1)  # limit * skp * 10 = milliseconds, skp=50 default\n",
    "    tsl_global = slice(0, None, 20)\n",
    "    n_labels = 6\n",
    "    skp_lbl = (n_shown // skp_n) // n_labels\n",
    "    for i in range(0, n_shown, skp_n):\n",
    "        lbl = \"{}\".format(i) if (i // skp_n) % skp_lbl == 0 else \"\"\n",
    "        axes[0].plot(tser_common[tsl_local]*tscale, eps_ser[tsl_local, i], \n",
    "                     alpha=0.7, lw=0.75, color=osn_colors[i], label=lbl)\n",
    "        axes[1].plot(tser_common[tsl_global]*tscale, eps_ser_smooth[tsl_global, i], \n",
    "                     alpha=0.7, lw=0.75, color=osn_colors[i])\n",
    "\n",
    "    axes[0].set(ylabel=r\"$\\epsilon_i(t)$\")\n",
    "    axes[1].set(xlabel=\"Time (min)\", ylabel=r\"Smoothed $\\epsilon_i(t)$\")\n",
    "    axleg.legend(*axes[0].get_legend_handles_labels(), frameon=False, title=\"OSN\")\n",
    "    axleg.set_axis_off()\n",
    "    t1, t2 = tser_common[tzoom_interv[0]]*tscale, tser_common[tzoom_interv[1]]*tscale\n",
    "    zoom_effect01(axes[0], axes[1], t1, t2, lw=0.8)\n",
    "    fig.tight_layout(h_pad=-0.1)\n",
    "    return fig, axes, axleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific model names and colors\n",
    "model_nice_names.update({\n",
    "    \"ibcm_adapt\": \"IBCM + adapt.\",\n",
    "    \"biopca_adapt\": \"BioPCA + adapt.\",\n",
    "    \"adapt\": \"OSN adaptation\",\n",
    "    \"optimal_adapt\": \"Optim. $P$ + adapt.\"\n",
    "})\n",
    "model_colors.update({\n",
    "    \"ibcm_adapt\": model_colors.get(\"ibcm\"),\n",
    "    \"biopca_adapt\": model_colors.get(\"biopca\"),\n",
    "    \"adapt\": \"xkcd:purple\",\n",
    "    \"optimal_adapt\": model_colors.get(\"optimal\")\n",
    "})\n",
    "\n",
    "# Plot jaccard similarities\n",
    "def plot_jaccards(jac_scores):\n",
    "    # Plot model histogram results for one new odor concentration\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(plt.rcParams[\"figure.figsize\"][0]*1.3, \n",
    "                       plt.rcParams[\"figure.figsize\"][1])\n",
    "    models = [m for m in [\"none\", \"adapt\", \"optimal_adapt\", \"biopca_adapt\", \"ibcm_adapt\"] \n",
    "              if m in jac_scores.keys()]\n",
    "    for m in models:  # Plot IBCM last\n",
    "        all_jacs = jac_scores[m]\n",
    "        hist_outline(\n",
    "            ax, all_jacs.flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        ax.axvline(\n",
    "            np.median(all_jacs), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "    # Labeling the graph, etc.\n",
    "    ax.set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    ax.set_ylabel(\"Probability density\")\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters, same for each tested epsilon\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.001  #5e-5\n",
    "tau_avg_ibcm = 1600  # 2000\n",
    "coupling_eta_ibcm = 0.7/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.5\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA habituation and simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters, same for all epsilons\n",
    "n_i_pca = n_components * 2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 9.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_seed = 0x7170b82d905839ddda1def555e3a43508\n",
    "main_seed = 0x7170b82d905839ddda1def555e3a43507\n",
    "simul_seed = 0x52e7bfc4e1f58395730de6afff855abc\n",
    "\n",
    "# IBCM\n",
    "back_ibcm, res_ibcm = run_ibcm_simulation_adapt(adaptation_params, n_components,  \n",
    "                         n_dimensions, main_seed, simul_seed)\n",
    "res_ibcm_clean = analyze_clean_ibcm_simul(res_ibcm, back_ibcm, main_seed)\n",
    "\n",
    "# BioPCA\n",
    "back_biopca, res_biopca = run_biopca_simulation_adapt(adaptation_params, n_components,\n",
    "                            n_dimensions, main_seed, simul_seed)\n",
    "res_biopca_clean = analyze_clean_biopca_simul(res_biopca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ibcm_results(res_ibcm, res_ibcm_clean)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biopca_results(res_biopca, res_biopca_clean)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check what the $W$ matrix is doing? TODO\n",
    "wser = res_ibcm_clean[1]  # or res_biopca_clean[3]\n",
    "plot_w_matrix(tser_common, wser, skp=50)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_ser_ibcm = res_ibcm[3]\n",
    "fig, axes, axleg = plot_eps_series(eps_ser_ibcm, 50, (600, 840), skp_n=4, smoothsize=101)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cbars_gamma, wser_ibcm, bkvecser_ibcm, eps_ser, ysernorm_ibcm, moments_conc, \n",
    "#                     cbars_gamma_mean, specif_gammas, correl_c_conc, back_components, \n",
    "#                     conc_ser, mixed_samples, new_odors)\n",
    "bkser_ibcm = res_ibcm_clean[2]\n",
    "bkvecs_ibcm = res_ibcm_clean[9]\n",
    "concser_ibcm = res_ibcm_clean[10]\n",
    "fig, ax, box = plot_manifold(bkser_ibcm, bkvecs_ibcm, concser_ibcm, {\"azim\":220, \"elev\":30}, dims=(0, 1, 2))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pairplots_background(bkser_ibcm, bkvecs_ibcm, eps_ser_ibcm)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, projection matrix, etc. \n",
    "n_new_od = 100\n",
    "rgen_od = np.random.default_rng(np.random.SeedSequence(main_seed).spawn(2).pop(1))\n",
    "new_odors_kmats = generate_odor_tanhcdf([n_new_od, n_dimensions], rgen_od, unit_scale=kscale)\n",
    "\n",
    "# Common parameters\n",
    "n_kc = 1000 * n_dimensions // 25\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3 * n_dimensions // 25,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_matrix = create_sparse_proj_mat(n_kc, n_dimensions, rgen_od)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain optimal matrix for new concentration = avg whiff\n",
    "optimal_matrix = get_optimal_mat_p(bkser_ibcm, concser_ibcm, eps_ser_ibcm, back_ibcm, np.ones(1))[0]\n",
    "\n",
    "# Run odor recognition tests for \n",
    "new_conc_rel = 0.5\n",
    "test_res = test_odor_recognition_adaptation(\n",
    "                back_ibcm, new_odors_kmats, res_ibcm, res_biopca, \n",
    "                ibcm_options, pca_options, proj_matrix, projection_arguments,\n",
    "                ibcm_rates, biopca_rates, optimal_matrix, \n",
    "                new_conc_rel=new_conc_rel, n_test_t=100, n_new=n_new_od\n",
    ")\n",
    "jaccard_scores, jaccard_backs, mixture_tags, new_odor_tags = test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_jaccards(jaccard_scores)\n",
    "\n",
    "ax.set_title(r\"New conc. = {:.1f} $\\langle c \\rangle$\".format(new_conc_rel))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results of interest for final plotting\n",
    "\n",
    "Example simulation runs. The odor performance recognition is tested across simulation seeds, new odors, etc. in the Python script ``supplementary_scripts/run_adaptation_performance_tests.py``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ibcm_simuls_to_disk(fname, clean_results):\n",
    "    # Save cbar gamma series, background series, ynorm series\n",
    "    (cbars_gamma, _, bkvecser_ibcm, eps_ser, ysernorm_ibcm, _, _, _, _, \n",
    "    back_comps, conc_ser, _, _) = clean_results\n",
    "    all_saved_series = dict()\n",
    "    all_saved_series[\"cbars_gamma_ser\"] = cbars_gamma\n",
    "    # For habituation and manifold plots, save back series\n",
    "    # and odor components\n",
    "    all_saved_series[\"bkvec_ser\"] = bkvecser_ibcm\n",
    "    all_saved_series[\"back_components\"] = back_comps\n",
    "    all_saved_series[\"conc_ser\"] = conc_ser\n",
    "    # For habituation plots, save norm of y\n",
    "    all_saved_series[\"y_norm_ser\"] = ysernorm_ibcm\n",
    "    # Save epsilon dynamics\n",
    "    all_saved_series[\"eps_ser\"] = eps_ser\n",
    "\n",
    "    np.savez_compressed(fname, **all_saved_series)\n",
    "    return 0\n",
    "\n",
    "def save_biopca_simuls_to_disk(fname, clean_results):\n",
    "    # Save true and learnt PCA, that's all we really need\n",
    "    true_learnt_pcas = {}\n",
    "    (bkvecsernorm_pca, epser_pca, ysernorm_pca, wser_pca, true_pca, \n",
    "     learnt_pca, off_diag_l_avg_abs, align_error_ser) = clean_results\n",
    "    true_learnt_pcas[\"true_pca_vals\"] = true_pca[0]\n",
    "    true_learnt_pcas[\"learnt_pca_vals\"] = learnt_pca[0]\n",
    "    true_learnt_pcas[\"pca_align_error\"] = align_error_ser\n",
    "    true_learnt_pcas[\"bkvec_norm_ser\"] = bkvecsernorm_pca\n",
    "    true_learnt_pcas[\"y_norm_ser\"] = ysernorm_pca\n",
    "    np.savez_compressed(fname, **true_learnt_pcas)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "fname_ibcm = pj(outputs_folder, \"saved_ibcm_simulations_adapt_osn.npz\")\n",
    "fname_biopca = pj(outputs_folder, \"saved_biopca_simulations_adapt_osn.npz\")\n",
    "save_ibcm_simuls_to_disk(fname_ibcm, res_ibcm_clean)\n",
    "save_biopca_simuls_to_disk(fname_biopca, res_biopca_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
