{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation with IBCM neurons gating inhibitory neurons\n",
    "The details of the model are described in other Jupyter notebooks (e.g., ibcm_inhibition_three_components.ipynb). \n",
    "\n",
    "![test](figures/feedforward_inhibitory_network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Saturation function on IBCM neurons\n",
    "Here, a mild saturation function is applied to IBCM neurons' activity after lateral inhibitory coupling. So, here, we still have simply\n",
    "$$ \\vec{c} = M\\vec{x} $$\n",
    "but now the inhibited activities are rather\n",
    "$$ \\vec{\\bar{c}} = \\sigma(LM\\vec{x}) $$\n",
    "where the nonlinear activation function $\\sigma$ applies element-wise to $LM \\vec{x}$. We choose a function that saturates only when $LM\\vec{x}$ is very large, and that is linear for small values of $LM\\vec{x}$. In particular, we take\n",
    "$$ \\sigma(u) = s \\tanh{\\left(\\frac{u}{s}\\right)} $$\n",
    "which has derivative\n",
    "$$ \\sigma'(u) = \\mathrm{sech}^2{\\left(\\frac{u}{s}\\right)} = 1 - \\tanh^2{\\left(\\frac{u}{s}\\right)} = 1 - \\left(\\frac{\\sigma(u)}{s}\\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turbulent olfactory environment\n",
    "\n",
    "Importantly, here, the background used is an approximation of realistic odor concentrations in turbulent environments, based on Celani, Villermaux, Vergassola, 2014. In summary, each odor concentration (sources treated as independent for now) is a succession of whiffs and blanks, with power law distributions of exponent $-3/2$ for the waiting times, and a long-tailed whiff concentration distribution of the form $\\sim e^{-c/c_0}/c$. \n",
    "\n",
    "Also, of note, we use the Euclidean distance between $\\vec{s}$ (the filtered odor, projection neurons layer) and the target odor as a performance metric, instead of the Jaccard similarity of the Kenyon cells layer. The reason: it is easier to work with analytically, easier to understand intuitively, and easier to compute numerically. Anyways, since KC cells implement a locality sensitive hashing, closeness in Euclidean distance should correspond to closeness in neural tags (hashes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on current status of the project (June 2022)\n",
    "The immediate next step is to test whether new odors can be detected after filtering of this kind of background. \n",
    "\n",
    "There are a few steps left towards a fully realistic olfaction model: \n",
    " 1. Add (anti)-correlation between odor sources and maybe make the concentration fluctuate during a whiff too. \n",
    " 2. Combine odors not linearly but with Gautam's ORN model; this will help saturate concentrations and will also make things less linear, hence further away from what an online PCA could do best. \n",
    " \n",
    "The backup strategy is to use an alternating background, maybe with blanks, as a proxy for turbulent fluctuations, but a higher chance of success from the IBCM model (which was built for this kind of processes). \n",
    "\n",
    "Then, the final step is to compare with other projection learning models, especially online PCA and online ICA.\n",
    "\n",
    "### Two remarks after playing with the notebook below\n",
    "\n",
    "1. Saturating the sum of odors with a tanh really helps the model to learn: it prevents the rare, huge concentration events that throw off the averaging. So using Gautam's model on the input layer will really help learn odor components even though it destroys the linearity: linearity is actually a problem when concentrations change too much. \n",
    "2. When inhibition between neurons is included, even if we initialize all neurons to pretty large random values, only the neurons with the most extreme values (largest m) converge fast enough; neurons \"in the middle\" take longer. That's because after inhibition, there is not much left in $\\bar{m}$ except for those extreme values. So, a good initial condition would be to start all neurons on $S^n(1)$, the unit hypersphere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sys import byteorder as sys_byteorder\n",
    "\n",
    "from modelfcts.ibcm import integrate_inhib_ibcm_network_skip_tanh\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    fixedpoint_thirdmoment_perturbtheory\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_background_powerlaw,\n",
    "    sample_ss_conc_powerlaw, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    update_alternating_inputs, \n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, create_sparse_proj_mat\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_inverse_transform, \n",
    "    truncexp1_density, \n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from modelfcts.checktools import check_conc_samples_powerlaw_exp1\n",
    "from utils.smoothing_function import moving_average\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition\n",
    ")\n",
    "from utils.metrics import jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response to a new odor\n",
    "This part of the code only runs if the simulation above had ``n_dimensions > n_components``. \n",
    "\n",
    "The goal is to see whether a new odor, not linearly dependent of the ones in the background, also gets repressed close to zero, or produces an inhibited output noticeably different from the inhibited background, and still similar to the new odor vector, at least its component perpendicular to the background subspace. \n",
    "\n",
    "Need to test for many samples from the background odor distribution. Keep the new odor at a constant concentration, typical of the concentration at which we actually want the system to pick up the new odor. \n",
    "\n",
    "I realize that it's fine if the disentanglement of odors isn't perfect at the PN layer: besides the question of habituation, the sparse tag network proposed by Dasgupta does not address too well how multiple odors are disentangled from a complicated mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import relu_inplace, ibcm_respond_new_odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete model: sparse Kenyon cell tags for odors\n",
    "We need to make new simulations with many more dimensions (ORN types). \n",
    "\n",
    "Consequently, to avoid running into memory issues, we only save a subset of time steps in the simulation: this is fine because we are only interested in the slowly-evolving $\\vec{w}$ and $\\vec{m}$, while we don't care too much for $\\vec{x}$'s fast fluctuations. We just want the final average $\\vec{w}$ to apply as inhibition to randomly sampled background odors, which we don't even take from simulations but just generate from the steady-state distribution. \n",
    "\n",
    "*Modification*: use the Law and Cooper, 1994 trick to improve convergence speed: make the learning rate inversely proportional to the threshold $\\Theta$. The IBCM equation (for 1 neuron) gets modified to\n",
    "\n",
    "$$ \\frac{d\\vec{m}}{dt} = \\frac{\\mu}{\\Theta} c(c - \\Theta) \\vec{x}(t) $$\n",
    "\n",
    "In fact, it works better if we limit the increase of the learning rate, and even more importantly, save the decrease of it only for very late times, once neurons have converged to specific fixed points. The trick is to normalize as follows:\n",
    "\n",
    "$$ \\frac{d\\vec{m}}{dt} = \\frac{\\mu}{\\Theta + k_{\\Theta}} c(c - \\Theta) \\vec{x}(t) $$\n",
    "\n",
    "where $k_{\\Theta}$ is relatively large, larger than the value of $\\Theta$ when the neurons reach the saddle (?) point where $c_d$ is large but there is no specificity yet. Else, the learning rate freezes too fast and neurons never become specific, they get stuck in that saddle. \n",
    "\n",
    "I find that $k_{\\Theta} = 5$ is in the sweet spot for this current background, but that depends on the absolute magnitude of the background... Maybe using a separate neuron computing the average $x$ and $m$ to estimate $k_{\\Theta}$ would help? But I need to make this scale-independent, it is currently not. \n",
    "\n",
    "### Run a new simulation with 25 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import integrate_inhib_lawibcm_network_skip_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions_tag = 25  # Half the real number for faster simulations\n",
    "n_neurons_tag = 24\n",
    "n_components_tag = 6\n",
    "\n",
    "# Simulation rates and coupling stay the same (try at least)\n",
    "duration = 320000.0\n",
    "deltat = 1.0\n",
    "learnrate_tag = 0.004  # 0.000001 = 1e-6\n",
    "tau_avg_tag = 800\n",
    "inhib_rates_tag = [0.00025, 0.00005]  # alpha, beta\n",
    "# Background components need to be redefined. Extra dimensions are somewhat superfluous\n",
    "coupling_eta_tag = 0.5/n_neurons_tag\n",
    "ssat_tag = 50.0\n",
    "k_c2bar_avg = 2.0\n",
    "lambd_ibcm = 2.0\n",
    "ibcm_rates_tag = [learnrate_tag, tau_avg_tag, coupling_eta_tag, lambd_ibcm, ssat_tag, k_c2bar_avg]\n",
    "\n",
    "use_tanh_sat_tag = False\n",
    "\n",
    "# Choose symmetric, normalized background odor components\n",
    "#back_components_tag = np.ones([n_components_tag, n_dimensions_tag]) * 0.2\n",
    "#for i in range(n_components_tag):\n",
    "#    back_components_tag[i, i] = 0.8\n",
    "#    back_components_tag[i] /= np.sqrt(np.sum(back_components_tag[i]**2))\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta_tag = np.random.default_rng(seed=0xf452aff441eb4c4568e97848aa1746b9)  # Good seed\n",
    "#rgen_meta_tag = np.random.default_rng(seed=0xf45daf0431eb4d4f68e97848aa1746a9)\n",
    "back_components_tag = np.zeros([n_components_tag, n_dimensions_tag])\n",
    "for i in range(n_components_tag):\n",
    "    back_components_tag[i] = generate_odorant(n_dimensions_tag, rgen_meta_tag, lambda_in=0.1)\n",
    "back_components_tag = back_components_tag / l2_norm(back_components_tag).reshape(-1, 1)\n",
    "    \n",
    "# Initial synaptic weights: small positive noise\n",
    "#init_synapses_tag = 0.1*rgen_meta_tag.random(size=[n_neurons_tag, n_dimensions_tag])\n",
    "init_synapses_tag = 0.1*rgen_meta_tag.standard_normal(size=[n_neurons_tag, n_dimensions_tag])\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params_tag = [\n",
    "    np.asarray([1.0] * n_components_tag),        # whiff_tmins\n",
    "    np.asarray([100.] * n_components_tag),       # whiff_tmaxs\n",
    "    np.asarray([2.0] * n_components_tag),        # blank_tmins\n",
    "    np.asarray([200.0] * n_components_tag),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components_tag),        # c0s\n",
    "    np.asarray([0.5] * n_components_tag),        # alphas\n",
    "]\n",
    "back_params_tag.append(back_components_tag)\n",
    "\n",
    "# Initial values of background process variables (t, c for each variable)\n",
    "init_concs_tag = sample_ss_conc_powerlaw(*back_params_tag[:-1], size=1, rgen=rgen_meta_tag)\n",
    "init_times_tag = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta_tag.random(size=n_components_tag), *back_params_tag[2:4])\n",
    "tc_init_tag = np.stack([init_times_tag, init_concs_tag.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector \n",
    "init_bkvec_tag = tc_init_tag[:, 1].dot(back_components_tag)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list_tag = [tc_init_tag, init_bkvec_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a heavy simulation\n",
    "skp_tag = 20\n",
    "update_fct = update_powerlaw_times_concs_saturate if use_tanh_sat_tag else update_powerlaw_times_concs\n",
    "sim_results = integrate_inhib_lawibcm_network_skip_tanh(init_synapses_tag, update_fct, \n",
    "                    init_back_list_tag, ibcm_rates_tag, inhib_rates_tag, back_params_tag, duration, deltat, \n",
    "                    seed=seed_from_gen(rgen_meta_tag), noisetype=\"uniform\", skp=skp_tag)\n",
    "tser_tag, nuser_tag, bkvecser_tag, mser_tag, cbarser_tag, thetaser_tag, wser_tag, yser_tag = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 100000 // skp_tag\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser_tag = mser_tag*(1.0 + coupling_eta_tag) - coupling_eta_tag*np.sum(mser_tag, axis=1, keepdims=True)\n",
    "c_gammas_tag = mser_tag.dot(back_components_tag.T)\n",
    "cbars_gamma_tag = mbarser_tag.dot(back_components_tag.T)\n",
    "\n",
    "sums_cbars_gamma_tag = np.sum(cbars_gamma_tag, axis=2)\n",
    "sums_cbars_gamma2_tag = np.sum(cbars_gamma_tag*cbars_gamma_tag, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. Easiest to compute numerically. \n",
    "# Although could do analytically by conditioning on puff==on or off\n",
    "conc_ser_tag = nuser_tag[:, :, 1]\n",
    "# Odors are all iid so we can average over all odors\n",
    "mean_conc = np.mean(conc_ser_tag)\n",
    "sigma2_conc = np.var(conc_ser_tag)\n",
    "thirdmom_conc = np.mean((conc_ser_tag - mean_conc)**3)\n",
    "moments_conc = [mean_conc, sigma2_conc, thirdmom_conc]\n",
    "\n",
    "# Analytical prediction\n",
    "res = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components_tag-1, lambd=lambd_ibcm)\n",
    "c_specif, c_nonspecif = res[:2]\n",
    "\n",
    "# Constaint 1: sum of c_gammas for each neuron\n",
    "#print(\"Comparison to analytical fixed points\")\n",
    "#print(\"This should be all ones:\", np.mean(sums_cbars_gamma_sym[transient:], axis=0) * averages_nu.mean())\n",
    "# Constraint 2: sum of c_gammas^2 for each neuron, copmared to 1/sigma^2\n",
    "#print(\"This should be all zeros:\", np.mean(sums_cbars_gamma2_sym[transient:], axis=0)*sigma2 - 1.0)\n",
    "\n",
    "# Count how many components are at each possible value. Use cbar = 2.0 as a split. \n",
    "split_val = 2.0\n",
    "cbars_gamma_mean_tag = np.mean(cbars_gamma_tag[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean_tag.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean_tag.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Late mean:\", np.mean(cbarser_tag[transient:]**2, axis=0))\n",
    "print(\"Early mean:\", np.mean(cbarser_tag[:3]**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cbar2_avg term throughout\n",
    "cbar2_avg_ser = moving_average(cbarser_tag*cbarser_tag, kernelsize=tau_avg_tag)\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_neurons_tag)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_neurons_tag):\n",
    "    ax.plot(tser_tag[:-tau_avg_tag], cbar2_avg_ser[:-tau_avg_tag, i], color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000)\", ylabel=r\"$\\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_tag, cbars_gamma_tag, skp=10, transient=300000 // skp_tag)\n",
    "# Compare to exact analytical fixed point solution\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "#mser_norm_centered = l2_norm(mser_tag, axis=-1) - np.mean(l2_norm(mser_tag[transient:], axis=-1), axis=0)\n",
    "cbarser_norm_centered = cbarser_tag - np.mean(cbarser_tag[transient:], axis=0)\n",
    "conc_ser_centered = nuser_tag[:, :, 1] - np.mean(nuser_tag[transient:, :, 1], axis=0)\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] * conc_ser_centered[transient:, None, :], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components_tag):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "                        comp, np.sum(np.mean(cbars_gamma_tag[-2000:, :, comp], axis=0) > 2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "The above looks quite promising: each IBCM neuron is specific to one odor. Problem is that apparently, it happens often that one of the odors has no specific neuron. Of course it's not going to inhibit well in this case... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(tser_tag, bkvecser_tag, yser_tag, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp_tag\n",
    "avg_bknorm = np.mean(bknorm_ser[transient:])\n",
    "avg_ynorm = np.mean(ynorm_ser[transient:])\n",
    "avg_reduction_factor = avg_ynorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser[transient:])\n",
    "std_ynorm = np.std(ynorm_ser[transient:])\n",
    "std_reduction_factor = std_ynorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and compare projection tags after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New odor, mix, and inhibit\n",
    "### Repeat for many new odors (and ideally, should repeat for many backgrounds)\n",
    "### But for now, assume all simulations would give similarly good inhibition. \n",
    "n_test_new_odors = 100\n",
    "mix_frac = 0.5\n",
    "\n",
    "# Average m and w with which we will inhibit\n",
    "transient_tag = 250000 // skp_tag\n",
    "mtag_mean = np.mean(mser_tag[transient_tag:], axis=0)\n",
    "wtag_mean = np.mean(wser_tag[transient_tag:], axis=0)\n",
    "\n",
    "# Background samples, valid for all new test odors\n",
    "back_samples_tag, _ = sample_background_powerlaw(back_components_tag, *back_params_tag[:-1], \n",
    "                                          size=100, rgen=rgen_meta_tag)\n",
    "inhib_ibcm_samples_tag = []\n",
    "inhib_avg_samples_tag = []\n",
    "mix_samples_tag = []\n",
    "new_odor_targets = []\n",
    "for i in range(n_test_new_odors):\n",
    "    # New odor\n",
    "    #new_odor_tag = np.roll(back_components_tag[0], shift=-1)  # Should be a new vector\n",
    "    new_odor_tag = generate_odorant(n_dimensions_tag, rgen_meta_tag)\n",
    "    new_odor_tag = new_odor_tag / l2_norm(new_odor_tag)\n",
    "    new_odor_targets.append(new_odor_tag)\n",
    "    \n",
    "    # Background samples, then add new odor\n",
    "    typical_conc = np.mean(back_params_tag[-3]) * np.mean(back_params_tag[-2])\n",
    "    mix_samples = back_samples_tag + mix_frac * new_odor_tag.reshape(1, -1) * typical_conc\n",
    "    if use_tanh_sat_tag:\n",
    "        mix_samples = 3.0 * np.tanh(mix_samples / 3.0)\n",
    "    mix_samples_tag.append(mix_samples)\n",
    "\n",
    "    # Compare to inhibition of the average background, followed by ReLU\n",
    "    if use_tanh_sat_tag:\n",
    "        avg_back = np.mean(3.0 * np.tanh(bkvecser_tag / 3.0), axis=0)\n",
    "    else:\n",
    "        avg_back = np.mean(bkvecser_tag, axis=0)\n",
    "    a_over_ab = inhib_rates_tag[0] / sum(inhib_rates_tag)\n",
    "    inhib_avg_samples = relu_inplace(mix_samples - a_over_ab * avg_back.reshape(1, -1)) + 0.0001\n",
    "    inhib_avg_samples_tag.append(inhib_avg_samples)\n",
    "\n",
    "    # Inhibition of each generated sample and statistics on performance\n",
    "    inhib_ibcm_samples = ibcm_respond_new_odors(mix_samples, mtag_mean, wtag_mean, ibcm_rates_tag)\n",
    "    inhib_ibcm_samples_tag.append(inhib_ibcm_samples)\n",
    "\n",
    "mix_samples_tag = np.asarray(mix_samples_tag)\n",
    "inhib_avg_samples_tag = np.asarray(inhib_avg_samples_tag)\n",
    "inhib_ibcm_samples_tag = np.asarray(inhib_ibcm_samples_tag)\n",
    "new_odor_targets = np.asarray(new_odor_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags. This won't be great because too few dimensions to begin with, but try anyways. \n",
    "projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=True, n_pn_per_kc=3, fix_thresh=None,\n",
    "                      project_thresh_fact=0.05)\n",
    "proj_mat = create_sparse_proj_mat(n_kc=int(2000/50*n_dimensions_tag), n_rec=n_dimensions_tag, \n",
    "                        rgen=rgen_meta_tag, fraction_filled=projtag_kwargs[\"n_pn_per_kc\"]/n_dimensions_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_none = []\n",
    "jaccards_inhib_avg = []\n",
    "jaccards_inhib_ibcm = []\n",
    "for i in range(mix_samples_tag.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(mix_samples_tag.shape[1]):\n",
    "        tag_none = project_neural_tag(mix_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_avg = project_neural_tag(inhib_avg_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_ibcm = project_neural_tag(inhib_ibcm_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_none.append(jaccard(target_tag, tag_none))\n",
    "        jaccards_inhib_avg.append(jaccard(target_tag, tag_avg))\n",
    "        jaccards_inhib_ibcm.append(jaccard(target_tag, tag_ibcm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "\n",
    "ax.hist(jaccards_inhib_none, label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_none), color=clr_none, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_avg, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_ibcm, label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ibcm), color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "do_save = False\n",
    "if mix_frac == 0.2 and do_save:\n",
    "    fig.savefig(\"figures/detection/jaccard_similarity_ibcm_average_none_f20percent.pdf\", transparent=True)\n",
    "elif mix_frac == 0.5 and do_save:\n",
    "    fig.savefig(\"figures/detection/jaccard_similarity_ibcm_average_none_f50percent.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are jaccard tags of inhibited background odors going towards empty?\n",
    "# Check a few tags\n",
    "tag_series = []\n",
    "norm_series = []\n",
    "for it in range(0, tser_tag.shape[0], 200):\n",
    "    yvec = yser_tag[it]\n",
    "    tag_series.append(len(project_neural_tag(yser_tag[it], bkvecser_tag[it], proj_mat, **projtag_kwargs)))\n",
    "    norm_series.append(l2_norm(bkvecser_tag[it])*20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(tag_series)), tag_series, marker=\"o\")\n",
    "ax.plot(range(len(tag_series)), norm_series, marker='s')\n",
    "ax.set(xlabel=\"Time step (x20 00)\", ylabel=\"Background tag size\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check background statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "t_window = slice(0, 2000 //skp_tag, 1)\n",
    "for i in range(n_components_tag):\n",
    "    ax.plot(tser_tag[t_window]/1000, nuser_tag[t_window, i, 1], label=\"Odor {}\".format(i))\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=\"Concentration\")\n",
    "fig.set_size_inches(4., 2.25)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/sample_background_concentrations_time_series.pdf\", transparent=True, \n",
    "#           bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the background process matches the intended distribution\n",
    "fig, axes = check_conc_samples_powerlaw_exp1(nuser_tag[:, :, 1].T, *back_params_tag[:-1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "None of the models work very well on turbulent background, even with tanh saturation of ORNs. In fact, for some cases, the new odor is better detected without any inhibition. IBCM only gives a marginally better median, nothing to brag about. Need to find a way to make it converge faster to a useful fixed point, to make neurons distribute themselves better, and to make the $\\vec{m}$ fluctuate less. \n",
    "\n",
    "Note: even without ORN saturation, it can work as well as with it, as long as the run and IBCM parameters are chosen well enough to ensure convergence. \n",
    "\n",
    "### Remarks\n",
    "1. We should also compute Jaccard similarity to background odors: if there is a background odor more similar, we are really failing the test. The proper metric should be the ratio of Jaccard(target)/Jaccard(most similar background). \n",
    "2. Instead of using the average $\\vec{m}$ and $\\vec{w}$, I should. Maybe this will be worse, if fluctuations of $\\vec{m}$ are detrimental. But maybe it will be better, if the $\\vec{w}$ track these fluctuations well enough and compensate for them. \n",
    "3. We should use multiple different background simulations, instead of just one like I did here. \n",
    "4. Pray for PCA to fare worse... \n",
    "\n",
    "So, once I have all my models working well enough, then I should subject all of them to this longer testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some results for further plotting\n",
    "In particular, save excerpt of concentration time series. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results_dir = \"results/for_plots/\"\n",
    "np.savez_compressed(results_dir + \"sample_turbulent_simulation.npz\", nuser=nuser_tag, tser=tser_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"I have not adapted the code below this point.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PN distances\n",
    "dist_pure_inhib_none_tag = distance_panel_target(mix_samples_tag/2, new_odor_tag)\n",
    "dist_pure_inhib_avg_tag = distance_panel_target(inhib_avg_samples_tag, new_odor_tag)\n",
    "dist_pure_inhib_ibcm_tag = distance_panel_target(inhib_ibcm_samples_tag, new_odor_tag)\n",
    "\n",
    "median_distances_none_tag = np.median(dist_pure_inhib_none_tag, axis=0)\n",
    "median_distances_avg_tag = np.median(dist_pure_inhib_avg_tag, axis=0)\n",
    "median_distances_ibcm_tag = np.median(dist_pure_inhib_ibcm_tag, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none_tag[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none_tag[i], color=clr_none, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_avg_tag[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "    ax.axvline(median_distances_avg_tag[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_ibcm_tag[:, i], label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True) \n",
    "    ax.axvline(median_distances_ibcm_tag[i], color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to ideal inhibitory network\n",
    "In a linear algebra perspective, the best inhibition that could possibly be achieved of a new odor plus background mixture is that the whole component of the new odor parallel to the vector subspace spanned by the background odors is suppressed, while the component perpendicular to it is kept. Indeed, the appearance of the new odor's component in the background space cannot be distinguished from a fluctuation of the background (unless we had neurons tracking statistics of typical activations in that space, but not obvious how to get that). At any rate, this is the best we can hope our IBCM inhibition network will achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_projector(a):\n",
    "    \"\"\" Calculate projector a a^+, which projects\n",
    "    a column vector on the vector space spanned by columns of a. \n",
    "    \"\"\"\n",
    "    a_inv = np.linalg.pinv(a)\n",
    "    return a.dot(a_inv)\n",
    "    \n",
    "def find_parallel_component(x, basis, projector=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (np.ndarray): 1d array of length D containing the vector to decompose. \n",
    "        basis (np.ndarray): 2d matrix of size DxK where each column is one\n",
    "            of the linearly independent background vectors. \n",
    "        projector (np.ndarray): 2d matrix A A^+, the projector on the vector\n",
    "            space spanned by columns of basis. \n",
    "    Return:\n",
    "        x_par (np.ndarray): component of x found in the vector space of basis\n",
    "            The perpendicular component can be obtained as x - x_par. \n",
    "    \"\"\"\n",
    "    # If the projector is not provided yet\n",
    "    if projector is None:\n",
    "        # Compute Moore-Penrose pseudo-inverse and AA^+ projector\n",
    "        projector = find_projector(basis)\n",
    "    x_par = projector.dot(x)\n",
    "    return x_par\n",
    "\n",
    "def ideal_linear_inhibitor(x_n_par, x_n_ort, x_back, f, alpha, beta):\n",
    "    \"\"\" Calculate the ideal projection neuron layer, which assumes\n",
    "    perfect inhibition (down to beta/(alpha+beta)) of the component of the mixture\n",
    "    parallel to the background odors' vector space, while leaving the orthogonal\n",
    "    component of the new odor untouched. \n",
    "    \n",
    "    Args:\n",
    "        x_n_par (np.1darray): new odor, component parallel to background vector space\n",
    "        x_n_ort (np.1darray): new odor, component orthogonal to background vector space \n",
    "        x_back (np.2darray): background samples, one per row\n",
    "        f (float): mixture fraction (hard case is f=0.2)\n",
    "        alpha (float): inhibitory weights learning rate alpha\n",
    "        beta (float): inhibitory weights decaying rate beta\n",
    "    \n",
    "    Returns:\n",
    "        s (np.1darray): projection neurons after perfect linear inhibition\n",
    "    \"\"\"\n",
    "    # Allow broadcasting for multiple x_back vectors\n",
    "    factor = beta / (alpha + beta)\n",
    "    s = factor * f*x_n_par + f*x_n_ort\n",
    "    # I thought the following would have been even better, but turns out it is worse for small f\n",
    "    #s = f*x_n_par + f*x_n_ort\n",
    "    s = s.reshape(1, -1) + factor * (1.0-f) * x_back\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse each new odor in new_odor_targets and each background in back_samples_tag\n",
    "# Compute the projector on the background odor components only once\n",
    "# Compute parallel component of each new odor\n",
    "# Mix it with all background samples at once using broadcasting capability of ideal_linear_inhibitor function\n",
    "background_projector = find_projector(back_components_tag.T)\n",
    "inhib_ideal_samples_tag = []\n",
    "for od in new_odor_targets:\n",
    "    # Decompose\n",
    "    od_par = find_parallel_component(od, basis=back_components_tag.T, projector=background_projector)\n",
    "    od_ort = od - od_par\n",
    "    # Compute the perfectly inhibited mixture with each background sample\n",
    "    inhib_ideal = ideal_linear_inhibitor(od_par, od_ort, back_samples_tag, mix_frac, *inhib_rates)\n",
    "    # Background reduced to b/(a+b), new odor intact? Perfect inhibition\n",
    "    #inhib_ideal = inhib_rates[1] / sum(inhib_rates) * (1.0 - mix_frac) * back_samples_tag + mix_frac * od.reshape(1, -1)\n",
    "    inhib_ideal_samples_tag.append(inhib_ideal)\n",
    "inhib_ideal_samples_tag = np.asarray(inhib_ideal_samples_tag)\n",
    "\n",
    "# Compute neural tags of the ideal inhibited mixtures and compare to target tags. \n",
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_ideal = []\n",
    "for i in range(new_odor_targets.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(back_samples_tag.shape[0]):\n",
    "        mix_sample_tag = back_samples_tag[j] + mix_frac * new_odor_targets[i]\n",
    "        tag_ideal = project_neural_tag(inhib_ideal_samples_tag[i, j], mix_sample_tag, proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_ideal.append(jaccard(target_tag, tag_ideal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_map = {\"none\": \"xkcd:navy blue\", \"average\": \"xkcd:orangey brown\", \n",
    "           \"ibcm\":\"xkcd:turquoise\", \"ideal\": \"xkcd:powder blue\", \"ideal2\":\"xkcd:pale rose\"}\n",
    "\n",
    "ax.hist(jaccards_inhib_ideal, label=\"Ideal inhibition\", facecolor=clr_map[\"ideal\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ideal\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ideal), color=clr_map[\"ideal\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_map[\"average\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"average\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_map[\"average\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_ibcm, label=\"IBCM inhibition\", facecolor=clr_map[\"ibcm\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ibcm\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ibcm), color=clr_map[\"ibcm\"], ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance as a function of f\n",
    "I expect to see a relatively sharp drop of median performance for IBCM a bit above $f=\\beta/(\\alpha + \\beta)$, and at this value for the ideal inhibition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median_performances(back_samples, new_odors, f, projmat, proj_kwargs, \n",
    "                                m_mean, w_mean, eta, inhib_ab, back_components):\n",
    "    \"\"\" Compute median Jaccard similarity for the different inhibition methods we have, \n",
    "    for a given value of mixture parameter f. \"\"\"\n",
    "    all_jaccard_pairs_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}  # list of lists, one per method\n",
    "    back_proj = find_projector(back_components.T)\n",
    "    for i in range(new_odors.shape[0]):\n",
    "        # Compute target tag\n",
    "        target_tag = project_neural_tag(new_odors[i], new_odors[i], projmat, **proj_kwargs)\n",
    "        # Prepare mixtures\n",
    "        mix_samples = back_samples*(1.0 - f) + new_odors[i:i+1]*f\n",
    "        \n",
    "        # Compute inhibited mixtures with the different methods\n",
    "        # No inhibition: just use mix_samples\n",
    "        # Average inhibition\n",
    "        avg_back_tag = averages_nu.dot(back_components_tag)\n",
    "        a_over_ab = inhib_ab[0] / sum(inhib_ab)\n",
    "        inhib_avg_samples = mix_samples - a_over_ab * avg_back_tag.reshape(1, -1)\n",
    "\n",
    "        # Inhibition of each generated sample and statistics on performance\n",
    "        inhib_ibcm_samples = ibcm_respond_new_odors(mix_samples, m_mean, w_mean, eta)\n",
    "        \n",
    "        # Ideal inhibition\n",
    "        od_par = find_parallel_component(new_odors[i], basis=back_components.T, projector=back_proj)\n",
    "        od_ort = new_odors[i] - od_par\n",
    "        # Compute the perfectly inhibited mixture with each background sample\n",
    "        inhib_ideal_samples = ideal_linear_inhibitor(od_par, od_ort, back_samples, f, *inhib_ab)\n",
    "        # Background reduced to b/(a+b), new odor intact?\n",
    "        inhib_ideal2_samples = (1.0 - a_over_ab) * (1.0 - f) * back_samples + f * new_odors[i:i+1]\n",
    "    \n",
    "        # For each inhibited mixture, compute jaccard similarity\n",
    "        current_jaccard_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "        for j in range(back_samples.shape[0]):\n",
    "            mix_tag = project_neural_tag(mix_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"none\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Average\n",
    "            mix_tag = project_neural_tag(inhib_avg_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"average\"].append(jaccard(target_tag, mix_tag))\n",
    "        \n",
    "            # IBCM\n",
    "            mix_tag = project_neural_tag(inhib_ibcm_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ibcm\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal\n",
    "            mix_tag = project_neural_tag(inhib_ideal_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal 2\n",
    "            mix_tag = project_neural_tag(inhib_ideal2_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal2\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "        # Add those values to the total list\n",
    "        for method in all_jaccard_pairs_dict.keys():\n",
    "            all_jaccard_pairs_dict[method].append(current_jaccard_dict[method])\n",
    "    \n",
    "    # Convert to 2d array and compute median\n",
    "    # Could choose to have one median per odor or per background sample\n",
    "    all_jaccard_pairs_dict = {k:np.asarray(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    median_jaccard_pairs_dict = {k:np.median(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    return median_jaccard_pairs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previous functions for various f values\n",
    "median_jaccards = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "f_range = np.arange(0.1, 0.8, 0.1)\n",
    "for f in f_range:\n",
    "    meds = compute_median_performances(back_samples_tag, new_odor_targets, f, proj_mat, projtag_kwargs, \n",
    "                                mtag_mean, wtag_mean, coupling_eta, inhib_rates, back_components_tag)\n",
    "    for k in meds:\n",
    "        median_jaccards[k].append(meds[k])\n",
    "    print(\"Done f = {:.2f}\".format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labelmap = {\"none\":\"None\", \"average\":\"Average\", \"ibcm\":\"IBCM\", \"ideal\":r\"Ideal $\\perp$\", \"ideal2\":\"Ideal all\"}\n",
    "for k in median_jaccards:\n",
    "    ax.plot(f_range, median_jaccards[k], color=clr_map[k], label=labelmap[k], lw=3)\n",
    "ax.set(xlabel=\"Fraction $f$ of new odor\", ylabel=\"Median Jaccard similarity\")\n",
    "ax.legend(title=\"Inhibition method\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/inhibition_jaccard_comparison_methods.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
