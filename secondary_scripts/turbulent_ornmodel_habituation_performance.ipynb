{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation with IBCM and BioPCA to turbulent backgrounds with non-linear ORN model\n",
    "The details of the model are described in other Jupyter notebooks (e.g., ibcm_gating_linear_multiple_components.ipynb). \n",
    "\n",
    "Importantly, the background used is an approximation of realistic odor concentrations in turbulent environments, based on Celani, Villermaux, Vergassola, 2014. In summary, each odor concentration (sources treated as independent for now) is a succession of whiffs and blanks, with power law distributions of exponent $-3/2$ for the waiting times, and a long-tailed whiff concentration distribution of the form $\\sim e^{-c/c_0}/c$. \n",
    "\n",
    "Also, of note, we use the Euclidean distance between $\\vec{s}$ (the filtered odor, projection neurons layer) and the target odor as a performance metric, instead of the Jaccard similarity of the Kenyon cells layer. The reason: it is easier to work with analytically, easier to understand intuitively, and easier to compute numerically. Anyways, since KC cells implement a locality sensitive hashing, closeness in Euclidean distance should correspond to closeness in neural tags (hashes). \n",
    "\n",
    "Lastly, odors are now encoded by two constant vectors: binding affinities $\\vec{\\kappa}$ and activation affinities $\\vec{\\eta}$, and they are combined according to the ORN model from Reddy et al., 2018, equations 16-17. So, if we have odors $\\gamma = 1, 2, \\ldots, K$ with concentrations $\\nu_{\\gamma}$ and vectors $(\\vec{\\kappa}_{\\gamma}, \\vec{\\eta}_{\\gamma})$, the total activity of ORN $j = 1, 2, \\ldots, N$ is:\n",
    "\n",
    "$$ F_i(\\vec{\\nu}) = \\frac{F_{max}}{1 + \\left(\\frac{1 + C/\\kappa_{j, mix}}{\\chi_{j, M} \\eta_{j, mix}C/\\kappa_{j, mix}} \\right)^n }$$ \n",
    "\n",
    "where\n",
    "\n",
    " - $ C = \\sum_{\\gamma=1}^K \\nu_{\\gamma}$ is the total concentration\n",
    " - $n = 4$ is the number of activation steps of ionic CNG channels\n",
    " - $\\kappa_{j, mix}^{-1} = \\sum_{\\gamma=1}^K \\frac{\\beta_{\\gamma}}{\\kappa_{j, \\gamma}} $ is the mixture's binding affinity for receptor type $j$.\n",
    " - $\\eta_{j, mix} = \\kappa_{j, mix} \\sum_{\\gamma=1}^K \\eta_{j, \\gamma} \\beta_{\\gamma} / \\kappa_{j, \\gamma}$ is the mixture's activation affinity for receptor type $j$.  \n",
    " - $\\beta_{\\gamma} = \\nu_{\\gamma} / C$ is the fraction of odor $\\gamma$ in the total concentration \n",
    " - $\\chi_{j, M} = 1 - \\sum_{\\gamma=1}^K \\nu_{\\gamma} \\tilde{M}_{j, \\gamma}$ is the masking coefficient\n",
    " - $\\tilde{M}_{\\gamma} = \\frac{K_{M, j, \\gamma}\\nu_{\\gamma}}{1 + \\sum_{\\rho=1}^K K_{M, j, \\rho} \\nu_{\\rho}}$ is the masking concentration of odor $\\gamma$ at olfactory receptor type $j$.\n",
    " - $\\mu_{j, \\gamma}$ is the masking efficiency, a number between $0$ and $1$. \n",
    " - $F_{max}$ is the maximum activation of an ORN, which we set to 1. \n",
    " \n",
    "For the moment, we set the masking coefficient $\\chi_{j, M} = 1$, i.e. there is no masking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of general interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(1, \"../\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    fixedpoint_thirdmoment_perturbtheory,\n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_ifpsp_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_optimal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    check_conc_samples_powerlaw_exp1,\n",
    "    compute_pca_meankept, \n",
    "    compute_projector_series, \n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_times_concs, \n",
    "    logof10, \n",
    "    sample_background_powerlaw,\n",
    "    sample_ss_conc_powerlaw, \n",
    "    decompose_nonorthogonal_basis, \n",
    "    update_alternating_inputs, \n",
    "    generate_odorant, \n",
    "    update_tc_odor\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    "    tags_list_to_csr_matrix\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_inverse_transform, \n",
    "    truncexp1_density, \n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gammas_sums, \n",
    "    plot_cbars_gamma_series, \n",
    "    plot_3d_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard\n",
    "from modelfcts.orn_model import (\n",
    "    generate_odorant_kappaeta, \n",
    "    combine_odors_ornmodel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main new function: background update with ORN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the background by combining odorant at the current concentrations with the ORN model\n",
    "def update_powerlaw_times_concs_ornmodel(tc_bk, params_bk, noises, dt):\n",
    "    \"\"\"\n",
    "    Simulate turbulent odors by pulling wait times until the end of a whiff\n",
    "        or until the next blank, and a concentration of the whiff.\n",
    "        For each odor, check whether the time left until switch is <= zero;\n",
    "        if so, pull either\n",
    "            - another wait time t_w if current c=0, and pull the new c > 0\n",
    "              (we were in a blank and are starting a whiff)\n",
    "            - another wait time t_b if current c > 0, and set c = 0\n",
    "              (we were in a whiff and are starting a blank)\n",
    "        Otherwise, decrement t by dt and don't change c.\n",
    "\n",
    "    Args:\n",
    "        tc_bk (np.ndarray): array of t, c for each odor in the background,\n",
    "            where t = time left until next change, c = current concentration\n",
    "            of the odor. Shaped [n_odors, 2]\n",
    "        params_bk (list): contains the following elements (a lot needed!):\n",
    "            whiff_tmins (np.ndarray): lower cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            whiff_tmaxs (np.ndarray): upper cutoff in the power law\n",
    "                of whiff durations, for each odor\n",
    "            blank_tmins (np.ndarray): same as whiff_tmins but for blanks\n",
    "            blank_tmaxs (np.ndarray): same as whiff_tmaxs but for blanks\n",
    "            c0s (np.ndarray): c0 concentration scale for each odor\n",
    "            alphas (np.ndarray): alpha*c0 is the lower cutoff of p_c\n",
    "            vecs (np.ndarray): 3d array where axis 0 has length 2, \n",
    "                the first sub-array giving the kappa vectors of each odor, \n",
    "                and the second, giving eta vectors. \n",
    "        noises (np.ndarray): fresh U(0, 1) samples, shaped [n_odors, 2],\n",
    "            in case we need to pull a new t and/or c.\n",
    "            TODO: most noises are wasted; for now memory isn't an issue\n",
    "            but this is a place where the code can be optimized a lot.\n",
    "        dt (float): time step duration, in simulation units\n",
    "    \"\"\"\n",
    "    # Update one odor's t and c at a time, if necessary\n",
    "    # TODO: reorder to have back_components first, then all params of each odor\n",
    "    # Would make the update faster, because we update one odor at a time. \n",
    "    tc_bk_new = np.zeros(tc_bk.shape)\n",
    "    for i in range(tc_bk.shape[0]):\n",
    "        tc_bk_new[i] = update_tc_odor(tc_bk[i], dt, noises[i],\n",
    "                                *[p[i] for p in params_bk[:-1]])\n",
    "\n",
    "    # Compute backgound vector (even if it didn't change)\n",
    "    vecs_nu = params_bk[-1]  # kappavecs, etavecs\n",
    "    new_bk_vec = combine_odors_ornmodel(tc_bk_new[:, 1], vecs_nu[0], vecs_nu[1], n_cng=4, fmax=1.0)\n",
    "    return new_bk_vec, tc_bk_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 25  # Half the real number for faster simulations\n",
    "n_components = 3  # Number of background odors\n",
    "\n",
    "inhib_rates = [0.0001, 0.00002]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 10\n",
    "skp = 10 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x270369e90599ffa80a743d99ac942f28)\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_times_concs_ornmodel\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "\n",
    "# Background odors: parameters (kappa^-1, eta) and x_gamma vectors\n",
    "rho_back = 0.5\n",
    "std_kappa = 4.0\n",
    "# Indexed [kappa_or_eta, n_odors, n_dimensions]\n",
    "back_components = [np.asarray(generate_odorant_kappaeta(rho_back, n_dimensions, std_kappa, rgen_meta)) \n",
    "                   for _ in range(n_components)]\n",
    "back_components = np.stack(back_components, axis=1)\n",
    "back_params.append(back_components)\n",
    "# We define x_gamma to be the ORN activation in response to odor gamma at large concentration 10\n",
    "x_gamma_vecs = np.asarray([combine_odors_ornmodel(np.ones(1)*10.0, back_components[0, i], back_components[1, i], \n",
    "                                       n_cng=4, fmax=1.0) for i in range(n_components)])\n",
    "print(x_gamma_vecs)\n",
    "\n",
    "# Initial values of background process variables (t, c for each variable)\n",
    "init_concs = sample_ss_conc_powerlaw(*back_params[:-1], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components), *back_params[2:4])\n",
    "tc_init = np.stack([init_times, init_concs.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector: combine odors with the tc_init concentrations\n",
    "init_bkvec = combine_odors_ornmodel(tc_init[:, 1], back_components[0], back_components[1], n_cng=4, fmax=1.0)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation\n",
    "\n",
    "The good question is: what are IBCM neurons going to learn? The input space is $\\vec{x}$, but it's not a linear combination of background odors, which are rather defined by $\\vec{\\eta}$ and $\\vec{\\kappa}$. So it's unclear even what exactly will be the input process distribution, what components will be learnt, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.0003  #5e-5\n",
    "tau_avg_ibcm = 1200  # 2000\n",
    "coupling_eta_ibcm = 0.5/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.3*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])*lambd_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "# Perform successive shorter runs/restarts for memory efficiency\n",
    "tser_ibcm = []\n",
    "nuser_ibcm = []\n",
    "bkvecser_ibcm = []\n",
    "mser_ibcm = []\n",
    "cbarser_ibcm = []\n",
    "wser_ibcm = []\n",
    "sser_ibcm = []\n",
    "thetaser_ibcm = []\n",
    "if n_chunks > 1:\n",
    "    seed_spawns = np.random.SeedSequence(simul_seed).spawn(10)\n",
    "else:\n",
    "    seed_spawns = [simul_seed]\n",
    "for i in range(n_chunks):\n",
    "    tstart = perf_counter()\n",
    "    if i == 0:\n",
    "        init_vari = init_synapses_ibcm\n",
    "        init_back = init_back_list\n",
    "    else:\n",
    "        init_vari = [mser_ibcm[i-1][-1], thetaser_ibcm[i-1][-1], wser_ibcm[i-1][-1]]\n",
    "        init_back = [nuser_ibcm[i-1][-1], bkvecser_ibcm[i-1][-1]]\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_vari, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params, duration/n_chunks, \n",
    "                deltat, seed=seed_spawns[i], noisetype=\"uniform\",  \n",
    "                skp=skp, **ibcm_options\n",
    "    )\n",
    "    tser_ibcm.append(sim_results[0] + i/n_chunks*duration)\n",
    "    nuser_ibcm.append(sim_results[1])\n",
    "    bkvecser_ibcm.append(sim_results[2])\n",
    "    mser_ibcm.append(sim_results[3]) \n",
    "    cbarser_ibcm.append(sim_results[4]) \n",
    "    thetaser_ibcm.append(sim_results[5])\n",
    "    wser_ibcm.append(sim_results[6])\n",
    "    sser_ibcm.append(sim_results[7])\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished chunk\", i, \"in {:.2f} s\".format(tend - tstart))\n",
    "\n",
    "# Concatenate\n",
    "tser_ibcm = np.concatenate(tser_ibcm, axis=0)\n",
    "nuser_ibcm = np.concatenate(nuser_ibcm)\n",
    "bkvecser_ibcm = np.concatenate(bkvecser_ibcm)\n",
    "mser_ibcm = np.concatenate(mser_ibcm)\n",
    "cbarser_ibcm = np.concatenate(cbarser_ibcm)\n",
    "thetaser_ibcm = np.concatenate(thetaser_ibcm)\n",
    "wser_ibcm = np.concatenate(wser_ibcm)\n",
    "sser_ibcm = np.concatenate(sser_ibcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background process plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_dimensions <= 2:\n",
    "    fig, ax = plt.subplots()\n",
    "    # Use a different color for 3 cases: either odor absent, both odors\n",
    "    where_12 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    where_both = np.logical_and(where_12[:, 0], where_12[:, 1])\n",
    "    where_1 = np.logical_and(where_12[:, 0], ~where_12[:, 1])\n",
    "    where_2 = np.logical_and(~where_12[:, 0], where_12[:, 1])\n",
    "    ax.scatter(bkvecser_ibcm[where_both, 0], bkvecser_ibcm[where_both, 1], label=\"Both odors\")\n",
    "    ax.scatter(bkvecser_ibcm[where_1, 0], bkvecser_ibcm[where_1, 1], label=\"Odor 0 only\")\n",
    "    ax.scatter(bkvecser_ibcm[where_2, 0], bkvecser_ibcm[where_2, 1], label=\"Odor 1 only\")\n",
    "    vecs = np.zeros(x_gamma_vecs.shape)\n",
    "    scale = 0.5\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = x_gamma_vecs[i] / np.sqrt(np.sum(x_gamma_vecs[i]**2)) * scale\n",
    "        ax.annotate(\"\", xytext=(0, 0), xy=x_gamma_vecs[i], \n",
    "                    arrowprops=dict(width=2.0, color=\"k\"))\n",
    "    figname = \"figures/powerlaw/background_two_odors_2d_ornmodel.pdf\"\n",
    "    zlbl = None\n",
    "elif n_components == 3:\n",
    "    dims = (4, 5, 6)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    fig.set_size_inches(6.0, 3.0)\n",
    "    where_123 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    locations = {\n",
    "        \"All odors\": np.all(where_123, axis=1),  #all\n",
    "        \"Odors 0&1\": (where_123[:, 0] & where_123[:, 1] & ~where_123[:, 2]),  # 12\n",
    "        \"Odors 0&2\": (where_123[:, 0] & ~where_123[:, 1] & where_123[:, 2]),  # 13\n",
    "        \"Odors 1&2\": (~where_123[:, 0] & where_123[:, 1] & where_123[:, 2]),  # 23\n",
    "        \"Odor 0 \": (where_123[:, 0] & ~where_123[:, 1] & ~where_123[:, 2]),  #1\n",
    "        \"Odor 1\": (~where_123[:, 0] & where_123[:, 1] & ~where_123[:, 2]), \n",
    "        \"Odor 2\": (~where_123[:, 0] & ~where_123[:, 1] & where_123[:, 2])\n",
    "    }\n",
    "    for lbl, slc in locations.items():\n",
    "        ax.scatter(bkvecser_ibcm[slc, dims[0]], bkvecser_ibcm[slc, dims[1]], \n",
    "                   bkvecser_ibcm[slc, dims[2]], label=lbl)\n",
    "    vecs = np.zeros(x_gamma_vecs.shape)\n",
    "    scale = 0.5\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = x_gamma_vecs[i] / np.sqrt(np.sum(x_gamma_vecs[i]**2)) * scale\n",
    "    ax.quiver(*orig, *(x_gamma_vecs[:, dims].T), color=\"k\", lw=2.0)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=100)\n",
    "    #ax.view_init(azim=30, elev=30)\n",
    "    figname = \"figures/powerlaw/background_three_odors_3d_ornmodel.pdf\"\n",
    "    zlbl = ax.set_zlabel(\"ORN {}\".format(dims[2]))\n",
    "elif n_components == 2:\n",
    "    dims = (0, 1, 4)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    fig.set_size_inches(6.0, 3.0)\n",
    "    where_12 = (nuser_ibcm[:, :, 1] > 0).astype(bool)\n",
    "    locations = {\n",
    "        \"Both odors\": np.all(where_12, axis=1),  # all\n",
    "        \"Odor 0 \": (where_12[:, 0] & ~where_12[:, 1]),  # 1\n",
    "        \"Odor 1\": (~where_12[:, 0] & where_12[:, 1])  # 2\n",
    "    }\n",
    "    for lbl, slc in locations.items():\n",
    "        ax.scatter(bkvecser_ibcm[slc, dims[0]], bkvecser_ibcm[slc, dims[1]], \n",
    "                   bkvecser_ibcm[slc, dims[2]], label=lbl)\n",
    "    vecs = np.zeros(x_gamma_vecs.shape)\n",
    "    scale = 0.5\n",
    "    orig = np.zeros([3, n_components])\n",
    "    for i in range(n_components):\n",
    "        vecs[i] = x_gamma_vecs[i] / np.sqrt(np.sum(x_gamma_vecs[i]**2)) * scale\n",
    "    ax.quiver(*orig, *(x_gamma_vecs[:, dims].T), color=\"k\", lw=2.0)\n",
    "    ax.scatter(0, 0, 0, color=\"k\", s=100)\n",
    "    #ax.view_init(azim=30, elev=30)\n",
    "    figname = \"figures/powerlaw/background_two_odors_3d_ornmodel.pdf\"\n",
    "    ax.set(xlabel=\"ORN {}\".format(dims[0]), ylabel=\"ORN {}\".format(dims[1]))\n",
    "    zlbl = ax.set_zlabel(\"ORN {}\".format(dims[2]))\n",
    "leg = ax.legend(loc=\"upper right\", bbox_to_anchor=(0.0, 1.0))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(figname, transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(zlbl, leg))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the time course of the different neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = int(5/6*duration / deltat) // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, x_gamma_vecs)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. Easiest to compute numerically. \n",
    "conc_ser = nuser_ibcm[:, :, 1]\n",
    "# Odors are all iid so we can average over all odors\n",
    "mean_conc = np.mean(conc_ser)\n",
    "sigma2_conc = np.var(conc_ser)\n",
    "thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "moments_conc = [mean_conc, sigma2_conc, thirdmom_conc]\n",
    "\n",
    "# Analytical prediction\n",
    "res = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1)\n",
    "c_specif, c_nonspecif = res[:2]\n",
    "cs_cn = res[:2]\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 2.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, x_gamma_vecs, cs_cn, specif_gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(tser_ibcm[:300], nuser_ibcm[:300, :, 1])\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm/1000, thetaser_ibcm[:, i], lw=0.5, color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"$\\bar{\\Theta} = \\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=320000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "#ax.set_xlim([350, 360])\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "fig.tight_layout()\n",
    "leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "\n",
    "#fig.savefig(\"figures/powerlaw/cbargammas_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = (nuser_ibcm[:, :, 1] \n",
    "                     - np.mean(nuser_ibcm[transient:, :, 1], axis=0))\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/specificities_turbulent_background_example.pdf\", \n",
    "#           transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, sser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/pn_activity_norm_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/w_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of average fixed points\n",
    "Check the eigenvalues of the jacobian for one neuron, for every possible specificity. There are $2^{n_B}$ possibilities: choosing specific or not for each odor\n",
    "\n",
    "That calculation does not really work here, since the background process is not at all the linear superposition used analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_eigenvalues = ibcm_all_largest_eigenvalues(\n",
    "    moments_conc, ibcm_rates, x_gamma_vecs, m3=1.0, cut=1e-16, options=ibcm_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ibcm_specif_keys = list(all_max_eigenvalues.keys())\n",
    "ibcm_eig_values = np.asarray([all_max_eigenvalues[a] for a in ibcm_specif_keys])\n",
    "reals, imags = np.real(ibcm_eig_values), np.imag(ibcm_eig_values)\n",
    "ibcm_eig_values_specif1 = np.asarray([len(s) == 1 for s in ibcm_specif_keys], dtype=bool)\n",
    "highlights = ibcm_eig_values_specif1\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "scaleup = 1e3\n",
    "ax.plot(reals[highlights]*scaleup, imags[highlights]*scaleup, marker=\"*\", mfc=\"b\", mec=\"b\", \n",
    "        ls=\"none\", label=\"One odor\", ms=8)\n",
    "ax.plot(reals[~highlights]*scaleup, imags[~highlights]*scaleup, marker=\"o\", mfc=\"k\", mec=\"k\", \n",
    "       ls=\"none\", label=\"0 or 2+ odors\", ms=6)\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.legend(title=\"Specificity\")\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{max}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{max}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components*2  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 8.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "# We selected a seed (out of 40+ tested) giving initial conditions leading to correct PCA\n",
    "# The model has trouble converging on this background, we're giving as many chances as possible here. \n",
    "rgen_pca = np.random.default_rng(seed=0x8b6664612cfeda4a121436fcfbbca449)\n",
    "init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_ifpsp_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " sser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.statistics import principal_component_analysis\n",
    "from modelfcts.checktools import compute_pca_meankept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, sser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, snorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response to a new odor\n",
    "This part of the code only runs if the simulation above had ``n_dimensions > n_components``. \n",
    "\n",
    "The goal is to see whether a new odor, not linearly dependent of the ones in the background, also gets repressed close to zero, or produces an inhibited output noticeably different from the inhibited background, and still similar to the new odor vector, at least its component perpendicular to the background subspace. \n",
    "\n",
    "Need to test for many samples from the background odor distribution. Keep the new odor at a constant concentration, typical of the concentration at which we actually want the system to pick up the new odor. \n",
    "\n",
    "I realize that it's fine if the disentanglement of odors isn't perfect at the PN layer: besides the question of habituation, the sparse tag network proposed by Dasgupta does not address too well how multiple odors are disentangled from a complicated mixture. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_odor = np.roll(back_components[0], shift=-1)  # Should be a new vector\n",
    "full_basis = np.hstack([back_components.T, new_odor.reshape(-1, 1)])\n",
    "\n",
    "# Mix new odor with background\n",
    "#new_odor = 0.5*new_odor + 0.5*np.sum(back_components, axis=0) / n_components  # Combine with mean background\n",
    "# Combine with off-average combination of background components\n",
    "new_odor_mix = new_odor + 0.6*back_components[0] + 0.2*np.sum(back_components[1:3], axis=0)\n",
    "# Inhibit with average m synapses and w\n",
    "new_odor_after_inhibition_average = respond_new_odors(new_odor_mix, np.mean(mser_sat[transient:], axis=0), \n",
    "                                                     np.mean(wser_sat[transient:], axis=0), coupling_eta)\n",
    "# Inhibit with latest m and w\n",
    "new_odor_after_inhibition_latest = respond_new_odors(new_odor_mix, mser_sat[-1], wser_sat[-1], coupling_eta)\n",
    "\n",
    "# Show components along the three background vectors plus the new odor vector (full_basis)\n",
    "print(\"Unhinibited new odor mix:\", decompose_nonorthogonal_basis(new_odor_mix, full_basis))\n",
    "print(\"Average:\", decompose_nonorthogonal_basis(new_odor_after_inhibition_average, full_basis))\n",
    "print(\"Latest:\", decompose_nonorthogonal_basis(new_odor_after_inhibition_latest, full_basis), \"\\n\")\n",
    "#print(\"Inhibition of mean:\", decompose_nonorthogonal_basis(np.mean(inhibited_proj_ser_sat[transient:], axis=0), full_basis))\n",
    "\n",
    "print(\"Mixture of new odor plus background in ORN coordinates:\", new_odor_mix)\n",
    "print(\"Average inhibited in ORN coordinates:\", new_odor_after_inhibition_average)\n",
    "print(\"New odor alone in ORN coordinates:\", new_odor, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_new_odors(odors, typical_m, typical_w, coupling):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        odors (np.ndarray): indexed [..., dimension] \n",
    "            so can take dot product properly with m and store many \n",
    "            odors along arbitrary other axes.\n",
    "        typical_m (np.ndarray): indexed [n_neurons, dimension]\n",
    "        typical_w (np.ndarray): indexed [n_neurons, dimension]\n",
    "    \"\"\"\n",
    "    # Compute activation of neurons to each new odor (new+background)\n",
    "    # Given the IBCM and inhibitory neurons' current state \n",
    "    # (either latest or some average state of the neurons)\n",
    "    c = odors.dot(typical_m.T)\n",
    "    cbar = c - coupling*(np.sum(c, axis=-1, keepdims=True) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "    print(cbar)\n",
    "    # Inhibit with the mean cbar*wser, to see how on average the new odor will show\n",
    "    n_neu = typical_m.shape[0]\n",
    "    new_outputs = odors - np.sum(np.expand_dims(cbar, cbar.ndim)*typical_w[np.newaxis, :], axis=-2)/n_neu\n",
    "    return new_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(vecs):\n",
    "    \"\"\" Computes l2 norm of vectors stored along the last axis of vecs. \n",
    "    Args:\n",
    "        vecs can be either a single vector (1d) or an  arbitrary array of vectors, \n",
    "            where the last dimension indexes elements of vectors. \n",
    "    \n",
    "    Returns: if vecs is of shape (K x L x ... M x N), \n",
    "        returns an array of distances of shape (K x L x ... x M) \n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(vecs**2, axis=-1))\n",
    "\n",
    "def l1_norm(vecs):\n",
    "    \"\"\" |x| = \\sum_i |x_i|\"\"\"\n",
    "    return np.sum(np.abs(vecs), axis=-1)\n",
    "\n",
    "def linf_norm(vecs):\n",
    "    \"\"\" |x| = max_i(|x_i|) \"\"\"\n",
    "    return np.max(np.abs(vecs), axis=-1)\n",
    "\n",
    "def cosine_dist(x, y):\n",
    "    \"\"\" d(x, y) = 1 - (x \\cdot y)/(|x| |y|)\"\"\"\n",
    "    xnorm, ynorm = l2_norm(x), l2_norm(y)\n",
    "    return 1.0 - x.dot(np.moveaxis(y, -1, 0)) / xnorm / ynorm\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Generate steady-state background samples and check their distribution before using them\n",
    "conc_samples = sample_ss_conc_powerlaw(*back_params_sym[:-1], size=int(1e5), rgen=None)\n",
    "conc_samples = conc_samples.T  # Each row is an odor now\n",
    "fig, axes = check_conc_samples_powerlaw_exp1(conc_samples, *back_params_sym[:-1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of improvement of recognition\n",
    "# New odor\n",
    "new_odor = np.roll(back_components_sym[0], shift=-1)  # Should be a new vector\n",
    "\n",
    "# mbar dot product with the new odor\n",
    "cdot_new = mser_sat.dot(new_odor)\n",
    "cbardot_new = cdot_new*(1 + coupling_eta) - coupling_eta*np.sum(cdot_new, axis=1, keepdims=True)\n",
    "\n",
    "# Background samples, then add new odor\n",
    "typical_conc = np.mean(back_params_sym[-3]) * np.mean(back_params_sym[-2])\n",
    "mix_samples = sample_background_powerlaw(back_components_sym, *back_params_sym[:-1], size=1000, rgen=rgen_meta)\n",
    "mix_samples = 0.8 * mix_samples + 0.2 * new_odor.reshape(1, -1) * typical_conc\n",
    "mix_samples = np.tanh(mix_samples / 0.5)\n",
    "\n",
    "# Compare to inhibition of the average background\n",
    "#avg_back = averages_nu.dot(back_components_sym)\n",
    "#a_over_ab = inhib_rates[0] / sum(inhib_rates)\n",
    "#inhib_avg_samples = mix_samples - a_over_ab * avg_back.reshape(1, -1)\n",
    "\n",
    "# Average m and w with which we will inhibit\n",
    "msat_mean = np.mean(mser_sat[transient:], axis=0)\n",
    "wsat_mean = np.mean(wser_sat[transient:], axis=0)\n",
    "\n",
    "# Inhibition of each generated sample and statistics on performance\n",
    "inhib_ibcm_samples = respond_new_odors(mix_samples, msat_mean, wsat_mean, coupling_eta)\n",
    "print(np.tanh(0.5*typical_conc*new_odor / 0.5))\n",
    "print(inhib_ibcm_samples)\n",
    "print(mix_samples)\n",
    "\n",
    "dist_pure_inhib_none = distance_panel_target(mix_samples, np.tanh(0.2* typical_conc*new_odor / 0.5))\n",
    "#dist_pure_inhib_avg = distance_panel_target(inhib_avg_samples, new_odor)\n",
    "dist_pure_inhib_ibcm = distance_panel_target(inhib_ibcm_samples, np.tanh(0.2*typical_conc*new_odor / 0.5))\n",
    "\n",
    "median_distances_none = np.median(dist_pure_inhib_none, axis=0)\n",
    "#median_distances_avg = np.median(dist_pure_inhib_avg, axis=0)\n",
    "median_distances_ibcm = np.median(dist_pure_inhib_ibcm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none[i], color=clr_none, ls=\"--\", lw=1.0)\n",
    "    #ax.hist(dist_pure_inhib_avg[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "    #    edgecolor=clr_avg, density=True)\n",
    "    #ax.axvline(median_distances_avg[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_ibcm[:, i], label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True) \n",
    "    ax.axvline(median_distances_ibcm[i], color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#np.mean(cbardot_new[transient:], axis=0)\n",
    "mbarser_sat = mser_sat*(1.0 + coupling_eta) - coupling_eta*np.sum(mser_sat, axis=1, keepdims=True)\n",
    "mbarser_sat_mean = np.mean(mbarser_sat[transient:], axis=0)\n",
    "#print(mbarser_sat_mean)\n",
    "full_basis = np.vstack([back_components_sym, new_odor])\n",
    "#print(full_basis)\n",
    "mbarser_sat_decomposed = np.asarray([decompose_nonorthogonal_basis(mbarser_sat_mean[i], full_basis) \n",
    "                                     for i in range(n_neurons)])\n",
    "print(mbarser_sat_decomposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mser_sat_decomposed = np.asarray([decompose_nonorthogonal_basis(np.mean(mser_sat, axis=0)[i], full_basis) \n",
    "                                     for i in range(n_neurons)])\n",
    "print(mser_sat_decomposed)\n",
    "# Because of the tanh transform, the component orthogonal to the background basis\n",
    "# slowly changes nevertheless. \n",
    "# Idea: add a small decay term -mu*m to the IBCM equations to force components of m orthogonal to the\n",
    "# background to disappear before a new odor appears. \n",
    "\n",
    "minit_bar = init_synapses * (1.0 + coupling_eta) - coupling_eta * np.sum(init_synapses, axis=0, keepdims=True)\n",
    "minit_bar_decomposed = np.asarray([decompose_nonorthogonal_basis(minit_bar[i], full_basis) \n",
    "                                     for i in range(n_neurons)])\n",
    "minit_decomposed = np.asarray([decompose_nonorthogonal_basis(init_synapses[i], full_basis) \n",
    "                                     for i in range(n_neurons)])\n",
    "print(minit_decomposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average dot product of mbars with the new odor\n",
    "np.mean(mbarser_sat_mean.dot(new_odor))\n",
    "#np.mean(mser_sat, axis=0).dot(new_odor)\n",
    "# That's not zero at all, even on average, which causes problems when the time comes\n",
    "# to inhibit the background plus odor mix. Not sure why. Need to fix that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the w_i, check whether they have a component not in the subspace of the x_gamma of the background\n",
    "wmean_sat = np.mean(wser_sat[transient:], axis=0)\n",
    "w_sat_decomp = np.asarray([decompose_nonorthogonal_basis(wmean_sat[i], full_basis) for i in range(n_neurons)])\n",
    "wmean_sat_norm = wmean_sat / np.sqrt(np.sum(wmean_sat**2, axis=1, keepdims=True))\n",
    "w_sat_norm_decomp = np.asarray([decompose_nonorthogonal_basis(wmean_sat_norm[i], full_basis)\n",
    "                                for i in range(n_neurons)])\n",
    "#print(wmean_sat)\n",
    "print(w_sat_decomp[:4])\n",
    "#print(wmean_sat_norm)\n",
    "#print(w_sat_norm_decomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cgammas_w = np.dot(cgammas_bar_mean_sat[[0, 1, 2]].T, wmean_sat[[0, 1, 2]]) / 3\n",
    "#[decompose_nonorthogonal_basis(sum_cgammas_w[i], full_basis) for i in range(n_components)]\n",
    "back_components_sym - sum_cgammas_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cgammas_bar_mean_sat == cgammas_bar_mean_sat.max(axis=1, keepdims=True), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wi_basis = np.vstack([wmean_sat[[0, 1, 2]], new_odor])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "vecs2 = wmean_sat[[0, 1, 2]]\n",
    "vecs2 = vecs2 / np.sqrt(np.sum(vecs2**2, axis=1, keepdims=True))\n",
    "vecs = vecs / np.sqrt(np.sum(vecs**2, axis=1, keepdims=True))\n",
    "ax.quiver(*(orig[0:3]), *(vecs2[:, 0:3].T), color=\"b\", lw=2.0)\n",
    "ax.quiver(*(orig[0:3]), *(vecs[:, 0:3].T), color=\"k\", lw=2.0)\n",
    "ax.set_xlim(0.0, max(vecs2.max(), vecs.max()))\n",
    "ax.set_ylim(0.0, max(vecs2.max(), vecs.max()))\n",
    "ax.set_zlim(0.0, max(vecs2.max(), vecs.max()))\n",
    "ax.view_init(azim=60, elev=30)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Stop here for now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric to measure the quality of the inhibition\n",
    "Distance to new odor alone? Compare to un-inhibited mixture?\n",
    "\n",
    "Ultimately, will compute sparse binary neural tag and compare with Jaccard metric, but for now, I want to avoid this complication, which requires using many more dimensions than 4. I might keep this for a separate notebook (or even C code if it seems to work well). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Random odor vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic model of olfactory receptor activation patterns:\n",
    "# each component is i.i.d. exponential\n",
    "def generate_odorant(n_rec, rgen, lambda_in=0.1):\n",
    "    \"\"\" Generate vectors eta and kappa^-1 for an odorant, with antagonism parameter rho. \n",
    "    \n",
    "    Args:\n",
    "        n_rec (int): number of receptor types, length of vectors\n",
    "        rgen (np.random.Generator): random generate (numpy >= 1.17)\n",
    "        lambda_in (float): lambda parameter of the exp distribution\n",
    "            Equals the inverse of the average of each vector component\n",
    "    Returns:\n",
    "        kappa1_vec (np.ndarray): 1d vector of receptor activities\n",
    "    \"\"\"\n",
    "    return rgen.exponential(scale=1.0/lambda_in, size=n_rec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 3  # Half the real number for faster simulations\n",
    "# The larger the dimension, the more likely the odors are orthogonal. \n",
    "n_components = 3  # no need to look at super complicated odors for now; keep effective space 3D\n",
    "# Can actually look at this latent space by Gram-Schmidt to find orthogonal axes spanning the input odors. \n",
    "n_neurons = 16  # Start small\n",
    "\n",
    "# Simulation times\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.001\n",
    "tau_avg = 200\n",
    "coupling_eta = 0.05 / n_neurons\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=92387)\n",
    "init_synapses = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Choose random exponential LI vectors\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=1.0)\n",
    "print(back_components)\n",
    "\n",
    "\n",
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# m_init, update_bk, bk_init, inhib_params, bk_params, tmax, dt, learnrate, seed=14345124, noisetype=\"normal\", tavg=10, coupling=0.1\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_alternating_inputs, init_back_altern, inhib_rates,\n",
    "                    back_params_altern, duration, deltat, learnrate=learnrate, seed=509811537, \n",
    "                    noisetype=\"uniform\", tavg=tau_avg, coupling=coupling_eta)\n",
    "tser, mser, nuser, cser, cbarser, thetaser, wser, bkvecser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete model: sparse Kenyon cell tags for odors\n",
    "We need to make new simulations with many more dimensions (ORN types). \n",
    "\n",
    "Consequently, to avoid running into memory issues, we only save a subset of time steps in the simulation: this is fine because we are only interested in the slowly-evolving $\\vec{w}$ and $\\vec{m}$, while we don't care too much for $\\vec{x}$'s fast fluctuations. We just want the final average $\\vec{w}$ to apply as inhibition to randomly sampled background odors, which we don't even take from simulations but just generate from the steady-state distribution. \n",
    "\n",
    "### Run a new simulation with 25 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions_tag = 25  # Half the real number for faster simulations\n",
    "n_neurons = 64\n",
    "\n",
    "# Simulation rates and coupling stay the same (try at least)\n",
    "duration = 160000.0\n",
    "deltat = 1.0\n",
    "tau_nu = 2.0  # Correlation time scale of the background nu_gammas (same for all)\n",
    "learnrate = 0.005\n",
    "tau_avg = 25\n",
    "inhib_rates = [0.0025, 0.0005]  # alpha, beta\n",
    "# Background components need to be redefined. Extra dimensions are somewhat superfluous\n",
    "coupling_eta = 0.5/n_neurons\n",
    "\n",
    "# Choose symmetric, normalized background odor components\n",
    "#back_components_tag = np.ones([n_components, n_dimensions_tag]) * 0.2\n",
    "#for i in range(n_components):\n",
    "#    back_components_tag[i, i] = 0.8\n",
    "#    back_components_tag[i] /= np.sqrt(np.sum(back_components_tag[i]**2))\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta_tag = np.random.default_rng(seed=38981211111)\n",
    "back_components_tag = np.zeros([n_components, n_dimensions_tag])\n",
    "for i in range(n_components):\n",
    "    back_components_tag[i] = generate_odorant(n_dimensions_tag, rgen_meta_tag, lambda_in=0.1)\n",
    "print(back_components_tag)\n",
    "back_components_tag = back_components_tag / l2_norm(back_components_tag).reshape(-1, 1)\n",
    "    \n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_tag = 0.1*rgen_meta.random(size=[n_neurons, n_dimensions_tag])\n",
    "\n",
    "# Initial nu values stay the same\n",
    "init_bkvec_tag = averages_nu.dot(back_components_tag)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list_tag = [init_nu, init_bkvec_tag]\n",
    "\n",
    "# Update matrices for nu process stay the same\n",
    "back_params_tag = [update_mat_A, update_mat_B, back_components_tag, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a heavy simulation\n",
    "skp_tag = 20\n",
    "sim_results = integrate_inhib_ibcm_network_skip(init_synapses_tag, update_thirdmoment_kinputs, init_back_list_tag, \n",
    "                    inhib_rates, back_params_tag, duration, deltat, learnrate=learnrate, seed=73001317, \n",
    "                    noisetype=\"normal\", tavg=tau_avg, coupling=coupling_eta, skp=skp_tag)\n",
    "tser_tag, mser_tag, nuser_tag, cser_tag, cbarser_tag, thetaser_tag, wser_tag, bkvecser_tag = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp = 50\n",
    "fig, ax = plt.subplots()\n",
    "w1_palette = sns.color_palette(\"Blues\", n_colors=n_neurons)\n",
    "w2_palette = sns.color_palette(\"Purples\", n_colors=n_neurons)\n",
    "w3_palette = sns.color_palette(\"Greens\", n_colors=n_neurons)\n",
    "for i in range(n_neurons-1):\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 0], color=w1_palette[i], alpha=0.8)\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 1], color=w2_palette[i], alpha=0.8)\n",
    "    ax.plot(tser_tag[::skp], mser_tag[::skp, i, 2], color=w3_palette[i], alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 0], color=w1_palette[-1], label=\"Neuron Component 0\", alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 1], color=w2_palette[-1], label=\"Neuron Component 1\", alpha=0.8)\n",
    "ax.plot(tser_tag[::skp], mser_tag[::skp, -1, 2], color=w3_palette[-1], label=\"Neuron Component 2\", alpha=0.8)\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Inhibition neurons components\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and compare projection tags after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New odor, mix, and inhibit\n",
    "### Repeat for many new odors (and ideally, should repeat for many backgrounds)\n",
    "### But for now, assume all simulations would give similarly good inhibition. \n",
    "n_test_new_odors = 100\n",
    "mix_frac = 0.5\n",
    "\n",
    "# Average m and w with which we will inhibit\n",
    "transient_tag = 100000 // skp_tag\n",
    "mtag_mean = np.mean(mser_tag[transient_tag:], axis=0)\n",
    "wtag_mean = np.mean(wser_tag[transient_tag:], axis=0)\n",
    "\n",
    "# Background samples, valid for all new test odors\n",
    "back_samples_tag = sample_background_thirdmoment(averages_nu, steady_covmat, epsilon_nu, back_components_tag, \n",
    "                                                  size=100, rgen=rgen_meta_tag)\n",
    "inhib_ibcm_samples_tag = []\n",
    "inhib_avg_samples_tag = []\n",
    "mix_samples_tag = []\n",
    "new_odor_targets = []\n",
    "for i in range(n_test_new_odors):\n",
    "    # New odor\n",
    "    #new_odor_tag = np.roll(back_components_tag[0], shift=-1)  # Should be a new vector\n",
    "    new_odor_tag = generate_odorant(n_dimensions_tag, rgen_meta_tag)\n",
    "    new_odor_tag = new_odor_tag / l2_norm(new_odor_tag)\n",
    "    new_odor_targets.append(new_odor_tag)\n",
    "\n",
    "    mix_samples = back_samples_tag*(1.0 - mix_frac) + new_odor_tag.reshape(1, -1)*mix_frac\n",
    "    mix_samples_tag.append(mix_samples)\n",
    "    \n",
    "    # Compare to inhibition of the average background\n",
    "    avg_back_tag = averages_nu.dot(back_components_tag)\n",
    "    a_over_ab = inhib_rates[0] / sum(inhib_rates)\n",
    "    inhib_avg_samples = mix_samples - a_over_ab * avg_back_tag.reshape(1, -1)\n",
    "    inhib_avg_samples_tag.append(inhib_avg_samples)\n",
    "\n",
    "    # Inhibition of each generated sample and statistics on performance\n",
    "    inhib_ibcm_samples = respond_new_odors(mix_samples, mtag_mean, wtag_mean, coupling_eta)\n",
    "    inhib_ibcm_samples_tag.append(inhib_ibcm_samples)\n",
    "\n",
    "mix_samples_tag = np.asarray(mix_samples_tag)\n",
    "inhib_avg_samples_tag = np.asarray(inhib_avg_samples_tag)\n",
    "inhib_ibcm_samples_tag = np.asarray(inhib_ibcm_samples_tag)\n",
    "new_odor_targets = np.asarray(new_odor_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags. This won't be great because too few dimensions to begin with, but try anyways. \n",
    "projtag_kwargs = dict(kc_sparsity=0.05, adapt_kc=True, n_pn_per_kc=6, fix_thresh=None)\n",
    "proj_mat = create_sparse_proj_mat(n_kc=int(2000/50*n_dimensions_tag), n_rec=n_dimensions_tag, \n",
    "                        rgen=rgen_meta, fraction_filled=projtag_kwargs[\"n_pn_per_kc\"]/n_dimensions_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_none = []\n",
    "jaccards_inhib_avg = []\n",
    "jaccards_inhib_ibcm = []\n",
    "for i in range(mix_samples_tag.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(mix_samples_tag.shape[1]):\n",
    "        tag_none = project_neural_tag(mix_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_avg = project_neural_tag(inhib_avg_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        tag_ibcm = project_neural_tag(inhib_ibcm_samples_tag[i, j], mix_samples_tag[i, j], proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_none.append(jaccard(target_tag, tag_none))\n",
    "        jaccards_inhib_avg.append(jaccard(target_tag, tag_avg))\n",
    "        jaccards_inhib_ibcm.append(jaccard(target_tag, tag_ibcm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "\n",
    "ax.hist(jaccards_inhib_none, label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_none), color=clr_none, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_avg, ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_ibcm, label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ibcm), color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#if mix_frac == 0.2:\n",
    "#    fig.savefig(\"figures/detection/jaccard_similarity_ibcm_average_none_f20percent.pdf\", transparent=True)\n",
    "#elif mix_frac == 0.5:\n",
    "#    fig.savefig(\"figures/detection/jaccard_similarity_ibcm_average_none_f50percent.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PN distances\n",
    "dist_pure_inhib_none_tag = distance_panel_target(mix_samples_tag/2, new_odor_tag)\n",
    "dist_pure_inhib_avg_tag = distance_panel_target(inhib_avg_samples_tag, new_odor_tag)\n",
    "dist_pure_inhib_ibcm_tag = distance_panel_target(inhib_ibcm_samples_tag, new_odor_tag)\n",
    "\n",
    "median_distances_none_tag = np.median(dist_pure_inhib_none_tag, axis=0)\n",
    "median_distances_avg_tag = np.median(dist_pure_inhib_avg_tag, axis=0)\n",
    "median_distances_ibcm_tag = np.median(dist_pure_inhib_ibcm_tag, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none_tag[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none_tag[i], color=clr_none, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_avg_tag[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "        edgecolor=clr_avg, density=True)\n",
    "    ax.axvline(median_distances_avg_tag[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_ibcm_tag[:, i], label=\"IBCM inhibition\", facecolor=clr_ibcm, alpha=0.6, \n",
    "        edgecolor=clr_ibcm, density=True) \n",
    "    ax.axvline(median_distances_ibcm_tag[i], color=clr_ibcm, ls=\"--\", lw=1.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important potential issue\n",
    "The way I inhibit the background based on the IBCM neurons' $\\vec{m}$ and activity $c$ and the inhibitory neurons' averaged weights $\\vec{w}$, that is, \n",
    "\n",
    "$$ \\vec{s} = \\vec{x} - \\sum_i \\overline{c}^i \\vec{w}^i $$\n",
    "\n",
    "is missing an important aspect: control of the inhibition intensity depending on a notion of \"expected\" fluctuations of the background. \n",
    "    \n",
    "The IBCM neurons and inhibitory neurons are, together, learning ($\\vec{w}$, $\\vec{m}$) off-average components, and getting activated ($c$) when those components are more or less present in the background. Very well. \n",
    "\n",
    "But then, how much should those components be suppressed when they are present? The network should not just \"blindly\" suppress each component in its entirety. Rather, we want it to suppress only \"typical\" levels of those components, and let through \"new\" or \"unexpected\" levels of those components. \n",
    "\n",
    "\n",
    "So, what we want are more neurons that learn the typical distribution of the activation levels $c^i$ of the different pairs of neurons (sensitive to different components), and prevent excessive inhibition when the $c^i$ reach highly improbable values. \n",
    " - One way to do this: would need to actually estimate the density of the joint distribution of the $c^i$, and stop inhibiting when the $c^i$ jump to low likelihood values. But that is hard and not necessarily so efficient. \n",
    " - Another, simpler way to do this: learn limits (e.g. mean plus or minus stdev) on $c$, and clip the inhibition levels controlled by $c$ in $\\vec{s}$. So, we should not just use , but some function of $c^i$ which is peaked at the mean and eventually decays to zero when $c$ goes too far off that mean. \n",
    " \n",
    "So, the idea is that, instead of pairs of IBCM and inhibitory neurons, we would have triplets: IBCM, inhibitory, gating. The \"gating\" neurons would have to learn a response function to $c$ which is like a gaussian. \n",
    "\n",
    "Is there a learning rule such that the output of a neuron is the \"probability\" of its input value? I'm sure there is, but unclear how to get that. \n",
    "\n",
    "I think the IBCM model may actually be able to do that. So we would need to put a second IBCM neuron that tracks the activity $c$ of the first and respond strongly to the mean $c$, and less and less strongly to off-average values. Is that the \"coincidence detector\" proposed by Intrator, 1997? Maybe! \n",
    "\n",
    "Read again this 1997 paper with this need of a system that learns \"expected\" fluctuations and lets through \"novelties\". \n",
    "\n",
    "### Summary of the inhibition model to explore:\n",
    "Have many triplets of neurons, each triplet learning to inhibit a different component of the background appropriately with respect to the distribution of fluctuations of that background. The three neurons in a triplet and their role are:\n",
    " - \"Component\" IBCM neuron: takes as an input the ORN levels, becomes specific to an off-average component in the background. \n",
    " - Inhibitory neuron: learn the background odor to which the \"component\", by averaging the ORN inputs weighted by the activity c of the \"component\" IBCM neuron (i.e. learning the average c^i*x). \n",
    " - \"Activity\" IBCM neuron: takes as an input the activation c of the \"component\" IBCM neuron, and controls the extent at which the component is suppressed from the projection neurons, depending on its own output, call it p^i: s = x - p^i w^i.  \n",
    " \n",
    "In prime, we could maybe get a novelty detection on an even slower time scale with one more IBCM neuron taking as a vector of input the activation level of all \"component\" IBCM neurons, and becoming specific to certain combinations of backgrounds. \n",
    "\n",
    "I'm not sure the \"activity\" neuron can be IBCM, but the goal is the same, just maybe need a different kind of neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to ideal inhibitory network\n",
    "In a linear algebra perspective, the best inhibition that could possibly be achieved of a new odor plus background mixture is that the whole component of the new odor parallel to the vector subspace spanned by the background odors is suppressed, while the component perpendicular to it is kept. Indeed, the appearance of the new odor's component in the background space cannot be distinguished from a fluctuation of the background (unless we had neurons tracking statistics of typical activations in that space, but not obvious how to get that). At any rate, this is the best we can hope our IBCM inhibition network will achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_projector(a):\n",
    "    \"\"\" Calculate projector a a^+, which projects\n",
    "    a column vector on the vector space spanned by columns of a. \n",
    "    \"\"\"\n",
    "    a_inv = np.linalg.pinv(a)\n",
    "    return a.dot(a_inv)\n",
    "    \n",
    "def find_parallel_component(x, basis, projector=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (np.ndarray): 1d array of length D containing the vector to decompose. \n",
    "        basis (np.ndarray): 2d matrix of size DxK where each column is one\n",
    "            of the linearly independent background vectors. \n",
    "        projector (np.ndarray): 2d matrix A A^+, the projector on the vector\n",
    "            space spanned by columns of basis. \n",
    "    Return:\n",
    "        x_par (np.ndarray): component of x found in the vector space of basis\n",
    "            The perpendicular component can be obtained as x - x_par. \n",
    "    \"\"\"\n",
    "    # If the projector is not provided yet\n",
    "    if projector is None:\n",
    "        # Compute Moore-Penrose pseudo-inverse and AA^+ projector\n",
    "        projector = find_projector(basis)\n",
    "    x_par = projector.dot(x)\n",
    "    return x_par\n",
    "\n",
    "def ideal_linear_inhibitor(x_n_par, x_n_ort, x_back, f, alpha, beta):\n",
    "    \"\"\" Calculate the ideal projection neuron layer, which assumes\n",
    "    perfect inhibition (down to beta/(alpha+beta)) of the component of the mixture\n",
    "    parallel to the background odors' vector space, while leaving the orthogonal\n",
    "    component of the new odor untouched. \n",
    "    \n",
    "    Args:\n",
    "        x_n_par (np.1darray): new odor, component parallel to background vector space\n",
    "        x_n_ort (np.1darray): new odor, component orthogonal to background vector space \n",
    "        x_back (np.2darray): background samples, one per row\n",
    "        f (float): mixture fraction (hard case is f=0.2)\n",
    "        alpha (float): inhibitory weights learning rate alpha\n",
    "        beta (float): inhibitory weights decaying rate beta\n",
    "    \n",
    "    Returns:\n",
    "        s (np.1darray): projection neurons after perfect linear inhibition\n",
    "    \"\"\"\n",
    "    # Allow broadcasting for multiple x_back vectors\n",
    "    factor = beta / (alpha + beta)\n",
    "    s = factor * f*x_n_par + f*x_n_ort\n",
    "    # I thought the following would have been even better, but turns out it is worse for small f\n",
    "    #s = f*x_n_par + f*x_n_ort\n",
    "    s = s.reshape(1, -1) + factor * (1.0-f) * x_back\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse each new odor in new_odor_targets and each background in back_samples_tag\n",
    "# Compute the projector on the background odor components only once\n",
    "# Compute parallel component of each new odor\n",
    "# Mix it with all background samples at once using broadcasting capability of ideal_linear_inhibitor function\n",
    "background_projector = find_projector(back_components_tag.T)\n",
    "inhib_ideal_samples_tag = []\n",
    "for od in new_odor_targets:\n",
    "    # Decompose\n",
    "    od_par = find_parallel_component(od, basis=back_components_tag.T, projector=background_projector)\n",
    "    od_ort = od - od_par\n",
    "    # Compute the perfectly inhibited mixture with each background sample\n",
    "    inhib_ideal = ideal_linear_inhibitor(od_par, od_ort, back_samples_tag, mix_frac, *inhib_rates)\n",
    "    # Background reduced to b/(a+b), new odor intact? Perfect inhibition\n",
    "    #inhib_ideal = inhib_rates[1] / sum(inhib_rates) * (1.0 - mix_frac) * back_samples_tag + mix_frac * od.reshape(1, -1)\n",
    "    inhib_ideal_samples_tag.append(inhib_ideal)\n",
    "inhib_ideal_samples_tag = np.asarray(inhib_ideal_samples_tag)\n",
    "\n",
    "# Compute neural tags of the ideal inhibited mixtures and compare to target tags. \n",
    "# Compute tags and Jaccard distances between target odor and mixture without or with inhibition\n",
    "jaccards_inhib_ideal = []\n",
    "for i in range(new_odor_targets.shape[0]):\n",
    "    target_tag = project_neural_tag(new_odor_targets[i], new_odor_targets[i], proj_mat, **projtag_kwargs)\n",
    "    for j in range(back_samples_tag.shape[0]):\n",
    "        mix_sample_tag = back_samples_tag[j] + mix_frac * new_odor_targets[i]\n",
    "        tag_ideal = project_neural_tag(inhib_ideal_samples_tag[i, j], mix_sample_tag, proj_mat, **projtag_kwargs)\n",
    "        jaccards_inhib_ideal.append(jaccard(target_tag, tag_ideal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Jaccard similarities: larger similarity is better\n",
    "fig, ax = plt.subplots()\n",
    "clr_map = {\"none\": \"xkcd:navy blue\", \"average\": \"xkcd:orangey brown\", \n",
    "           \"ibcm\":\"xkcd:turquoise\", \"ideal\": \"xkcd:powder blue\", \"ideal2\":\"xkcd:pale rose\"}\n",
    "\n",
    "ax.hist(jaccards_inhib_ideal, label=\"Ideal inhibition\", facecolor=clr_map[\"ideal\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ideal\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ideal), color=clr_map[\"ideal\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_avg, label=\"Average inhibition\", facecolor=clr_map[\"average\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"average\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_avg), color=clr_map[\"average\"], ls=\"--\", lw=1.0)\n",
    "ax.hist(jaccards_inhib_ibcm, label=\"IBCM inhibition\", facecolor=clr_map[\"ibcm\"], alpha=0.6, \n",
    "        edgecolor=clr_map[\"ibcm\"], density=True)\n",
    "ax.axvline(np.median(jaccards_inhib_ibcm), color=clr_map[\"ibcm\"], ls=\"--\", lw=1.0)\n",
    "\n",
    "ax.set(xlabel=\"Jaccard similarity\", ylabel=\"Probability density\", title=\"Jaccard similarity (higher is better)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance as a function of f\n",
    "I expect to see a relatively sharp drop of median performance for IBCM a bit above $f=\\beta/(\\alpha + \\beta)$, and at this value for the ideal inhibition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median_performances(back_samples, new_odors, f, projmat, proj_kwargs, \n",
    "                                m_mean, w_mean, eta, inhib_ab, back_components):\n",
    "    \"\"\" Compute median Jaccard similarity for the different inhibition methods we have, \n",
    "    for a given value of mixture parameter f. \"\"\"\n",
    "    all_jaccard_pairs_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}  # list of lists, one per method\n",
    "    back_proj = find_projector(back_components.T)\n",
    "    for i in range(new_odors.shape[0]):\n",
    "        # Compute target tag\n",
    "        target_tag = project_neural_tag(new_odors[i], new_odors[i], projmat, **proj_kwargs)\n",
    "        # Prepare mixtures\n",
    "        mix_samples = back_samples*(1.0 - f) + new_odors[i:i+1]*f\n",
    "        \n",
    "        # Compute inhibited mixtures with the different methods\n",
    "        # No inhibition: just use mix_samples\n",
    "        # Average inhibition\n",
    "        avg_back_tag = averages_nu.dot(back_components_tag)\n",
    "        a_over_ab = inhib_ab[0] / sum(inhib_ab)\n",
    "        inhib_avg_samples = mix_samples - a_over_ab * avg_back_tag.reshape(1, -1)\n",
    "\n",
    "        # Inhibition of each generated sample and statistics on performance\n",
    "        inhib_ibcm_samples = respond_new_odors(mix_samples, m_mean, w_mean, eta)\n",
    "        \n",
    "        # Ideal inhibition\n",
    "        od_par = find_parallel_component(new_odors[i], basis=back_components.T, projector=back_proj)\n",
    "        od_ort = new_odors[i] - od_par\n",
    "        # Compute the perfectly inhibited mixture with each background sample\n",
    "        inhib_ideal_samples = ideal_linear_inhibitor(od_par, od_ort, back_samples, f, *inhib_ab)\n",
    "        # Background reduced to b/(a+b), new odor intact?\n",
    "        inhib_ideal2_samples = (1.0 - a_over_ab) * (1.0 - f) * back_samples + f * new_odors[i:i+1]\n",
    "    \n",
    "        # For each inhibited mixture, compute jaccard similarity\n",
    "        current_jaccard_dict = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "        for j in range(back_samples.shape[0]):\n",
    "            mix_tag = project_neural_tag(mix_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"none\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Average\n",
    "            mix_tag = project_neural_tag(inhib_avg_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"average\"].append(jaccard(target_tag, mix_tag))\n",
    "        \n",
    "            # IBCM\n",
    "            mix_tag = project_neural_tag(inhib_ibcm_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ibcm\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal\n",
    "            mix_tag = project_neural_tag(inhib_ideal_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "            # Ideal 2\n",
    "            mix_tag = project_neural_tag(inhib_ideal2_samples[j], mix_samples[j], projmat, **proj_kwargs)\n",
    "            current_jaccard_dict[\"ideal2\"].append(jaccard(target_tag, mix_tag))\n",
    "            \n",
    "        # Add those values to the total list\n",
    "        for method in all_jaccard_pairs_dict.keys():\n",
    "            all_jaccard_pairs_dict[method].append(current_jaccard_dict[method])\n",
    "    \n",
    "    # Convert to 2d array and compute median\n",
    "    # Could choose to have one median per odor or per background sample\n",
    "    all_jaccard_pairs_dict = {k:np.asarray(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    median_jaccard_pairs_dict = {k:np.median(a) for k, a in all_jaccard_pairs_dict.items()}\n",
    "    return median_jaccard_pairs_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previous functions for various f values\n",
    "median_jaccards = {\"none\":[], \"average\":[], \"ibcm\":[], \"ideal\":[], \"ideal2\":[]}\n",
    "f_range = np.arange(0.1, 0.8, 0.1)\n",
    "for f in f_range:\n",
    "    meds = compute_median_performances(back_samples_tag, new_odor_targets, f, proj_mat, projtag_kwargs, \n",
    "                                mtag_mean, wtag_mean, coupling_eta, inhib_rates, back_components_tag)\n",
    "    for k in meds:\n",
    "        median_jaccards[k].append(meds[k])\n",
    "    print(\"Done f = {:.2f}\".format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labelmap = {\"none\":\"None\", \"average\":\"Average\", \"ibcm\":\"IBCM\", \"ideal\":r\"Ideal $\\perp$\", \"ideal2\":\"Ideal all\"}\n",
    "for k in median_jaccards:\n",
    "    ax.plot(f_range, median_jaccards[k], color=clr_map[k], label=labelmap[k], lw=3)\n",
    "ax.set(xlabel=\"Fraction $f$ of new odor\", ylabel=\"Median Jaccard similarity\")\n",
    "ax.legend(title=\"Inhibition method\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/inhibition_jaccard_comparison_methods.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical prediction for the \"perfect\" inhibition model\n",
    "Perfect inhibition: only the background is reduced to $\\frac{\\beta}{\\alpha + \\beta}$ of its original amplitude, while the new odor is untouched:\n",
    "\n",
    "$$ \\vec{s} = \\frac{\\beta}{\\alpha + \\beta} (1-f) \\vec{x}^b(t) + f \\vec{x}^n $$\n",
    "\n",
    "One would think this is the best possible inhibition, but for low $f$, it fares worse than deleting the parallel component of $\\vec{x}^n$ as well, because inhibition of the parallel component lowers even further the activity of Kenyon cells specific to the background; the new odor does not reinforce them and only KCs specific to the new odor can cross the threshold. \n",
    "\n",
    "My analytical calculation is for this \"perfect\" inhibition, because it is easier to treat analytically than the . Also, I computed the mean rather than median Jaccard similarity. Anyways, let's see what it looks like and if it makes sense. \n",
    "\n",
    "UPDATE: It just does not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_approx_jaccard_perfect_inhibition_wrong(proj_kappa, avg_orn, avg_nu, k_back, \n",
    "                                                 kc_sparsity, f_mix, ab_rates):\n",
    "    \"\"\"\n",
    "    This is not right at all, but I tried nevertheless. The average delta is causing\n",
    "    a lot of issues. It seems one should really marginalize over deltas rather\n",
    "    than replace it by some average delta value. \n",
    "    Args:\n",
    "        proj_kappa (int): number of non-zero elements per row of the projection matrix\n",
    "        avg_orn (float): average value of a component of an odor vector\n",
    "        avg_nu (float): average value of the linear combination coefficients for t\n",
    "            the background odors. \n",
    "        k_back (int): number of odors in the background\n",
    "        kc_sparsity (float): fraction of KC cells kept in the tag (usually 0.05)\n",
    "        f_mix (float): fraction of the mixture attributed to the new odor\n",
    "        ab_rates (list of 2 floats): alpha, beta\n",
    "    \"\"\"\n",
    "    # 1. Each KC activity, y_i, in response to the new odor follows gamma(proj_kappa, 1/avg_orn)\n",
    "    # and in response to the average background, gamma(proj_kappa*k_back, 1/avg_orn/avg_nu)\n",
    "    # 2.1 Calculate the threshold of most active KC cells in response to the new odor alone. \n",
    "    tau_new = sp.special.gammaincinv(proj_kappa, 1 - kc_sparsity) * avg_orn\n",
    "    \n",
    "    # 2.2 Calculate average margin above threshold, Delta, of the activities of the most active\n",
    "    # KC in response to the new odor alone.\n",
    "    # Try an approximation first: use some fraction of tau_new itself\n",
    "    avg_delta = 0.05*tau_new\n",
    "    # Otherwise, need to calculate complicated integral numerically\n",
    "    \n",
    "    # 3. Compute the probability, on average, that one of the KCs in the\n",
    "    # new odor tag is in the tag of the background-new odor mixture\n",
    "    lamb_nu = 1.0 / (avg_orn * avg_nu)\n",
    "    factor = f_mix/(1 - f_mix) * sum(ab_rates)/ab_rates[1]\n",
    "    avg_common_i = sp.special.gammaincc(proj_kappa*k_back, lamb_nu*(tau_new - factor * avg_delta))\n",
    "    \n",
    "    # 4. The average Jaccard similarity, using the inclusion-exclusion rule, is\n",
    "    avg_jaccard = avg_common_i / (2.0 - avg_common_i)  # Ranges between 0 and 1, as it should\n",
    "    \n",
    "    return avg_jaccard\n",
    "\n",
    "def analytical_approx_jaccard_perfect_inhibition(proj_kappa, avg_orn, avg_nu, k_back, \n",
    "                                                 kc_sparsity, f_mix, ab_rates):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proj_kappa (int): number of non-zero elements per row of the projection matrix\n",
    "        avg_orn (float): average value of a component of an odor vector\n",
    "        avg_nu (float): average value of the linear combination coefficients for t\n",
    "            the background odors. \n",
    "        k_back (int): number of odors in the background\n",
    "        kc_sparsity (float): fraction of KC cells kept in the tag (usually 0.05)\n",
    "        f_mix (float): fraction of the mixture attributed to the new odor\n",
    "        ab_rates (list of 2 floats): alpha, beta\n",
    "    \"\"\"\n",
    "    # 1. Each KC activity, y_i, in response to the new odor follows gamma(proj_kappa, 1/avg_orn),\n",
    "    # and in response to the average background, gamma(proj_kappa*k_back, 1/avg_orn/avg_nu)\n",
    "    # 2. Calculate the threshold of most active KC cells in response to the new odor alone. \n",
    "    tau_new = sp.special.gammaincinv(proj_kappa, 1 - kc_sparsity) * avg_orn\n",
    "    \n",
    "    # 3. Calculate the threshold of most active KC cells \n",
    "    # in response to the average background alone\n",
    "    tau_b = sp.special.gammaincinv(proj_kappa*k_back, 1 - kc_sparsity) * (avg_orn * avg_nu)\n",
    "    \n",
    "    # The probability of a KC from the new odor's tag to be present in the mixture\n",
    "    # tag as well is given by:\n",
    "    #     avg_common_i = sp.special.gammaincc(proj_kappa*k_back, lamb_nu*(tau_new - factor * delta_i))\n",
    "    # for a given value of delta_i. We need to average this over possible delta_i values\n",
    "    # The prob distribution of delta_i is the gamma distribution, conditioned on knowing that y_i is larger\n",
    "    # than tau_i, which is the (100*(1-kc_sparsity))-th percentile \n",
    "    # (so it is a gamma pdf(tau+delta) divided by kc_sparsity)\n",
    "    loglambda_k = -proj_kappa * np.log(avg_orn)\n",
    "    factor = f_mix/(1 - f_mix) * sum(ab_rates)/ab_rates[1]\n",
    "    lamb_nu = 1.0 / (avg_orn * avg_nu)\n",
    "    loggammak = sp.special.gammaln(proj_kappa)\n",
    "    def integrand(delt):\n",
    "        pdf_delta = (proj_kappa - 1)*np.log(tau_new + delt)  - (tau_new + delt)/avg_orn + loglambda_k\n",
    "        pdf_delta = pdf_delta - loggammak\n",
    "        cumul_dist_yib = sp.special.gammaincc(proj_kappa*k_back, max(0, lamb_nu*(tau_b - factor * delt)))\n",
    "        return np.exp(pdf_delta) * cumul_dist_yib\n",
    "    \n",
    "    \n",
    "    # Integrate (we kept the conditional prob. factor 1/kc_sparsity for the end)\n",
    "    avg_common_i = sp.integrate.quad(integrand, 0, np.inf)[0]\n",
    "    avg_common_i /= kc_sparsity\n",
    "    \n",
    "    # 4. The average Jaccard similarity, using the inclusion-exclusion rule, is\n",
    "    avg_jaccard = avg_common_i / (2.0 - avg_common_i)  # Ranges between 0 and 1, as it should\n",
    "    \n",
    "    return avg_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the analytical prediction for each f in f_range\n",
    "predicted_perfect_jaccard = []\n",
    "for f in f_range:\n",
    "    predicted_perfect_jaccard.append(analytical_approx_jaccard_perfect_inhibition(\n",
    "        proj_kappa=projtag_kwargs[\"n_pn_per_kc\"], \n",
    "        avg_orn=np.mean(back_components_tag), \n",
    "        avg_nu=averages_nu.mean() + epsilon_nu*sigma2, \n",
    "        k_back=back_components_tag.shape[0],\n",
    "        #avg_nu=averages_nu.mean()*0.36,  # Hack to get real distrib. of KC in response to background alone\n",
    "        #k_back=back_components_tag.shape[0]/2.3,   # Hack\n",
    "        kc_sparsity=projtag_kwargs[\"kc_sparsity\"], \n",
    "        f_mix=f, ab_rates=inhib_rates)\n",
    "    )\n",
    "print(predicted_perfect_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labelmap = {\"none\":\"None\", \"average\":\"Average\", \"ibcm\":\"IBCM\", \"ideal\":r\"Ideal $\\perp$\", \"ideal2\":\"Ideal all\"}\n",
    "for k in [\"ideal\", \"ideal2\"]:\n",
    "    ax.plot(f_range, median_jaccards[k], color=clr_map[k], label=labelmap[k], lw=3)\n",
    "ax.plot(f_range, predicted_perfect_jaccard, color=\"k\", label=\"Predicted all\", lw=3)\n",
    "ax.set(xlabel=\"Fraction $f$ of new odor\", ylabel=\"Median Jaccard similarity\")\n",
    "ax.legend(title=\"Inhibition method\")\n",
    "fig.set_size_inches(4, 3)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/ideal_inhibition_jaccard_vs_predicted.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gammadist(x, k, lam):\n",
    "    return (k-1)*np.log(x) - lam*x + k*np.log(lam) - sp.special.gammaln(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check distribution of KC in response to background, compare to the postulated gamma\n",
    "# Dot product each back_sample_tag with the projmat, use all resulting KCs to determine distrib\n",
    "# The correct kappa, K, avg_nu, etc. do not give the right distribution\n",
    "# I can fit manually the gamma to get close to the actual distribution\n",
    "# Try that hack in the calculation of the predicted performance\n",
    "# UPDATE: does not seem to help...\n",
    "back_samples_kenyon = proj_mat.dot(back_samples_tag.T)\n",
    "kc_axis = np.linspace(back_samples_kenyon.min(), back_samples_kenyon.max(), 101)\n",
    "# Compute gamma distribution that we postulated: gamma(K*kappa, lambda/avg_nu)\n",
    "gamma_kc_axis = np.exp(log_gammadist(kc_axis, \n",
    "                            k=projtag_kwargs[\"n_pn_per_kc\"]*back_components_tag.shape[0], \n",
    "                            lam=1.0 / (np.mean(back_components_tag) * averages_nu.mean())))\n",
    "                            #k=projtag_kwargs[\"n_pn_per_kc\"]*back_components_tag.shape[0]/2.3, \n",
    "                            #lam=0.36 / (np.mean(back_components_tag) * averages_nu.mean())))\n",
    "plt.hist(back_samples_kenyon.flatten(), bins=20, density=True)\n",
    "plt.plot(kc_axis, gamma_kc_axis)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of KC in response to single new odor. Should really be a gamma distribution...\n",
    "# This seems to match pretty closely. \n",
    "target_samples_kenyon = proj_mat.dot(new_odor_targets.T)\n",
    "kc_axis = np.linspace(target_samples_kenyon.min(), target_samples_kenyon.max(), 101)\n",
    "# Compute gamma distribution that we postulated: gamma(K*kappa, lambda/avg_nu)\n",
    "gamma_kc_axis = np.exp(log_gammadist(kc_axis, \n",
    "                            k=projtag_kwargs[\"n_pn_per_kc\"], \n",
    "                            lam=1.0 / (np.mean(new_odor_targets))))\n",
    "plt.hist(target_samples_kenyon.flatten(), bins=20, density=True)\n",
    "plt.plot(kc_axis, gamma_kc_axis)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate where IBCM fails\n",
    "Is it just because of noise in  $\\vec{m}$ and $\\vec{w}$? Is it only some odors that fail terribly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_samples_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
