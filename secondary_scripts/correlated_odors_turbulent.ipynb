{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to turbulent odor backgrounds\n",
    "Look at a 6-odor case. Compare BioPCA and IBCM models for habituation and new odor detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlating turbulent background processes\n",
    "\n",
    "We first simulate independent concentration variables $\\nu(t)$ and consider their zero-meaned version, $\\tilde{\\nu}(t) = \\nu(t) - \\langle \\nu \\rangle$. These variables have mean zero, a diagonal covariance matrix $\\sigma^2 \\mathbb{1}$, and third moment $m_3$ (while mixed third-order moments of different $\\tilde{\\nu}_\\mu$ are zero due to their zero average). \n",
    "\n",
    "Then, we can transform these variables to have some desired covariance matrix $C$ with Cholesky decomposition $\\sigma^2 RR^T = \\Sigma$ (i.e. $R$ is the Cholesky decomposition of $\\Sigma / \\sigma^2$ and thus has scale $1/\\sigma$). Indeed, we take concentrations $c = R \\tilde{\\nu} + \\langle \\nu \\rangle$. These have the same mean as the original variables, $\\langle c \\rangle = \\langle \\nu \\rangle$, and the desired covariance matrix, \n",
    "\n",
    "$$ C = \\langle (c - \\langle c \\rangle) (c - \\langle c \\rangle)^T \\rangle = \\langle R \\tilde{\\nu} \\tilde{\\nu}^T R^T \\rangle = \\sigma^2 R R^T = \\Sigma $$\n",
    "\n",
    "The third moments are altered by this transformation, but they are not generally zero, so we can still numerically get the IBCM model to converge. \n",
    "\n",
    "\n",
    "Let's use as a target the covariance matrix\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & \\rho & \\ldots & \\rho \\\\\n",
    "    \\rho & 1 & \\ldots & \\rho \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    \\rho & \\rho & \\ldots & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "where $-1 < \\rho < 1$ is the Pearson correlation coefficient. Then $R$ is the Cholesky decomposition of the matrix on the right. \n",
    "\n",
    "### Asymmetry between odors?\n",
    "If we instead keep turbulent odors independent, but give them different fluctuation magnitudes, for instance forcing the following covariance matrix with the same Cholesky decomposition trick:\n",
    "$$\n",
    "    \\Sigma = \\begin{pmatrix}\n",
    "    \\frac12 \\sigma^2 (1 + u_0) & 0 & \\ldots & 0 \\\\\n",
    "    0 & \\frac12 \\sigma^2 (1 + u_1) & \\ldots & 0 \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    0 & 0 & \\ldots & \\frac12 \\sigma^2 (1 + u_n)\n",
    "\\end{pmatrix} $$\n",
    "with $u_i \\sim U(0, 1)$ uniform random samples, then the IBCM network still gets neurons specific to each odor. So asymmetry in magnitude between background odors is not a major challenge for this model. It is really the difference in temporal statistics between odors introduced by the Cholesky approach to correlate them that is the issue: since odor $n$ is a mixture of all $n$ previous underlying processes, it gets a lot intra-whiff fluctuations from all other odors' fluctuations. So each odor's temporal fluctuations have different scales, and fixed points aligned with some odors thus become more stable than others, with the network potentially missing some background components for that reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "import os, json, sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "    \n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning, \n",
    "    check_conc_samples_powerlaw_exp1\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_powerlaw_mixed_concs,\n",
    "    logof10, \n",
    "    sample_ss_conc_powerlaw,\n",
    "    sample_ss_mixed_concs_powerlaw,\n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import (\n",
    "    truncexp1_average,\n",
    "    powerlaw_cutoff_inverse_transform\n",
    ")\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_outputs = False\n",
    "\n",
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"optimal\": \"Optimal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"optimal\": \"xkcd:powder blue\",\n",
    "    \"ideal\": \"xkcd:light green\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 50  # Half the real number for faster simulations\n",
    "n_components = 4  # Number of background odors\n",
    "\n",
    "inhib_rates = [5e-5, 1e-5]  # alpha, beta  [0.00025, 0.00005]\n",
    "\n",
    "# Simulation duration\n",
    "duration = 360000.0\n",
    "deltat = 1.0\n",
    "n_chunks = 1\n",
    "skp = 10 * int(1.0 / deltat)\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  # \"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_powerlaw_mixed_concs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "# This seed gave nicely spread out odors easier to learn 0xe329714605b83365e67b44ed7e001ec\n",
    "rgen_meta = np.random.default_rng(seed=0xe629712605b83365e67b24ed7e001ec)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Turbulent background parameters: same rates and constants for all odors\n",
    "back_params = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([500.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([1.0] * n_components),        # blank_tmins\n",
    "    np.asarray([800.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.6] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "\n",
    "# Choose desired Pearson correlation between odors\n",
    "# Up to 0.5 there is some convergence, beyond, issue. \n",
    "# Note that for rho = 1, there would effectively be only one odor. \n",
    "correl_rho = -0.6\n",
    "all_correl = False\n",
    "\n",
    "# Compute mean of independent underlying variables, \n",
    "# to determine the mean and target covariance of mixed variables\n",
    "#indep_conc_samples = sample_ss_conc_powerlaw(*back_params, size=int(1e5))\n",
    "#mean_conc_empirical = np.mean(indep_conc_samples)\n",
    "#vari_conc = np.var(indep_conc_samples)\n",
    "tblo, tbhi, twlo, twhi = back_params[2], back_params[3], back_params[0], back_params[1]\n",
    "whiffprob = np.mean(1.0 / (1.0 + np.sqrt(tblo*tbhi/twlo/twhi)))\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*back_params[4:6]))\n",
    "mean_conc = whiffprob * avg_whiff_conc  # average time in whiffs vs blanks * average whiff conc\n",
    "print(\"Analytical mean conc:\", mean_conc)\n",
    "#print(\"Numerical mean conc:\", mean_conc_empirical)\n",
    "\n",
    "# Target covariance matrix (scaled by variance of underlying independent variables)\n",
    "if all_correl:\n",
    "    target_covmat_scaled = np.full((n_components, n_components), correl_rho)\n",
    "    target_covmat_scaled[np.diag_indices(n_components)] = 1.0\n",
    "    target_cholesky = np.linalg.cholesky(target_covmat_scaled)\n",
    "    # Independent but unequal amplitude for odors: IBCM still finds all components\n",
    "    # so asymmetry in scale (within reasonable range) is not an issue\n",
    "    # Temporal statistics differences between odors coming from Cholesky mixing\n",
    "    # of turbulent processes are a tougher challenge for IBCM. \n",
    "    #target_covmat_scaled = np.zeros((n_components, n_components))\n",
    "    #target_covmat_scaled[np.diag_indices(n_components)] = 0.5 * (1.0 + rgen_meta.random(size=n_components))\n",
    "    #target_cholesky = np.linalg.cholesky(target_covmat_scaled)\n",
    "# Other option where we correlate only a pair of odors\n",
    "else:\n",
    "    target_covmat_scaled = np.zeros((n_components, n_components))\n",
    "    target_covmat_scaled[[-1, -2], [-2, -1]] = correl_rho\n",
    "    target_covmat_scaled[np.diag_indices(n_components)] = 1.0\n",
    "    if abs(correl_rho) < 1.0:\n",
    "        target_cholesky = np.linalg.cholesky(target_covmat_scaled)\n",
    "    else:\n",
    "        target_cholesky = np.zeros((n_components, n_components))\n",
    "        target_cholesky[np.diag_indices(n_components)] = 1.0\n",
    "        target_cholesky[-1, -1] = 0.0\n",
    "        target_cholesky[-1, -2] = correl_rho  # Replace odor 1 by odor 0 for rho = +-1\n",
    "print(target_cholesky)\n",
    "print(target_covmat_scaled)\n",
    "# Add mean conc and Cholesky mixing matrix to parameters\n",
    "back_params.append(mean_conc)\n",
    "back_params.append(target_cholesky)\n",
    "# Then add background odor vectors last to that list\n",
    "back_params.append(back_components)\n",
    "\n",
    "# Initial values of background process variables (underlying independent (t, c))\n",
    "init_concs_ind = sample_ss_conc_powerlaw(*back_params[:-3], size=1, rgen=rgen_meta)\n",
    "init_times = powerlaw_cutoff_inverse_transform(\n",
    "                rgen_meta.random(size=n_components), *back_params[2:4])\n",
    "tc_init = np.stack([init_times, init_concs_ind.squeeze()], axis=1)\n",
    "\n",
    "# Initial background vector \n",
    "init_concs_mix = target_cholesky.dot(tc_init[:, 1] - mean_conc) + mean_conc\n",
    "init_bkvec = init_concs_mix.dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [tc_init, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise similarity between background odors\n",
    "Determines how well-posed the PCA is and how easy it is for the IBCM model to disentangle odors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00125 #5e-5\n",
    "tau_avg_ibcm = 1200  # 2000\n",
    "coupling_eta_ibcm = 0.6/n_i_ibcm\n",
    "ssat_ibcm = 50.0\n",
    "k_c2bar_avg = 0.1\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm \n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"law\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.5*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])*lambd_ibcm\n",
    "#init_synapses_ibcm = (0.3 * back_components[rgen_meta.choice(n_components, size=n_i_ibcm), :]\n",
    "#                      + 0.1*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions]))* lambd_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_theta_series(cbser, tau, dt):\n",
    "    theta = np.zeros([cbser.shape[0], cbser.shape[1]])\n",
    "    theta[0] = cbser[0]**2\n",
    "    for i in range(cbser.shape[0]-1):\n",
    "        theta[i+1] = theta[i] + dt/tau*(cbser[i]*cbser[i] - theta[i])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "# Perform successive shorter runs/restarts for memory efficiency\n",
    "tser_ibcm = []\n",
    "nuser_ibcm = []\n",
    "bkvecser_ibcm = []\n",
    "mser_ibcm = []\n",
    "cbarser_ibcm = []\n",
    "wser_ibcm = []\n",
    "yser_ibcm = []\n",
    "thetaser_ibcm = []\n",
    "if n_chunks > 1:\n",
    "    seed_spawns = np.random.SeedSequence(simul_seed).spawn(10)\n",
    "else:\n",
    "    seed_spawns = [simul_seed]\n",
    "for i in range(n_chunks):\n",
    "    tstart = perf_counter()\n",
    "    if i == 0:\n",
    "        init_vari = init_synapses_ibcm\n",
    "        init_back = init_back_list\n",
    "    else:\n",
    "        init_vari = [mser_ibcm[i-1][-1], thetaser_ibcm[i-1][-1], wser_ibcm[i-1][-1]]\n",
    "        init_back = [nuser_ibcm[i-1][-1], bkvecser_ibcm[i-1][-1]]\n",
    "    sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_vari, update_fct, init_back, \n",
    "                ibcm_rates, inhib_rates, back_params, duration/n_chunks, \n",
    "                deltat, seed=seed_spawns[i], noisetype=\"uniform\",  \n",
    "                skp=skp, **ibcm_options\n",
    "    )\n",
    "    tser_ibcm.append(sim_results[0] + i/n_chunks*duration)\n",
    "    nuser_ibcm.append(sim_results[1])\n",
    "    bkvecser_ibcm.append(sim_results[2])\n",
    "    mser_ibcm.append(sim_results[3]) \n",
    "    cbarser_ibcm.append(sim_results[4]) \n",
    "    thetaser_ibcm.append(sim_results[5])\n",
    "    wser_ibcm.append(sim_results[6])\n",
    "    yser_ibcm.append(sim_results[7])\n",
    "    tend = perf_counter()\n",
    "    print(\"Finished chunk\", i, \"in {:.2f} s\".format(tend - tstart))\n",
    "\n",
    "# Concatenate\n",
    "tser_ibcm = np.concatenate(tser_ibcm, axis=0)\n",
    "nuser_ibcm = np.concatenate(nuser_ibcm)\n",
    "bkvecser_ibcm = np.concatenate(bkvecser_ibcm)\n",
    "mser_ibcm = np.concatenate(mser_ibcm)\n",
    "cbarser_ibcm = np.concatenate(cbarser_ibcm)\n",
    "thetaser_ibcm = np.concatenate(thetaser_ibcm)\n",
    "wser_ibcm = np.concatenate(wser_ibcm)\n",
    "yser_ibcm = np.concatenate(yser_ibcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check background process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check that the background process matches the intended distribution\n",
    "fig, axes = check_conc_samples_powerlaw_exp1(nuser_ibcm[:, :, 1].T, *back_params[:-3])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed concentrations time series\n",
    "conc_ser = np.einsum(\"ij,kj->ki\", target_cholesky, nuser_ibcm[:, :, 1] - mean_conc) + mean_conc\n",
    "print(conc_ser.shape)\n",
    "fig, ax = plt.subplots()\n",
    "tslice = slice(0, 500)\n",
    "for i in range(n_components):\n",
    "    ax.plot(tser_ibcm[tslice]/1000, conc_ser[tslice, i], lw=0.8, label=\"Odor {}\".format(i))\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Mixed odor concentrations\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed concentrations time series\n",
    "tslice = slice(0, 50000, 100)\n",
    "n_cols = 6\n",
    "n_plots = n_dimensions // 2\n",
    "n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_cols*1.75, n_rows*1.75)\n",
    "for i in range(n_plots):\n",
    "    ax = axes.flat[i]\n",
    "    ax.scatter(bkvecser_ibcm[tslice, 2*i+1], bkvecser_ibcm[tslice, 2*i], \n",
    "               s=9, alpha=0.5, color=\"k\")\n",
    "    for j in range(n_components):\n",
    "        ax.plot(*zip([0.0, 0.0], 3.0*back_components[j, 2*i:2*i+2:][::-1]), lw=2.0)\n",
    "    ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "for i in range(n_plots, n_rows*n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "\n",
    "If the background has very strong correlation between odors, it may not be effectively $N_\\mathrm{B}$-dimensional anymore, so the representation in terms of the dot products with original odors, $h_{\\gamma}$, may not be appropriate. There could be a new basis on which the neurons pick up; some decomposition of the mixture $\\sum_\\gamma c_\\gamma \\mathbf{y}_\\gamma$. Try the SVD, compute the dot products with the first two singular vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize, SymLogNorm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Matrix of third moments, to see how asymmetric components become\n",
    "# thus explaining the more difficult convergence to specificity\n",
    "mean_concs_num = np.mean(nuser_ibcm[:, :, 1], axis=0)  # unmixed\n",
    "conc_0mean = (nuser_ibcm[:, :, 1] - mean_concs_num).dot(target_cholesky.T)\n",
    "thirdmoments = np.mean(conc_0mean[:, :, None, None] \n",
    "                       * conc_0mean[:, None, :, None] \n",
    "                       * conc_0mean[:, None, None, :], axis=0)\n",
    "\n",
    "# Check that the covmat is approx. what we wanted\n",
    "covnum = np.mean(conc_0mean[:, :, None] \n",
    "                       * conc_0mean[:, None, :], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "absrange = np.abs(covnum).max()\n",
    "linthresh = np.abs(covnum).min()\n",
    "#cmap_norm = SymLogNorm(linthresh=linthresh, vmin=-absrange, vmax=absrange)\n",
    "#cmap_norm = Normalize(vmin=-absrange, vmax=absrange)\n",
    "cmap_choice = \"viridis\"\n",
    "cmap_norm = Normalize(vmin=min(0, np.amin(covnum)), vmax=np.amax(covnum))\n",
    "im = ax.imshow(covnum, norm=cmap_norm, cmap=cmap_choice)\n",
    "ax.set(xlabel=\"j\", ylabel=\"k\", title=r\"$C_{ij}$\")\n",
    "cb = fig.colorbar(im, location=\"right\", orientation=\"vertical\", \n",
    "             ax=ax, label=\"Covariance matrix\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig, axes = plt.subplots(1, n_components, constrained_layout=True)\n",
    "absrange = np.abs(thirdmoments).max()\n",
    "linthresh = np.abs(thirdmoments).min()\n",
    "#cmap_norm = SymLogNorm(linthresh=linthresh, vmin=-absrange, vmax=absrange)\n",
    "#cmap_norm = Normalize(vmin=-absrange, vmax=absrange)\n",
    "#cmap_choice = \"RdBu\"\n",
    "cmap_choice = \"viridis\"\n",
    "cmap_norm = Normalize(vmin=min(0, np.amin(thirdmoments)), vmax=np.amax(thirdmoments))\n",
    "for i in range(n_components):\n",
    "    ax = axes.flat[i]\n",
    "    im = ax.imshow(thirdmoments[i], norm=cmap_norm, cmap=cmap_choice)\n",
    "    ax.set_title(\"$\" + f\"C_{i}jk\" + \"$\")\n",
    "    ax.set(xlabel=\"j\", ylabel=\"k\")\n",
    "cb = fig.colorbar(im, location=\"bottom\", orientation=\"horizontal\", \n",
    "             ax=axes.flatten(), label=\"Third-order correlation\")\n",
    "#cb.set_ticks([-1e-2, -1e-3, -1e-4, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find the independent components of the process, see if IBCM aligns with them\n",
    "fast_ica = FastICA(n_components=n_components, algorithm=\"parallel\", whiten=\"unit-variance\", \n",
    "                  random_state=seed_from_gen(rgen_meta, nbits=32)).fit(bkvecser_ibcm)\n",
    "# Unmixing matrix: apply to bkvecser_ibcm.T to recover sources\n",
    "unmix_vectors = fast_ica.components_\n",
    "unmix_concser = unmix_vectors.dot(bkvecser_ibcm.T)  # [source, ORN]x[time, ORN].T\n",
    "unmix_results = unmix_concser.copy()\n",
    "unmix_vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Recovered components: normalize and compensate unmixing results\n",
    "recovered_components = fast_ica.mixing_.copy().T  # [source, ORN]\n",
    "for i in range(recovered_components.shape[0]):\n",
    "    norm_of_comp = np.sqrt(np.sum(recovered_components[i]**2))\n",
    "    recovered_components[i] /= norm_of_comp\n",
    "    unmix_results[i] *= norm_of_comp\n",
    "    if np.all(recovered_components[i] < 0):\n",
    "        recovered_components[i] *= -1\n",
    "        unmix_results[i] *= -1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_permutation(shuffled, reference):\n",
    "    \"\"\" Find permutation indices that reorder shuffled into reference. \n",
    "    I.e. find the indices where each row of reference is in shuffled, \n",
    "    so calling shuffled[indices] puts it back in order. \n",
    "    Do it by minimizing the squared difference between rows. \n",
    "    Greedy algorithm, may not be optimal if reconstruction\n",
    "    is not very accurate. \n",
    "    \"\"\"\n",
    "    indices = np.arange(shuffled.shape[0]).astype(int)\n",
    "    sum_axes = tuple(range(1, shuffled.ndim))\n",
    "    for i in range(len(indices)):\n",
    "        squared_dists = np.sum((shuffled - reference[i:i+1])**2, axis=sum_axes)\n",
    "        indices[i] = np.argmin(squared_dists)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check ICA recovered vectors vs true vectors\n",
    "permut_indices = find_permutation(recovered_components, back_components)\n",
    "recovered_components_ordered = recovered_components[permut_indices]\n",
    "unmix_results_ordered = unmix_results[permut_indices]\n",
    "n_cols = 6\n",
    "n_plots = n_dimensions // 2\n",
    "n_rows = n_plots // n_cols + min(1, n_plots % n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True)\n",
    "fig.set_size_inches(n_cols*1.75, n_rows*1.75)\n",
    "for i in range(n_plots):\n",
    "    ax = axes.flat[i]\n",
    "    for j in range(n_components):\n",
    "        li, = ax.plot(*zip([0.0, 0.0], 3.0*back_components[j, 2*i:2*i+2:][::-1]), lw=2.0, alpha=0.5)\n",
    "        ax.plot(*zip([0.0, 0.0], 3.0*recovered_components_ordered[j, 2*i:2*i+2:][::-1]), \n",
    "                lw=2.0, ls=\"--\", color=li.get_color())\n",
    "    ax.set(xlabel=\"OSN {}\".format(2*i+2), ylabel=\"OSN {}\".format(2*i+1))\n",
    "for i in range(n_plots, n_rows*n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "tslice = slice(0, 500)\n",
    "for i in range(unmix_results.shape[0]):\n",
    "    ax.plot(tser_ibcm[tslice], unmix_results_ordered[i, tslice], lw=0.8, label=\"Component {}\".format(i))\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Inferred independent components\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bkveccovmat = np.mean(bkvecser_ibcm[:, None, :] * bkvecser_ibcm[:, :, None], axis=0)\n",
    "eigvalsbk, vvecsbk = np.linalg.eig(bkveccovmat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "uvecs, singvals, vvecs = np.linalg.svd(mser_ibcm[-1], full_matrices=False)\n",
    "ax.plot(np.arange(min(n_i_ibcm, n_dimensions)), singvals)\n",
    "ax.set(xlabel=\"Rank\", ylabel=\"Singular value of $M$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "n_cols = 5\n",
    "n_rows = n_dimensions // n_cols + min(1, n_dimensions% n_cols)\n",
    "fig , axes = plt.subplots(n_rows, n_cols, sharey=True, sharex=True)\n",
    "fig.set_size_inches(n_rows*1.85, n_cols*1.85)\n",
    "neuron_colors = sns.color_palette(\"colorblind\", n_colors=n_i_ibcm)\n",
    "for i in range(n_dimensions):  # dimension\n",
    "    ax = axes.flat[i]\n",
    "    ax.set_title(\"From OSN {}\".format(i), y=0.96, fontsize=10)\n",
    "    for j in range(n_i_ibcm):  # neuron\n",
    "        ax.plot(tser_ibcm/1000.0, mser_ibcm[:, j, i], \n",
    "                label=\"LN {}\".format(j), lw=0.75, color=neuron_colors[j])\n",
    "for i in range(n_dimensions, n_rows * n_cols):\n",
    "    axes.flat[i].set_axis_off()\n",
    "for i in range(n_dimensions-n_cols, n_dimensions):\n",
    "    axes.flat[i].set_xlabel(\"Time (x1000 steps)\")\n",
    "for i in range(n_rows):\n",
    "    axes[i, 0].set_ylabel(\"Synaptic weight\")\n",
    "\n",
    "\n",
    "#fig.savefig(\"figures/powerlaw/m_series_correlated_turbulent_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = int(5/6*duration / deltat) // skp\n",
    "basis = back_components #recovered_components_ordered  # back_components\n",
    "if correl_rho == 1.0:  # effectively N-1 vectors only\n",
    "    basis = np.concatenate([back_components[:2], \n",
    "        np.sum(back_components[2:4], axis=0, keepdims=True)], axis=0)\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                        mser_ibcm, coupling_eta_ibcm, basis)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. Easiest to compute numerically. \n",
    "# Odors are all iid so we can average over all odors\n",
    "mean_conc = np.mean(conc_ser)\n",
    "sigma2_conc = np.var(conc_ser)\n",
    "thirdmom_conc = np.mean((conc_ser - mean_conc)**3)\n",
    "moments_conc = [float(mean_conc), float(sigma2_conc), float(thirdmom_conc)]\n",
    "print(moments_conc)\n",
    "\n",
    "# Analytical prediction\n",
    "res = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1)\n",
    "c_specif, c_nonspecif = res[:2]\n",
    "cs_cn = res[:2]\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 2.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, \n",
    "                                            basis, cs_cn, specif_gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.plot(tser_ibcm[:300], nuser_ibcm[:300, :, 1])\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm/1000, thetaser_ibcm[:, i], lw=0.5, color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000 steps)\", ylabel=r\"$\\bar{\\Theta} = \\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=320000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "#ax.set_xlim([200, 360])\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "fig.tight_layout()\n",
    "leg = ax.legend(loc=\"upper left\", bbox_to_anchor=(1., 1.))\n",
    "\n",
    "#fig.savefig(\"figures/powerlaw/cbargammas_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = (nuser_ibcm[:, :, 1] \n",
    "                     - np.mean(nuser_ibcm[transient:, :, 1], axis=0))\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/specificities_turbulent_background_example.pdf\", \n",
    "#           transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/pn_activity_norm_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/powerlaw/w_series_turbulent_background_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 1e-4  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 8.0\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "# We selected a seed (out of 40+ tested) giving initial conditions leading to correct PCA\n",
    "# The model has trouble converging on this background, we're giving as many chances as possible here. \n",
    "rgen_pca = np.random.default_rng(seed=0x8b6664612cfeda4a121436fcfbbca449)\n",
    "init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compute_pca_gap(true, learnt, n_comp, transient_p):\n",
    "    s = np.sum(np.log10(true[0][:n_comp]) - np.mean(np.log10(learnt[0][transient_p:, :n_comp]), axis=0))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Look for a good seed automatically... really trying to help the PCA model here\n",
    "logpc_gaps_sum = 1.0\n",
    "log_align_error = 1.0\n",
    "transient_pca = 300000 // skp\n",
    "n_trials = 0\n",
    "good_seed = 0x8b6664612cfeda4a121436fcfbbca43e\n",
    "\n",
    "# Record all PCA results, in case one comes close\n",
    "all_pca_gap_results = {}\n",
    "all_pca_align_errors = {}\n",
    "while logpc_gaps_sum > 0.25 and n_trials < 20 and log_align_error > 0.025:\n",
    "    # Try a new seed...\n",
    "    good_seed += 1\n",
    "    rgen_pca = np.random.default_rng(seed=good_seed)\n",
    "    init_synapses_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "    init_mmat_pca = rgen_pca.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "    init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "    ml_inits_pca = [init_mmat_pca, init_lmat_pca]\n",
    "    \n",
    "    # Run simulation\n",
    "    sim_results = integrate_inhib_biopca_network_skip(\n",
    "                    ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                    inhib_rates, back_params, duration, deltat, \n",
    "                    seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "    (tser_pca, \n",
    "     nuser_pca, \n",
    "     bkvecser_pca, \n",
    "     mser_pca, \n",
    "     lser_pca, \n",
    "     xser_pca, \n",
    "     cbarser_pca, \n",
    "     wser_pca, \n",
    "     yser_pca) = sim_results\n",
    "    \n",
    "    # Analyze simulation PCA\n",
    "    res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "    true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res\n",
    "    log_align_error = np.mean(np.log10(align_error_ser[transient_pca:]))\n",
    "    \n",
    "    # Update convergence criteria\n",
    "    n_trials += 1\n",
    "    logpc_gaps_sum = compute_pca_gap(true_pca, learnt_pca, n_components, transient_pca)\n",
    "    all_pca_gap_results[good_seed] = logpc_gaps_sum\n",
    "    all_pca_align_errors[good_seed] = log_align_error\n",
    "    print(\"Finished trial {}\".format(n_trials-1))\n",
    "\n",
    "best_gap = 1e12\n",
    "best_align_error = 1e12\n",
    "best_pca_seed = good_seed\n",
    "for k in all_pca_gap_results:\n",
    "    if all_pca_gap_results[k] < best_gap:\n",
    "        best_gap = all_pca_gap_results[k]\n",
    "        best_pca_seed = k\n",
    "    if all_pca_align_errors[k] < best_align_error:\n",
    "        best_align_error = all_pca_align_errors[k]\n",
    "print(\"Finished after {} trials\".format(n_trials))\n",
    "print(\"Last gap was {}\".format(logpc_gaps_sum))\n",
    "print(\"Best gap was {} for seed {}\".format(best_gap, best_pca_seed))\n",
    "print(\"With alignment error {}\".format(all_pca_align_errors[best_pca_seed]))\n",
    "print(\"Best align error overall was {}\".format(best_align_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " yser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, yser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "yser_ideal = bkvecser_ibcm * ideal_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yser_orthogonal = np.zeros(bkvecser_ibcm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ideal import compute_optimal_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*back_params[4:6]))\n",
    "print(\"Average whiff concentration: {:.4f}\".format(avg_whiff_conc))\n",
    "new_test_concs *= avg_whiff_conc\n",
    "n_new_concs = len(new_test_concs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal W matrix for all new odors possible\n",
    "dummy_rgen = np.random.default_rng(0x115959f92ef489f1fe6b034c2ebfcb37)\n",
    "n_samp = int(1e4)\n",
    "new_odors_from_distrib = generate_odorant([n_samp, n_dimensions], \n",
    "                                    dummy_rgen, lambda_in=0.1)\n",
    "new_odors_from_distrib /= l2_norm(new_odors_from_distrib)[:, None]\n",
    "optimal_ws = compute_optimal_matrices(back_components, \n",
    "                new_odors_from_distrib, moments_conc, new_test_concs)\n",
    "\n",
    "yser_optimal = bkvecser_ibcm - bkvecser_ibcm.dot(optimal_ws[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment between IBCM, PCA projectors and the subspace projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import subspace_align_error\n",
    "from utils.random_matrices import random_orthogonal_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcm_matrix = np.mean(np.einsum(\"...ik,...kj\", wser_ibcm[transient:], mbarser[transient:]), axis=0)\n",
    "biopca_matrix = np.mean(np.einsum(\"...ik,...kj\", wser_pca[transient:], fser[transient:]), axis=0)\n",
    "align_err_ibcm = subspace_align_error(ibcm_matrix, back_projector)\n",
    "align_err_pca = subspace_align_error(biopca_matrix, back_projector)\n",
    "align_err_optimal = subspace_align_error(optimal_ws[0], back_projector)\n",
    "align_err_random_matrix = subspace_align_error(random_orthogonal_mat(back_projector.shape[0], rng=dummy_rgen), back_projector)\n",
    "print(\"Alignment error of IBCM projector vs orthogonal projector:\", align_err_ibcm)\n",
    "print(\"Alignment error of BioPCA projector vs orthogonal projector:\", align_err_pca)\n",
    "print(\"For comparison, alignment error of optimal manifold learning matrix:\", align_err_optimal)\n",
    "print(\"And that of a random orthogonal matrix:\", align_err_random_matrix)\n",
    "print(\"This tells us that both the IBCM mand BioPCA projectors are well aligned with the background subspace\")\n",
    "print(\"The lesser alignment for BioPCA could come from the approximate L inverse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ynorm_series = {\n",
    "    \"ibcm\": l2_norm(yser_ibcm), \n",
    "    \"biopca\": l2_norm(yser_pca), \n",
    "    \"avgsub\": l2_norm(yser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(yser_ideal), \n",
    "    \"optimal\": l2_norm(yser_optimal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(ynorm_series[a], **std_options)) for a in ynorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(ynorm_series[a], **mean_options) for a in ynorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "ynorm_string = r\"$\\|\\mathbf{y}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + ynorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + ynorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background tagging after habituation\n",
    "We create a projection matrix, then compute the tag assigned to the background after inhibition by each habituation model, over time. Hopefully, only IBCM inhibits enough to see tags go to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)\n",
    "\n",
    "yser_dict = {\n",
    "    \"ibcm\": yser_ibcm, \n",
    "    \"biopca\": yser_pca, \n",
    "    \"avgsub\": yser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": yser_ideal, \n",
    "    \"optimal\": yser_optimal,\n",
    "    \"orthogonal\": yser_orthogonal\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Computing background tag lengths at various time points for each model\n",
    "tag_length_series = {a: np.zeros(tser_ibcm.shape[0]) for a in yser_dict.keys()}\n",
    "for a in yser_dict.keys():\n",
    "    for i in range(0, tag_length_series[a].shape[0]):\n",
    "        if bkvecser_ibcm[i].max() > 0:\n",
    "            tag = project_neural_tag(yser_dict[a][i], bkvecser_ibcm[i], \n",
    "                                 proj_mat, **projection_arguments)\n",
    "        else:\n",
    "            tag = (1,)*int(projection_arguments[\"kc_sparsity\"]*n_kc)\n",
    "        tag_length_series[a][i] = len(tag)\n",
    "tag_length_series_smooth = {a: moving_average(tag_length_series[a], **mean_options)\n",
    "                            for a in tag_length_series}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "for model in yser_dict.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    ax.plot(tser_ibcm / 1000, tag_length_series_smooth[model], **props)\n",
    "ynorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "ax.set_ylabel(r\"Tag length, $\\mathrm{card}(z)$\")\n",
    "ax.set_xlabel(\"Time (x1000 steps)\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 100\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "avg_whiff_conc = np.mean(truncexp1_average(*back_params[4:6]))\n",
    "print(\"Average whiff concentration: {:.4f}\".format(avg_whiff_conc))\n",
    "new_test_concs *= avg_whiff_conc\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "conc_samples = sample_ss_mixed_concs_powerlaw(\n",
    "                    *back_params[:8], size=n_test_times*(n_back_samples-1), rgen=rgen_meta\n",
    "                )  # Shaped [sample, component]\n",
    "back_samples = conc_samples.dot(back_components)\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_yvecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in yser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in yser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in yser_dict.keys()}\n",
    "jaccard_scores_with_back = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples, n_components]) \n",
    "                  for a in yser_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ideal reduction factor for each concentration\n",
    "dummy_rgen = np.random.default_rng(0x6e3e2886c30163741daaaf7c8b8a00e6)\n",
    "ideal_factors = [compute_ideal_factor(c, moments_conc[:2], [n_components, n_dimensions], \n",
    "                    generate_odorant, (dummy_rgen,)) for c in new_test_concs]\n",
    "\n",
    "# Compute background odor tags\n",
    "back_tags = []\n",
    "for i in range(n_components):\n",
    "    back_tags.append(project_neural_tag(back_components[i], \n",
    "            back_components[i], proj_mat, **projection_arguments\n",
    "    ))\n",
    "\n",
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_yvecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_yvecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_yvecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_yvecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, back_samples[j], new_test_concs[k], \n",
    "                ideal_factors[k], **avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"orthogonal\"][i, j, k] = x_new_ort\n",
    "            mixture_yvecs[\"optimal\"][i, j, k] = mixtures - mixtures.dot(optimal_ws[k].T)\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_yvecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_yvecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_yvecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_yvecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)\n",
    "                    # For comparison, compute similarity to each background odor\n",
    "                    for m in range(n_components):\n",
    "                        jaccard_scores_with_back[mod][i, j, k, l, m] = jaccard(mix_tag, back_tags[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"optimal\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"optimal\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_yvecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/detection/compare_models_onerun_ynorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_scores_with_back[\"none\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Jaccard similarity to new odor alone vs similarity to most similar background odor\n",
    "jaccard_scores_with_back_max = {a:np.amax(jaccard_scores_with_back[a], axis=4) for a in jaccard_scores_with_back}\n",
    "\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"optimal\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\", \"orthogonal\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = np.median(jaccard_scores[m], axis=(1,))  # Median time point, all back samples there\n",
    "    all_jacs_back = np.median(jaccard_scores_with_back_max[m], axis=(1,))\n",
    "    for i in range(n_new_concs):\n",
    "        axes[i].scatter(all_jacs_back[:, i].flatten(), all_jacs[:, i].flatten(), \n",
    "            s=9.0, color=model_colors.get(m), alpha=0.5, label=model_nice_names[m])\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", color=\"grey\", lw=1.0, zorder=0)\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard with most similar background odor\")\n",
    "    axes[i].set_ylabel(\"Jaccard with new odor\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "#fig.savefig(os.path.join(\"figures\", \"detection\", \"compare_models_onerun_jaccards_with_back.pdf\"), \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some results for further plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background process\n",
    "We need a supplementary simulation, fully sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a fully-sampled background simulation for figure 1\n",
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"uniform\", skp=1, **avg_options\n",
    ")\n",
    "tser_example, nuser_example = sim_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.checktools import check_turbulent_background_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Background process: save all elements of histograms of concentration during whiffs, t_blank, t_whiff\n",
    "# Use all six i.i.d. odors\n",
    "def check_turbulent_background_stats(tcser, back_rates):\n",
    "    # Extract parameters for analytical predictions\n",
    "    for i in range(len(back_rates)):\n",
    "        if not np.all(back_rates[i] == back_rates[i][0]):\n",
    "            raise ValueError(\"Odors not i.i.d., this functions assumes so\")\n",
    "    twlo, twhi, tblo, tbhi, c0, alpha = [a[0] for a in back_rates]\n",
    "    chi = 1.0 / (1.0 + np.sqrt(tblo*tbhi/twlo/twhi))\n",
    "    \n",
    "    stats = {}\n",
    "        \n",
    "    # First, check concentrations\n",
    "    conc_ser = tcser[:, :, 1]\n",
    "    nonzero_samples = conc_ser[conc_ser > 0.0].flatten()\n",
    "    conc_counts, conc_bins = np.histogram(nonzero_samples, bins=\"doane\")\n",
    "    stats[\"conc_bins\"] = conc_bins\n",
    "    conc_binwidths = np.diff(conc_bins)  # This can be recomputed easily\n",
    "    # Center of bins on a log scale, but given in linear coordinates\n",
    "    conc_bin_centers = (conc_bins[1:] + conc_bins[:-1])/2  # easily recomputed\n",
    "    stats[\"conc_pdf\"] = conc_counts / conc_binwidths / conc_ser.size\n",
    "    stats[\"conc_prob_zero\"] = 1.0 - nonzero_samples.size / conc_ser.size\n",
    "    \n",
    "    # Analytical distribution  of concentrations\n",
    "    conc_axis = np.linspace(stats[\"conc_bins\"][0], stats[\"conc_bins\"][-1], 201)  # easily recomputed\n",
    "    stats[\"conc_analytical_pdf\"] = chi*truncexp1_density(conc_axis, c0, alpha)\n",
    "    stats[\"conc_analytical_prob_zero\"] = 1.0 - chi\n",
    "    \n",
    "    # Now, analyze whiff and blank durations\n",
    "    # Whiffs start when concentration goes up; blanks, when concentration goes down\n",
    "    conc_diffs = np.diff(conc_ser, prepend=conc_ser[0:1], axis=0)  # Prepend so the first element is 0\n",
    "    whiff_starts = np.nonzero(conc_diffs > 0.0)\n",
    "    blank_starts = np.nonzero(conc_diffs < 0.0)\n",
    "    tstarts = {\"w\":whiff_starts, \"b\":blank_starts}\n",
    "    \n",
    "    for i in tstarts:  # loop over whiffs and blanks\n",
    "        if i == \"w\":\n",
    "            thresh_min, thresh_max = twlo, twhi\n",
    "        else:\n",
    "            thresh_min, thresh_max = tblo, tbhi\n",
    "        t_samples = tcser[tstarts[i][0], tstarts[i][1], 0].flatten()  # durations\n",
    "        # Choose log-spaced bins \n",
    "        t_counts, t_bins = np.histogram(np.log10(t_samples), bins=\"doane\")\n",
    "        # Center of bins on a log scale, but given in linear coordinates\n",
    "        bin_centers_forlog = 10**((t_bins[1:] + t_bins[:-1])/2)\n",
    "        # For plotting, bin limits in linear scale; set the log scale on the plot\n",
    "        t_bins = 10**t_bins\n",
    "        stats[\"t_{}_bins\".format(i)] = t_bins\n",
    "        t_binwidths = np.diff(t_bins)\n",
    "        stats[\"t_{}_pdf\".format(i)] = t_counts / t_binwidths / t_samples.size\n",
    "        # Analytical density with an upper cutoff. \n",
    "        # Should be a straight line in log scale, no need for a more finely sampled axis. \n",
    "        \n",
    "        stats[\"t_{}_axis\".format(i)] = bin_centers_forlog\n",
    "        norm_factor = 2 * thresh_min * (1.0 - np.sqrt(thresh_min / thresh_max))\n",
    "        stats[\"t_{}_analytical_pdf\".format(i)] = (bin_centers_forlog/thresh_min)**(-3/2) / norm_factor\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_stats = check_turbulent_background_stats(nuser_example, back_params[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the statistics to make sure it's convenient and correct. \n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.set_size_inches(9, 4)\n",
    "cmyk_blue = \"#3E529F\"\n",
    "cmyk_red = \"#DA3833\"\n",
    "cmyk_green = \"#307F54\"\n",
    "\n",
    "# Concentrations\n",
    "ax = axes[0]\n",
    "ax.bar(x=back_stats[\"conc_bins\"][:-1], align=\"edge\", height=back_stats[\"conc_pdf\"], \n",
    "       width=np.diff(back_stats[\"conc_bins\"]), color=cmyk_blue, label=\"Samples\")\n",
    "ax.plot(0.0, back_stats[\"conc_prob_zero\"], color=cmyk_blue, marker=\"o\", ls=\"none\", mec=\"k\", ms=8)\n",
    "ax.plot(np.linspace(back_stats[\"conc_bins\"][0], back_stats[\"conc_bins\"][-1], 201), \n",
    "        back_stats[\"conc_analytical_pdf\"], color=cmyk_red, lw=3.,\n",
    "        label=r\"$p_c(c) \\sim \\frac{e^{-c/c_0}}{Ac}$\")\n",
    "ax.plot(0.0, back_stats[\"conc_analytical_prob_zero\"], marker=\"*\", color=cmyk_red, ls=\"none\",\n",
    "        mec=cmyk_red, ms=4.5)\n",
    "ax.set(xlabel=r\"$c$\", ylabel=r\"$p_c(c)$\", yscale=\"log\")\n",
    "\n",
    "# Blanks\n",
    "ax = axes[1]\n",
    "ax.bar(x=back_stats[\"t_b_bins\"][:-1], align=\"edge\", height=back_stats[\"t_b_pdf\"], \n",
    "        width=np.diff(back_stats[\"t_b_bins\"]), color=cmyk_blue, label=\"Samples\")\n",
    "ax.plot(back_stats[\"t_b_axis\"], back_stats[\"t_b_analytical_pdf\"], color=cmyk_red, lw=3.,\n",
    "        label=r\"$p(t_d) \\sim t_d^{-3/2}$\")\n",
    "ax.set(xlabel=r\"$t_b$\", ylabel=r\"$p_t(t_b)$\", yscale=\"log\", xscale=\"log\")\n",
    "       \n",
    "\n",
    "# Whiffs\n",
    "ax = axes[2]\n",
    "ax.bar(x=back_stats[\"t_w_bins\"][:-1], align=\"edge\", height=back_stats[\"t_w_pdf\"], \n",
    "        width=np.diff(back_stats[\"t_w_bins\"]), color=cmyk_blue, label=\"Samples\")\n",
    "ax.plot(back_stats[\"t_w_axis\"], back_stats[\"t_w_analytical_pdf\"], color=cmyk_red, lw=3.,\n",
    "        label=r\"$p(t_w) \\sim t_w^{-3/2}$\")\n",
    "ax.set(xlabel=r\"$t_w$\", ylabel=r\"$p_t(t_w)$\", yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results/for_plots/\"\n",
    "if do_save_outputs:\n",
    "    np.savez_compressed(results_dir + \"sample_turbulent_background.npz\", \n",
    "        nuser=nuser_example, \n",
    "        tser=tser_example, \n",
    "        **back_stats\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "save_skp = 2  # Extra skip for saving?\n",
    "results_dir = \"results/for_plots/\"\n",
    "if do_save_outputs:\n",
    "    np.savez_compressed(\n",
    "        results_dir + \"sample_turbulent_ibcm_simulation.npz\", \n",
    "        back_vecs=back_components,\n",
    "        tser_range=np.asarray([0.0, duration, deltat*skp*save_skp]),   # Start, stop, step\n",
    "        cbars_gamma=cbars_gamma[::save_skp],  # That's what I actually plot\n",
    "        yser=yser_ibcm[::save_skp],\n",
    "        cs_cn=np.asarray([c_specif, c_nonspecif]),\n",
    "        specif_gammas=specif_gammas, \n",
    "        #analytical_w=analytical_w  # I don't need W a priori\n",
    "        #wser=wser_ibcm[::save_skp],  # I don't need W a priori\n",
    "        #mbarser_ibcm=mbarser[::save_skp],  # I don't need M directly\n",
    "        #bkvecser=bkvecser_ibcm[::save_skp],  # Easy to reconstruct with nu series\n",
    "        conc_ser=nuser_ibcm[::save_skp, :, 1],  # To reconstruct the background, to compare to s\n",
    "        ibcm_eig_values=ibcm_eig_values\n",
    "    )\n",
    "    # Series of s norm\n",
    "    np.savez_compressed(\n",
    "        os.path.join(results_dir, \"yser_norm_turbulent_model_comparison.npz\"), \n",
    "        **{a:ynorm_series[a][::save_skp] for a in ynorm_series} \n",
    "    )\n",
    "    \n",
    "    with open(os.path.join(results_dir, \"ibcm_eigenvalues_keys_turbulent_example.json\"), \"w\") as f:\n",
    "        json.dump(ibcm_specif_keys, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PCA simulation and its analysis too\n",
    "save_skp = 2  # Extra skip for saving?\n",
    "results_dir = \"results/for_plots/\"\n",
    "if do_save_outputs:\n",
    "    np.savez_compressed(\n",
    "        os.path.join(results_dir, \"sample_turbulent_biopca_simulation.npz\"), \n",
    "        true_pca_values=true_pca[0], \n",
    "        true_pca_vectors=true_pca[1],\n",
    "        learnt_pca_values=learnt_pca[0][::save_skp],\n",
    "        learnt_pca_vectors=learnt_pca[1][::save_skp],\n",
    "        align_error_ser=align_error_ser[::save_skp]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
