{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to weakly non-Gaussian odor backgrounds\n",
    "Look at a case with three odors. Compare BioPCA and IBCM models for habituation and new odor detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some notes on the setting\n",
    "### Saturation function on IBCM neurons\n",
    "Since the fluctuations are still fast compared to the IBCM neurons, we do not really need saturation functions to prevent numerical divergences (while still using a large time step). Also, the model converges fast enough that we don't need the Law and Cooper, 1994 modification. \n",
    "\n",
    "### Background process\n",
    "We simulate a zero-mean Ornstein-Uhlenbeck process $g_\\gamma$ with mean $\\langle c \\rangle$ and variance $\\sigma^2$ at steady-state, then we set the actual odor concentrations to be $c_\\gamma = 10^{g_\\gamma}$. This ensures that the odor concentrations are log-normal and have a third moment. For each $\\gamma$, the moments are\n",
    "\n",
    "$$ \\langle c \\rangle = 10^{\\langle g \\rangle + \\frac12 \\sigma^2 \\ln{10}} $$\n",
    "$$ \\mathrm{Var}[c] = \\left(10^{\\sigma^2 \\ln{10}} - 1 \\right) 10^{2 \\langle g \\rangle + \\sigma^2 \\ln{10}} $$\n",
    "$$ \\langle (c - \\langle{c}\\rangle)^3 \\rangle = \\mathrm{Var}[c]^{3/2} \\left(10^{\\sigma^2 \\ln{10}} + 2\\right) \\sqrt{10^{\\sigma^2 \\ln{10}} - 1} \\,\\, . $$\n",
    "\n",
    "For multiple odors, the set of $\\{g_{\\gamma}\\}$ is a multivariate O-U process as defined in Gardiner's Handbook. But in practice, we only consider independent odors, each can thus be thought of as a scalar O-U process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General case of the Ornstein-Uhlenbeck process\n",
    "The multivariate Langevin equation for the Ornstein-Uhlenbeck process is:\n",
    "\n",
    "$$ d\\vec{x} = -A \\vec{x}(t) dt + B dW(t) $$\n",
    "\n",
    "where $\\frac{dW}{dt} = \\vec{\\eta}(t)$, a vector of gaussian white noise (independent components), $A$ and $B$ are matrices. Assume the matrix $A$ is normal and can be diagonalized as $A = U D u^\\dagger$, $D = \\mathrm{diag}(\\lambda^1, ..., \\lambda^n)$. For a deterministic initial condition $\\vec{x}(t_0) = \\vec{x}_0$, the general solution is that $\\vec{x}(t)$ follows a multivariate normal distribution, with mean and variance given by\n",
    "\n",
    "$$ \\langle \\vec{x}(t) \\rangle = U\\mathrm{e}^{-D(t-t_0)}U^{\\dagger} \\vec{x}_0 $$\n",
    "$$ \\langle \\vec{x}(t) \\vec{x}(t)^T \\rangle = U J(t, t_0) U^\\dagger $$\n",
    "\n",
    "where the components of $J$ are \n",
    "\n",
    "$$ J^{ij}(t, t_0) = \\left(\\frac{U^\\dagger B B^T U}{\\lambda^i + \\lambda^j} \\right)^{ij} \\left(1 - e^{-(\\lambda^i + \\lambda^j)(t - t_0)}  \\right) $$\n",
    "\n",
    "\n",
    "The stationary distribution of $\\vec{x}$ is\n",
    "\n",
    "$$ \\vec{x}^* \\sim \\mathcal{N} \\left(\\vec{0}, U^{ik} \\left(\\frac{B B^T}{\\lambda^k + \\lambda^l} \\right)^{kl} (U^\\dagger)^{lj} \\right) \\,\\, .$$\n",
    "\n",
    "### Exact numerical simulation, general case\n",
    "To simulate a realization of this process exactly, we use a trick suggested by Gillespie in the univariate case (which only works for the Ornstein-Uhlenbeck process because it's linear and gaussian). We iteratively take $\\vec{x}(t)$ as the initial condition of the evolution up to $\\vec{x}(t + \\Delta t)$, the distribution of which is\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) \\sim \\mathcal{N}\\left( U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) , U J(t + \\Delta t, t) U^\\dagger \\right) $$\n",
    "\n",
    "which can be rewritten using the following property of multivariate normal distributions: if $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$, then $\\vec{x} = \\vec{\\mu} + \\Psi \\vec{n} \\sim \\mathcal{N}(\\vec{\\mu}, \\Psi \\Psi^T)$ ($\\Psi$ is the Cholesky decomposition of the desired covariance matrix). This property is easily demonstrated by computing $\\langle \\vec{x} \\rangle$ and $\\langle \\vec{x} \\vec{x}^T \\rangle$ and using the linearity of multivariate normal distributions. For our update rule, this gives\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) + \\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right] \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n}$ is a vector of standard normal(0, 1) samples. The matrices $U e^{-D \\Delta t}U^\\dagger$ and $\\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right]$ can be computed only once and applied repeatedly to the $\\vec{x}(t)$ obtained in sequence and the $\\vec{n}$ drawn at each iteration. The Cholesky decomposition of $UJU^\\dagger$ is not obviously expressed in terms of $B$, because the possibly different $\\lambda^i$ values mix up components. \n",
    "\n",
    "\n",
    "### Simple case and exact simulation of it\n",
    "If $A$ is diagonal, the $U$ matrices are just identity matrices and disappear, but the Cholesky decomposition of $J(t + \\Delta t, t)$ is still not obvious. More explicit expressions can be obtained in the simplifying case where $A$ is proportional to the identity matrix, i.e., all components of $\\vec{x}$ have the same fluctuation time scale. \n",
    "\n",
    "Let's say that $A =  \\frac{1}{\\tau} \\mathbb{1}$, where $\\tau$ is the fluctuation time scale ($\\lambda^i = \\tau \\,\\, \\forall i$). Then, the matrix $J$ simplifies to \n",
    "\n",
    "$$J(t, t_0) = \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  $$\n",
    "\n",
    "and its Cholesky decomposition is simply $\\sqrt{\\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right) } B$. Hence, the distribution of $\\vec{x}(t)$ at any time since $t_0$ (deterministic initial condition $\\vec{x}_0$) is\n",
    "\n",
    "$$ \\vec{x}(t) \\sim \\mathcal{N} \\left(e^{-(t-t_0)/\\tau} \\vec{x}_0, \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  \\right) $$\n",
    "\n",
    "The stationary distribution is simply the above with the exponential factors set to 0. The update rule from $\\vec{x}(t)$ to $\\vec{x}(t + \\Delta t)$ to simulate a realization of the process is nicer as well:\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = e^{-\\Delta t / \\tau} \\vec{x}(t) + \\sqrt{\\frac{\\tau}{2} \\left(1 - e^{-2\\Delta t/\\tau}  \\right)} B \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$ is a vector of independent standard normal samples.\n",
    "\n",
    "As before, we can compute once the (scalar) factor $e^{-\\Delta t / \\tau}$ and the . This is exact for any $\\Delta t$, there is no increase in accuracy by decreasing $\\Delta t$. You just choose the $\\Delta t$ resolution at which you want to sample the realization of the process. \n",
    "\n",
    "### Symmetric choices for correlations\n",
    "We want all pairs of $\\nu_\\alpha$ to have the same correlation. More specifically, we want to force a Pearson correlation coefficient of $0 < \\rho < 1$ between any pair of $\\nu$s. We suppose all background components have the same individual variance $\\sigma^2$. The corresponding covariance matrix we want for the steady-state distribution is\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & \\rho & \\ldots & \\rho \\\\\n",
    "    \\rho & 1 & \\ldots & \\rho \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    \\rho & \\rho & \\ldots & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we apply Cholesky decomposition to get $\\Sigma = \\Psi \\Psi^T$, then $\\sqrt{\\tau/2} B = \\Psi$, since the steady-state covariance of the Ornstein-Uhlenbeck process is, in this simplified case, $\\frac{\\tau}{2} BB^T$. The $M_B$ coefficient in the update rule is then\n",
    "\n",
    "$$ M_B = \\sqrt{\\tau/2(1 - e^{-2 \\Delta t/\\tau})}B = \\sqrt{(1 - e^{-2 \\Delta t/\\tau})} \\Psi $$\n",
    "\n",
    "The other coefficient is just\n",
    "\n",
    "$$ M_A = e^{-\\Delta t / \\tau} \\mathbb{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modelfcts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodelfcts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mibcm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     integrate_inhib_ibcm_network_options,\n\u001b[32m     10\u001b[39m     ibcm_respond_new_odors,\n\u001b[32m     11\u001b[39m     compute_mbars_cgammas_cbargammas,\n\u001b[32m     12\u001b[39m     ibcm_respond_new_odors\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodelfcts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mibcm_analytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     fixedpoint_thirdmoment_exact, \n\u001b[32m     16\u001b[39m     ibcm_fixedpoint_w_thirdmoment, \n\u001b[32m     17\u001b[39m     ibcm_all_largest_eigenvalues,\n\u001b[32m     18\u001b[39m     ibcm_saddle_eigenvalues\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodelfcts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbiopca\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     integrate_inhib_biopca_network_skip,\n\u001b[32m     22\u001b[39m     build_lambda_matrix,\n\u001b[32m     23\u001b[39m     biopca_respond_new_odors\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'modelfcts'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoint_thirdmoment_exact, \n",
    "    ibcm_fixedpoint_w_thirdmoment, \n",
    "    ibcm_all_largest_eigenvalues,\n",
    "    ibcm_saddle_eigenvalues\n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import (\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor, \n",
    "    compute_optimal_matrices\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_logou_kinputs,\n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray\n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)\n",
    "plt.rcParams[\"axes.facecolor\"] = (1,1,1,0)  # transparent background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"optimal\": \"Optimal\", \n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:light green\",\n",
    "    \"optimal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 25  # Half the real number for faster simulations\n",
    "n_components = 6  # Number of background odors\n",
    "\n",
    "inhib_rates = [0.0001, 0.00002]  # alpha, beta\n",
    "\n",
    "# Simulation duration\n",
    "duration = 320000.0\n",
    "deltat = 1.0\n",
    "skp = 20\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_logou_kinputs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x6fe5a179ffe6f22c0d705e844176ab8e)\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "# Log-normal concentrations, nus are the logs of concentrations\n",
    "ln10 = np.log(10.0)\n",
    "averages_nu = -0.5*np.ones(n_components)  # Average of log(c); for c < 1, these averages are < 0\n",
    "init_nu = averages_nu.copy()\n",
    "init_bkvec = np.exp(averages_nu*ln10).dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "\n",
    "# Mean and variance of the concentrations themselves\n",
    "# Using moments of log-normal: https://en.wikipedia.org/wiki/Log-normal_distribution\n",
    "mean_nu_lnbase = averages_nu.mean() * ln10\n",
    "vari_nu_lnbase = sigma2 * ln10**2\n",
    "lognorm_mean = np.exp(mean_nu_lnbase + vari_nu_lnbase/2.0)\n",
    "lognorm_vari = (np.exp(vari_nu_lnbase) - 1.0)*np.exp(2*mean_nu_lnbase + vari_nu_lnbase)\n",
    "# Third centered moment: rom skewness, multiply by its variance**3\n",
    "lognorm_skewness = (np.exp(vari_nu_lnbase) + 2)*np.sqrt(np.exp(vari_nu_lnbase) - 1)\n",
    "lognorm_thirdmom = lognorm_skewness * lognorm_vari**1.5\n",
    "moments_conc_lognorm = [lognorm_mean, lognorm_vari, lognorm_thirdmom]\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 24  # Number of inhibitory neurons for IBCM case\n",
    "\n",
    "# Model rates\n",
    "learnrate_ibcm = 0.00075  # 0.000001 = 1e-6\n",
    "tau_avg_ibcm = 200\n",
    "coupling_eta_ibcm = 0.5/n_i_ibcm\n",
    "decay_relative_ibcm = 0.0  # dummy\n",
    "k_c2bar_avg = 1.0  # dummy\n",
    "ssat_ibcm = 50.0  # dummy\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    ssat_ibcm, \n",
    "    k_c2bar_avg,\n",
    "    decay_relative_ibcm\n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"tanh\", \n",
    "    \"variant\": \"intrator\", \n",
    "    \"decay\": False\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.1*rgen_meta.standard_normal(size=[n_i_ibcm, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back_list, \n",
    "                ibcm_rates, inhib_rates, back_params, duration, \n",
    "                deltat, seed=simul_seed, noisetype=\"normal\",  \n",
    "                skp=skp, **ibcm_options\n",
    ")\n",
    "\n",
    "(tser_ibcm, \n",
    " nuser_ibcm, \n",
    " bkvecser_ibcm, \n",
    " mser_ibcm, \n",
    " cbarser_ibcm, \n",
    " thetaser_ibcm,\n",
    " wser_ibcm, \n",
    " yser_ibcm) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 160000 // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, back_components)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction, exact: need moments of nu. \n",
    "mean_g = np.mean(averages_nu)\n",
    "mean_conc = 10.0**(mean_g + 0.5*sigma2*ln10)\n",
    "variance_conc = (10.0**(sigma2*ln10) - 1.0) * 10.0**(2*mean_g + sigma2*ln10)\n",
    "thirdmoment_conc = variance_conc**1.5 * (10.0**(sigma2*ln10) + 2.0) * np.sqrt(10.0**(sigma2*ln10) - 1.0)\n",
    "\n",
    "# Compare to previously computed analytical moments (moments_conc_lognorm) and to numerical values\n",
    "# The nuser are the zero-meaned gaussian variables: need to add back average nu first. \n",
    "fullconcser = np.exp((nuser_ibcm + averages_nu) * ln10)\n",
    "mean_conc_sim = np.mean(fullconcser)  # all odors i.i.d., can average over them. \n",
    "variance_conc_sim = np.mean((fullconcser - mean_conc_sim)**2)\n",
    "thirdmoment_sim = np.mean((fullconcser - mean_conc_sim)**3)\n",
    "moments_conc = [mean_conc, variance_conc, thirdmoment_conc]\n",
    "moments_conc_sim = [mean_conc_sim, variance_conc_sim, thirdmoment_sim]\n",
    "# These three lists of moments should all agree. First two exactly, last should be close\n",
    "print(moments_conc_lognorm)\n",
    "print(moments_conc)\n",
    "print(moments_conc_sim)\n",
    "\n",
    "# Analytical prediction\n",
    "hs_hn = fixedpoint_thirdmoment_exact(moments_conc, 1, n_components-1, lambd=lambd_ibcm)\n",
    "hsum_fixed, h2sum_fixed = hs_hn[2:]\n",
    "hs_hn = hs_hn[:2]\n",
    "h_specif, h_nonspecif = hs_hn\n",
    "\n",
    "# Also check the saddle point where all h_gammas are equal, I suspect the model goes there first. \n",
    "saddle_h = fixedpoint_thirdmoment_exact(moments_conc, n_components, 0, lambd=lambd_ibcm)[0]\n",
    "print(saddle_h)\n",
    "\n",
    "# Count how many dot products are at each possible value. Use cbar = 1.0 as a split. \n",
    "split_val = 1.0\n",
    "cbars_gamma_mean = np.mean(cbars_gamma[transient:], axis=0)\n",
    "cgammas_bar_counts = {\"above\": int(np.sum(cbars_gamma_mean.flatten() > split_val)), \n",
    "                      \"below\": int(np.sum(cbars_gamma_mean.flatten() <= split_val))}\n",
    "print(cgammas_bar_counts)\n",
    "\n",
    "specif_gammas = np.argmax(np.mean(cbars_gamma[transient:], axis=0), axis=1)\n",
    "print(specif_gammas)\n",
    "\n",
    "# Analytical W\n",
    "analytical_w = ibcm_fixedpoint_w_thirdmoment(inhib_rates, moments_conc, back_components, hs_hn, specif_gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cbar2_avg term throughout\n",
    "cbar2_avg_ser = moving_average(cbarser_ibcm*cbarser_ibcm, kernelsize=tau_avg_ibcm)\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm[:-tau_avg_ibcm], cbar2_avg_ser[:-tau_avg_ibcm, i], \n",
    "            color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000)\", ylabel=r\"$\\bar{c}^2$ moving average\")\n",
    "ax.axhline(h2sum_fixed * variance_conc, ls=\"--\", color=\"k\", lw=1.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=160000 // skp)\n",
    "ax.legend(loc=\"center right\")\n",
    "# Compare to exact analytical fixed point solution\n",
    "ax.axhline(h_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(h_nonspecif, ls=\"-\", color=\"k\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "ax.axhline(saddle_h, ls=\":\", color=\"grey\", \n",
    "           label=\"Saddle point?\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = fullconcser - mean_conc\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=1)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "i_colors = sns.color_palette(n_colors=n_dimensions)\n",
    "for j in range(n_i_ibcm):\n",
    "    for i in range(n_dimensions):\n",
    "        axes.flat[j].axhline(analytical_w[i, j], color=i_colors[i], ls=\"--\", \n",
    "                             lw=0.5, alpha=0.7, zorder=-j*n_dimensions-i)\n",
    "    #axes.flat[j].set_ylim([axes.flat[j].get_ylim()[0], np.amax(analytical_w[:, j])*1.1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of average fixed points\n",
    "Check the eigenvalues of the jacobian for one neuron, for every possible specificity. There are $2^{n_B}$ possibilities: choosing specific or not for each odor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_eigenvalues = ibcm_all_largest_eigenvalues(\n",
    "    moments_conc, ibcm_rates, back_components, m3=1.0, cut=1e-16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ibcm_specif_keys = list(all_max_eigenvalues.keys())\n",
    "ibcm_eig_values = np.asarray([all_max_eigenvalues[a] for a in ibcm_specif_keys])\n",
    "reals, imags = np.real(ibcm_eig_values), np.imag(ibcm_eig_values)\n",
    "ibcm_eig_values_specif1 = np.asarray([len(s) == 1 for s in ibcm_specif_keys], dtype=bool)\n",
    "highlights = ibcm_eig_values_specif1\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\")\n",
    "scaleup = 1e3\n",
    "ax.plot(reals[highlights]*scaleup, imags[highlights]*scaleup, marker=\"*\", mfc=\"b\", mec=\"b\", \n",
    "        ls=\"none\", label=\"One odor\", ms=8)\n",
    "ax.plot(reals[~highlights]*scaleup, imags[~highlights]*scaleup, marker=\"o\", mfc=\"k\", mec=\"k\", \n",
    "       ls=\"none\", label=\"0 or 2+ odors\", ms=6)\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.legend(title=\"Specificity\")\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{max}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{max}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also curious about the eigenvalues at the saddle point\n",
    "all_saddle_eigenvalues = ibcm_saddle_eigenvalues(\n",
    "    moments_conc, ibcm_rates, back_components, m3=1.0, cut=1e-16\n",
    ")\n",
    "\n",
    "# Note that there's always an eigenvalue for the threshold Theta. So the true number of eigenvalues is 1 + N_B\n",
    "# Now if N_B are positive, only one is negative, I'm not sure this is even a saddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "reals, imags = np.real(all_saddle_eigenvalues), np.imag(all_saddle_eigenvalues)\n",
    "ax.axvline(0.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "ax.axhline(0.0, ls=\"--\", color=\"k\")\n",
    "scaleup = 1e3\n",
    "sizes = np.arange(6, len(all_saddle_eigenvalues)+6)\n",
    "for i in range(len(reals)):\n",
    "    ax.plot(reals[i]*scaleup, imags[i]*scaleup, marker=\"o\", \n",
    "            mfc=(0, 0, 0, 1.0-0.1*i), mec=\"k\", ls=\"none\", ms=sizes[i])\n",
    "for side in (\"top\", \"right\"):\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.set(xlabel=r\"$\\mathrm{Re}(\\lambda_{\\mathrm{saddle}})$    ($\\times 10^{-3}$)\", \n",
    "      ylabel=r\"$\\mathrm{Im}(\\lambda_{\\mathrm{saddle}})$     ($\\times 10^{-3}$)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm_analytics import lambda_pca_equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = n_components  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 0.0005  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "# but scale it up to counteract W regularization\n",
    "lambda_range_pca = 0.8   # With range of 0.8, breaks degeneracy, find actual PCA. But here, want to show it doesn't always converge? \n",
    "lambda_max_pca = lambda_pca_equivalent(hs_hn, moments_conc, n_components, inhib_rates, verbose=True)\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "\n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca, xavg_rate_pca]\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " yser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lser_pca[-1])\n",
    "plt.colorbar()\n",
    "plt.title(\"L matrix\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 100000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, yser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "yser_ideal = bkvecser_ibcm * ideal_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal W manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matrix depends on new odor concentrations, so define them here\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "new_test_concs *= moments_conc[0]\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Compute optimal W matrix for all new odors possible\n",
    "dummy_rgen = np.random.default_rng(0x6e3e2886c30163741daaaf7c8b8a00e6)\n",
    "new_odors_from_distrib = generate_odorant([int(1e5), n_dimensions], dummy_rgen, lambda_in=0.1)\n",
    "new_odors_from_distrib /= l2_norm(new_odors_from_distrib)[:, None]\n",
    "optimal_matrices = compute_optimal_matrices(back_components, new_odors_from_distrib, moments_conc, new_test_concs)\n",
    "\n",
    "# Use the W matrix for the lowest concentration to inhibit the background\n",
    "yser_optimal = bkvecser_ibcm - bkvecser_ibcm.dot(optimal_matrices[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynorm_series = {\n",
    "    \"ibcm\": l2_norm(yser_ibcm), \n",
    "    \"biopca\": l2_norm(yser_pca), \n",
    "    \"avgsub\": l2_norm(yser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(yser_ideal),\n",
    "    \"optimal\": l2_norm(yser_optimal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(ynorm_series[a], **std_options)) for a in ynorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(ynorm_series[a], **mean_options) for a in ynorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "ynorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + ynorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + ynorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background tagging after habituation\n",
    "We create a projection matrix, then compute the tag assigned to the background after inhibition by each habituation model, over time. Hopefully, only IBCM inhibits enough to see tags go to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing background tag lengths at various time points for each model\n",
    "yser_dict = {\n",
    "    \"ibcm\": yser_ibcm, \n",
    "    \"biopca\": yser_pca, \n",
    "    \"avgsub\": yser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": yser_ideal, \n",
    "    \"optimal\": yser_optimal, \n",
    "    \"orthogonal\": np.zeros(yser_ibcm.shape)\n",
    "}\n",
    "tag_length_series = {a: np.zeros(tser_ibcm.shape[0]) for a in yser_dict.keys()}\n",
    "for a in yser_dict.keys():\n",
    "    for i in range(0, tag_length_series[a].shape[0]):\n",
    "        if bkvecser_ibcm[i].max() > 0:\n",
    "            tag = project_neural_tag(yser_dict[a][i], bkvecser_ibcm[i], \n",
    "                                 proj_mat, **projection_arguments)\n",
    "        else:\n",
    "            tag = (1,)*int(projection_arguments[\"kc_sparsity\"]*n_kc)\n",
    "        tag_length_series[a][i] = len(tag)\n",
    "tag_length_series_smooth = {a: moving_average(tag_length_series[a], **mean_options)\n",
    "                            for a in tag_length_series}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for model in yser_dict.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    ax.plot(tser_ibcm / 1000, tag_length_series_smooth[model], **props)\n",
    "ynorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "ax.set_ylabel(r\"Tag length, $\\mathrm{card}(z)$\")\n",
    "ax.set_xlabel(\"Time (x1000 steps)\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 100\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "# New odor concentrations, defined above\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "# sample_ss_distrib_thirdmoment(means_nu, covmat_nu, epsil, size=1, rgen=None)\n",
    "# First sample Gaussian\n",
    "conc_samples = rgen_meta.multivariate_normal(averages_nu, steady_covmat, \n",
    "                        size=n_test_times*(n_back_samples-1))  # Shaped [sample, component]\n",
    "conc_samples = 10.0**conc_samples\n",
    "back_samples = conc_samples.dot(back_components)\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_yvecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in yser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in yser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in yser_dict.keys()}\n",
    "jaccard_backs = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in yser_dict.keys()}\n",
    "back_tags = [project_neural_tag(b, b,proj_mat, **projection_arguments) for b in back_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ideal reduction factor for each concentration\n",
    "ideal_factors = [compute_ideal_factor(c, moments_conc[:2], [n_components, n_dimensions], \n",
    "                    generate_odorant, (dummy_rgen,)) for c in new_test_concs]\n",
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_yvecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_yvecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_yvecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_yvecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, mixtures, new_test_concs[k], \n",
    "                ideal_factors[k], **avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"optimal\"][i, j, k] = mixtures - mixtures.dot(optimal_matrices[k].T)\n",
    "            mixture_yvecs[\"orthogonal\"][i, j, k] = x_new_ort\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_yvecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_yvecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_yvecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_yvecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)\n",
    "                    jaccard_backs[mod][i, j, k, l] = max((jaccard(mix_tag, b) for b in back_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\", \"optimal\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/detection/compare_models_onerun_lognormal_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity with background odors vs with new odor: should be as low as possible\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\", \"optimal\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    # Take median across test times\n",
    "    all_jacs = np.median(jaccard_scores[m], axis=1)\n",
    "    all_back_jacs = np.median(jaccard_backs[m], axis=1)\n",
    "    for i in range(n_new_concs):\n",
    "        axes[i].plot(all_back_jacs[:, i].flatten(), all_jacs[:, i].flatten(),\n",
    "            marker=\"o\", ls=\"none\", ms=2.0, alpha=0.4, \n",
    "            label=model_nice_names.get(m, m), color=model_colors.get(m))\n",
    "# Draw a diagonal on each graph\n",
    "\n",
    "for i in range(n_new_concs):\n",
    "    ylim = [0, 0.95]\n",
    "    xlim = [0, 0.95]\n",
    "    axes[i].plot(xlim, ylim, ls=\"--\", color=\"grey\", lw=1.0)\n",
    "    axes[i].set_xlim(xlim)\n",
    "    axes[i].set_ylim(ylim)\n",
    "\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard with background\\n(lower is better)\")\n",
    "    axes[i].set_ylabel(\"Jaccard with new odor\\n(higher is better)\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/detection/compare_models_onerun_lognormal_jaccards_back_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\", \"optimal\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_yvecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/detection/compare_models_onerun_non-gaussian_ynorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save some results for further plotting\n",
    "In particular, save excerpt of concentration time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want an unskipped nu time series for the figure, run a dummy simulation for its background\n",
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration/2.0, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=1, **avg_options\n",
    ")\n",
    "_, nuser_noskp, _, _, _ = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "results_filename = os.path.join(\"..\", \"results\", \"for_plots\", \"sample_lognormal_simulation.npz\")\n",
    "save_skp = 10\n",
    "# Include some results from BioPCA too. \n",
    "np.savez_compressed(\n",
    "    results_filename, \n",
    "    tser=tser_ibcm[::save_skp], \n",
    "    nuser=nuser_noskp,   # Can get conc_ser and bkvecser from that and components\n",
    "    mbarser=mbarser[::save_skp],  # Can get cbars_gamma_ser from that and components\n",
    "    cbarser=cbarser_ibcm[::save_skp],\n",
    "    wser=wser_ibcm[::save_skp], \n",
    "    yser=yser_ibcm[::save_skp], \n",
    "    true_pca_vals=true_pca[0],\n",
    "    true_pca_vecs=true_pca[1], \n",
    "    learnt_pca_vals=learnt_pca[0][::save_skp],\n",
    "    learnt_pca_vecs=learnt_pca[1][::save_skp],\n",
    "    off_diag_l_pca=off_diag_l_avg_abs[::save_skp],\n",
    "    align_error_pca=align_error_ser[::save_skp],\n",
    "    yser_pca=yser_pca[::save_skp],\n",
    "    yser_optimal=yser_optimal[::save_skp], \n",
    "    back_vecs=back_components, \n",
    "    hs_hn=hs_hn,\n",
    "    saddle_h=saddle_h,\n",
    "    analytical_w=analytical_w,  # Can get analytical yser from that, hs_hn, and specif_gammas\n",
    "    specif_gammas=specif_gammas, \n",
    "    skp=skp*save_skp,\n",
    "    averages_nu0=averages_nu, \n",
    "    sigma2=sigma2, \n",
    "    ibcm_eig_values=ibcm_eig_values, \n",
    "    new_test_concs=new_test_concs,\n",
    "    moments_conc=moments_conc\n",
    ")\n",
    "\n",
    "# Save Jaccard similarities for this run\n",
    "jaccards_filename = os.path.join(\"..\", \"results\", \"for_plots\", \"jaccards_onerun_lognormal_simulation.npz\")\n",
    "all_jacs2 = {m:jaccard_scores[m] for m in jaccard_scores.keys()}\n",
    "all_jacs2.pop(\"ideal\")\n",
    "all_back_jacs2 = {m+\"_back\": jaccard_backs[m] for m in jaccard_backs.keys()}\n",
    "all_back_jacs2.pop(\"ideal_back\")\n",
    "all_jacs2.update(all_back_jacs2)\n",
    "np.savez_compressed(\n",
    "    jaccards_filename, \n",
    "    **all_jacs2\n",
    ")\n",
    "\n",
    "with open(os.path.join(\"..\", \"results\", \"for_plots\", \"ibcm_eigenvalues_keys_lognormal_example.json\"), \"w\") as f:\n",
    "    json.dump(ibcm_specif_keys, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jacs2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
