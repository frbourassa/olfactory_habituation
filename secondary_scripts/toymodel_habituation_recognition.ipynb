{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habituation to a toy model of odor background\n",
    "Look at new odor recognition on top of a two-odor, one fluctuating proportion background:\n",
    "\n",
    "$$ \\vec{x}(t) = \\left(\\frac12 + \\nu \\right) \\vec{x}_a + \\left(\\frac12 - \\nu \\right) \\vec{x}_b $$\n",
    "\n",
    "The odor vectors are generated with i.i.d. exponential elements with scale (mean) mu=0.1, then normalized to have an L2 norm of 1. I take $\\nu(t)$ following a Ornstein-Uhlenbeck process, typically with standard deviation $\\sigma = 0.3$ and average $\\langle \\nu \\rangle = 0$. \n",
    "\n",
    "Here, I optionally clip elements of $\\vec{x}(t)$ to be non-negative, although this does not make a significant difference since samples where $|\\nu| > 0.5$ are rare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "\n",
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network_options,\n",
    "    ibcm_respond_new_odors,\n",
    "    compute_mbars_cgammas_cbargammas,\n",
    "    ibcm_respond_new_odors\n",
    ")\n",
    "from modelfcts.ibcm_analytics import (\n",
    "    fixedpoints_barm_2vectors, \n",
    "    fixedpoints_w_2vectors, \n",
    "    fixedpoint_s_2vectors_instant, \n",
    ")\n",
    "from modelfcts.biopca import (\n",
    "    integrate_inhib_biopca_network_skip,\n",
    "    build_lambda_matrix,\n",
    "    biopca_respond_new_odors\n",
    ")\n",
    "from modelfcts.average_sub import (\n",
    "    integrate_inhib_average_sub_skip, \n",
    "    average_sub_respond_new_odors\n",
    ")\n",
    "from modelfcts.ideal import(\n",
    "    find_projector, \n",
    "    find_parallel_component, \n",
    "    ideal_linear_inhibitor, \n",
    "    compute_ideal_factor_toy\n",
    ")\n",
    "from modelfcts.checktools import (\n",
    "    analyze_pca_learning\n",
    ")\n",
    "from modelfcts.backgrounds import (\n",
    "    update_ou_2inputs,\n",
    "    generate_odorant\n",
    ")\n",
    "from modelfcts.tagging import (\n",
    "    project_neural_tag, \n",
    "    create_sparse_proj_mat, \n",
    "    SparseNDArray, \n",
    ")\n",
    "from utils.statistics import seed_from_gen\n",
    "from utils.smoothing_function import (\n",
    "    moving_average, \n",
    "    moving_var\n",
    ")\n",
    "from simulfcts.plotting import (\n",
    "    plot_cbars_gamma_series, \n",
    "    plot_w_matrix, \n",
    "    plot_background_norm_inhibition, \n",
    "    plot_background_neurons_inhibition, \n",
    "    plot_pca_results, \n",
    "    hist_outline\n",
    ")\n",
    "from simulfcts.analysis import compute_back_reduction_stats\n",
    "from utils.metrics import jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import l2_norm, l1_norm, linf_norm, cosine_dist\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aesthetic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.style.use(['dark_background'])\n",
    "plt.rcParams[\"figure.figsize\"] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ibcm\", \"biopca\", \"avgsub\", \"ideal\", \"orthogonal\", \"none\"]\n",
    "model_nice_names = {\n",
    "    \"ibcm\": \"IBCM\",\n",
    "    \"biopca\": \"BioPCA\",\n",
    "    \"avgsub\": \"Average\",\n",
    "    \"ideal\": \"Ideal\",\n",
    "    \"orthogonal\": \"Orthogonal\",\n",
    "    \"none\": \"None\"\n",
    "}\n",
    "model_colors = {\n",
    "    \"ibcm\": \"xkcd:turquoise\",\n",
    "    \"biopca\": \"xkcd:orangey brown\",\n",
    "    \"avgsub\": \"xkcd:navy blue\",\n",
    "    \"ideal\": \"xkcd:powder blue\",\n",
    "    \"orthogonal\": \"xkcd:pale rose\",\n",
    "    \"none\": \"grey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common simulation parameters\n",
    "n_dimensions = 25\n",
    "n_components = 2  # Two odors but only one fluctuating proportion\n",
    "\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# Simulation duration\n",
    "duration = 80000.0\n",
    "deltat = 1.0\n",
    "skp = 1  # We can save all time steps, small enough network\n",
    "\n",
    "# Common model options\n",
    "activ_function = \"identity\"  #\"ReLU\"\n",
    "\n",
    "# Background process\n",
    "update_fct = update_ou_2inputs\n",
    "\n",
    "# Choose randomly generated background vectors\n",
    "rgen_meta = np.random.default_rng(seed=0x4146e7d68791560a406669280c37e5bb)\n",
    "\n",
    "back_components = np.zeros([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components[i] = generate_odorant(n_dimensions, rgen_meta, lambda_in=0.1)\n",
    "back_components = back_components / l2_norm(back_components).reshape(-1, 1)\n",
    "\n",
    "# Seed for background simulation, to make sure all models are the same\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "average_nu = np.zeros(1)\n",
    "init_nu = np.zeros(1)\n",
    "init_bkvec = 0.5*back_components[0] + 0.5*back_components[1]\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "sigma2_nu = 0.09\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_alphas (same for all)\n",
    "update_coefs_mean = np.exp(-deltat/tau_nu)\n",
    "update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*deltat/tau_nu)))\n",
    "\n",
    "back_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBCM habituation\n",
    "### IBCM simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBCM model parameters\n",
    "n_i_ibcm = 2  # Two neurons is enough, one per fixed point x_{\\pm}\n",
    "\n",
    "learnrate_ibcm = 0.0025\n",
    "tau_avg_ibcm = 300\n",
    "coupling_eta_ibcm = 0.2\n",
    "decay_relative_ibcm = 0.005\n",
    "lambd_ibcm = 1.0\n",
    "ibcm_rates = [\n",
    "    learnrate_ibcm, \n",
    "    tau_avg_ibcm, \n",
    "    coupling_eta_ibcm, \n",
    "    lambd_ibcm,\n",
    "    None,   # Saturation, none\n",
    "    None,   # For the Law and Cooper variant\n",
    "    decay_relative_ibcm\n",
    "]\n",
    "ibcm_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"saturation\": \"linear\", \n",
    "    \"variant\": \"intrator\", \n",
    "    \"decay\": True\n",
    "}\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_ibcm = 0.1*rgen_meta.random(size=[n_i_ibcm, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the IBCM simulations\n",
    "simul_seed = seed_from_gen(rgen_meta)\n",
    "sim_results = integrate_inhib_ibcm_network_options(\n",
    "                init_synapses_ibcm, update_fct, init_back_list, \n",
    "                ibcm_rates, inhib_rates, back_params, duration, \n",
    "                deltat, seed=simul_seed, noisetype=\"normal\",  \n",
    "                skp=skp, **ibcm_options\n",
    ")\n",
    "\n",
    "(tser_ibcm, \n",
    " nuser_ibcm, \n",
    " bkvecser_ibcm, \n",
    " mser_ibcm, \n",
    " cbarser_ibcm, \n",
    " thetaser_ibcm,\n",
    " wser_ibcm, \n",
    " yser_ibcm) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBCM habituation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 50000 // skp\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser_ibcm, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(\n",
    "                                    mser_ibcm, coupling_eta_ibcm, back_components)\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Analytical prediction of fixed points\n",
    "analytical_barm, analytical_sign_labels = fixedpoints_barm_2vectors(back_components, np.sqrt(sigma2_nu), \n",
    "                                               coupling_eta_ibcm, n_r=n_dimensions, lambd=lambd_ibcm)\n",
    "# Determine which fixed point the system is at\n",
    "mdots_signs = np.sign(mbarser_ibcm[-1].dot(back_components[0] - back_components[1]))\n",
    "sign_map = {1:\"+\", -1:\"-\"}\n",
    "mdots_signs = \"(\" + \", \".join(map(lambda x: sign_map.get(x), mdots_signs)) + \")\"\n",
    "sign_idx = analytical_sign_labels.index(mdots_signs)\n",
    "\n",
    "# Plot dot products with x_a and x_b, in terms of reduced m vectors. \n",
    "# Take any of the fixed points for one neuron, its two dot products are the only two possible dot product values. \n",
    "c_specif = np.dot(analytical_barm[sign_idx, 0], back_components[0])\n",
    "c_nonspecif = np.dot(analytical_barm[sign_idx, 0], back_components[1])\n",
    "\n",
    "# Analytical predictions of W and s\n",
    "# rates, fixed_mbar, bk_components, sigma2\n",
    "ibcm_w_pred = fixedpoints_w_2vectors(inhib_rates, analytical_barm[sign_idx, [0, 1]], back_components, sigma2_nu) \n",
    "ibcm_yser_pred = fixedpoint_s_2vectors_instant(inhib_rates, bkvecser_ibcm, options=ibcm_options)\n",
    "analytical_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cbar2_avg term throughout\n",
    "cbar2_avg_ser = moving_average(cbarser_ibcm*cbarser_ibcm, kernelsize=tau_avg_ibcm)\n",
    "neurons_cmap = sns.color_palette(\"Greys\", n_colors=n_i_ibcm)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_i_ibcm):\n",
    "    ax.plot(tser_ibcm[:-tau_avg_ibcm], cbar2_avg_ser[:-tau_avg_ibcm, i], \n",
    "            color=neurons_cmap[i])\n",
    "ax.set(xlabel=\"Time (x1000)\", ylabel=r\"$\\bar{c}^2$ moving average\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax, _ = plot_cbars_gamma_series(tser_ibcm, cbars_gamma, \n",
    "                        skp=10, transient=50000 // skp)\n",
    "# Compare to exact analytical fixed point solution\n",
    "ax.axhline(c_specif, ls=\"--\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{specific}}$\")\n",
    "ax.axhline(c_nonspecif, ls=\"-.\", color=\"grey\", \n",
    "           label=r\"Analytical $\\bar{c}_{\\gamma=\\mathrm{non}}$\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between nu's and c's, see if some neurons are specific to odors\n",
    "# Each neuron turns out to correlate its response to  one concentration\n",
    "# that means it is specific to that odor. \n",
    "cbarser_norm_centered = cbarser_ibcm - np.mean(cbarser_ibcm[transient:], axis=0)\n",
    "conc_ser_centered = np.stack([nuser_ibcm[:, 0], -nuser_ibcm[:, 0]], axis=1)\n",
    "correl_c_nu = np.mean(cbarser_norm_centered[transient:, :, None] \n",
    "                      * conc_ser_centered[transient:, None, :], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(correl_c_nu.T)\n",
    "ax.set(ylabel=r\"Component $\\gamma$\", xlabel=r\"Neuron $i$\")\n",
    "fig.colorbar(img, label=r\"$\\langle (\\bar{c}^i - \\langle \\bar{c}^i \\rangle)\"\n",
    "             r\"(\\nu_{\\gamma} - \\langle \\nu_{\\gamma} \\rangle) \\rangle$\", \n",
    "            location=\"top\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Check if each component has at least one neuron\n",
    "split_val = 0.0\n",
    "for comp in range(n_components):\n",
    "    print(\"Number of neurons specific to component {}: {}\".format(\n",
    "            comp, np.sum(np.mean(cbars_gamma[-2000:, :, comp], axis=0) > split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_ibcm, bkvecser_ibcm, yser_ibcm, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_ibcm, wser_ibcm, skp=100)\n",
    "for j in range(axes.size):\n",
    "    for i in range(n_dimensions):\n",
    "        axes.flat[j].axhline(ibcm_w_pred[j, i], color=\"grey\", zorder=-i, lw=1.0)\n",
    "fig.set_size_inches(*[1.5*a for a in fig.get_size_inches()])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second moment of s, simulation vs prediction\n",
    "sts_ibcm_ser = np.sum(yser_ibcm*yser_ibcm, axis=1)\n",
    "sts_ibcm_pred = np.sum(ibcm_yser_pred*ibcm_yser_pred, axis=1)\n",
    "fig, axes = plt.subplots(2, sharex=True)\n",
    "axes = axes.flatten()\n",
    "axes[0].plot(tser_ibcm/1000, sts_ibcm_ser, color=\"k\")\n",
    "axes[0].plot(tser_ibcm/1000, sts_ibcm_pred, color=\"blue\", alpha=0.9)\n",
    "axes[0].set(ylabel=r\"$\\vec{s}^T \\vec{s}$\")\n",
    "axes[0].set_ylim([-0.01, 0.1])\n",
    "# Plot the difference\n",
    "axes[1].plot(tser_ibcm/1000, sts_ibcm_pred - sts_ibcm_ser, color=\"k\")\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Pred. $-$ sim. $\\vec{s}^T \\vec{s}$\")\n",
    "axes[1].set_ylim([-0.1, 0.05])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export IBCM simulation results for plotting\n",
    "Export background concentration time series, IBCM time series of $M$ and $W$. \n",
    "\n",
    "Maybe PCA time series for supplementary plots. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results_filename = os.path.join(\"..\", \"results\", \"for_plots\", \"sample_2d_simulation.npz\")\n",
    "save_skp = 10\n",
    "np.savez_compressed(results_filename, \n",
    "    tser=tser_ibcm[::save_skp], \n",
    "    nuser=nuser_ibcm, \n",
    "    mbarser=mbarser_ibcm[::save_skp], \n",
    "    cbarser=cbarser_ibcm[::save_skp], \n",
    "    wser=wser_ibcm[::save_skp], \n",
    "    yser=yser_ibcm[::save_skp], \n",
    "    back_vecs=back_components, \n",
    "    analytical_barm=analytical_barm[sign_idx], \n",
    "    analytical_w=ibcm_w_pred, \n",
    "    analytical_factor=np.ones(1)*analytical_factor,\n",
    "    save_skp=np.ones(1, dtype=int)*save_skp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioPCA simulation\n",
    "### BioPCA habituation simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPCA model parameters\n",
    "n_i_pca = 1  # Number of inhibitory neurons for BioPCA case\n",
    "\n",
    "# Model rates\n",
    "learnrate_pca = 0.0005  # Learning rate of M\n",
    "# Choose Lambda diagonal matrix as advised in Minden et al., 2018\n",
    "lambda_range_pca = 0.5\n",
    "lambda_max_pca = 5.0\n",
    "lambda_mat_diag = build_lambda_matrix(lambda_max_pca, lambda_range_pca, n_i_pca)\n",
    "# Learning rate of L, relative to learnrate. Adjusted to Lambda in the integration function\n",
    "rel_lrate_pca = 2.0  #  / lambda_max_pca**2 \n",
    "xavg_rate_pca = learnrate_pca\n",
    "pca_options = {\n",
    "    \"activ_fct\": activ_function, \n",
    "    \"remove_lambda\": False, \n",
    "    \"remove_mean\": True\n",
    "}\n",
    "biopca_rates = [learnrate_pca, rel_lrate_pca, lambda_max_pca, lambda_range_pca]\n",
    "if pca_options[\"remove_mean\"]:\n",
    "    biopca_rates.append(xavg_rate_pca)\n",
    "\n",
    "\n",
    "# Initial synaptic weights: small positive noise\n",
    "init_synapses_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_i_pca)\n",
    "init_mmat_pca = rgen_meta.standard_normal(size=[n_i_pca, n_dimensions]) / np.sqrt(n_dimensions)\n",
    "init_lmat_pca = np.eye(n_i_pca, n_i_pca)  # Supposed to be near-identity, start as identity\n",
    "ml_inits_pca = [init_mmat_pca, init_lmat_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "sim_results = integrate_inhib_biopca_network_skip(\n",
    "                ml_inits_pca, update_fct, init_back_list, biopca_rates, \n",
    "                inhib_rates, back_params, duration, deltat, \n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **pca_options)\n",
    "(tser_pca, \n",
    " nuser_pca, \n",
    " bkvecser_pca, \n",
    " mser_pca, \n",
    " lser_pca, \n",
    " xser_pca, \n",
    " cbarser_pca, \n",
    " wser_pca, \n",
    " yser_pca) = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioPCA simulation analysis\n",
    "\n",
    "#### Analytical predictions for one PCA neuron\n",
    "$L$ matrix: 1x1, a scalar, equal to the inverse of the eigenvalue: $L = 1/L' = \\frac{1}{\\sigma^2 \\| \\vec{x}_s \\|^2}$\n",
    "\n",
    "$M$ matrix: 1x$n_R$, parallel to the fluctuating part of the background, $\\vec{m} = \\sigma^2 \\| \\vec{x}_s \\| \\vec{x}_s$.\n",
    "\n",
    "$W$ matrix: $n_Rx1$, a vector also parallel to $\\vec{x}_s$, we find $\\vec{w} = \\frac{\\sigma^2 \\| \\vec{x}_s \\|}{\\sigma^2 \\| \\vec{x}_s \\|^2 + \\beta/\\alpha} \\vec{x}_s$. \n",
    "\n",
    "Then the instantaneous PN activity should be $\\vec{s}(t) = \\vec{x}(t) - \\langle \\vec{x} \\rangle - WLM(\\vec{x}(t) - \\langle \\vec{x} \\rangle) = \\frac{\\beta/\\alpha}{\\beta/\\alpha + \\sigma^2 \\| \\vec{x}_s \\|^2} \\nu(t) \\vec{x}_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.pca_analytics import (\n",
    "    fixedpoints_pca_2vectors, \n",
    "    pca_fixedpoint_s_2vectors_instant, \n",
    "    pca_fixedpoint_s_2vectors_variance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical prediction\n",
    "x_s = (back_components[0] - back_components[1])\n",
    "x_d = (back_components[0] + back_components[1])/2\n",
    "m_pca_pred, l_pca_pred, w_pca_pred = fixedpoints_pca_2vectors(back_components, \n",
    "                                        sigma2_nu, inhib_rates, lambd=lambda_max_pca)\n",
    "yser_pca_pred = pca_fixedpoint_s_2vectors_instant(back_components, sigma2_nu, \n",
    "                                    inhib_rates, bkvecser_pca, lambd=lambda_max_pca)\n",
    "yvari_pca_pred = pca_fixedpoint_s_2vectors_variance(back_components,\n",
    "                                sigma2_nu, inhib_rates, lambd=lambda_max_pca)\n",
    "\n",
    "# Analysis of simulation\n",
    "res = analyze_pca_learning(bkvecser_pca, mser_pca, lser_pca, \n",
    "                           lambda_mat_diag, demean=pca_options[\"remove_mean\"])\n",
    "true_pca, learnt_pca, fser, off_diag_l_avg_abs, align_error_ser = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m_pca_pred)\n",
    "print(l_pca_pred)\n",
    "print(w_pca_pred)\n",
    "print(yser_pca_pred[-1])\n",
    "xvari_pca = np.var(l2_norm(bkvecser_pca, axis=1))\n",
    "xvari_pca = np.mean(np.sum((bkvecser_pca - x_d)**2, axis=1), axis=0)\n",
    "print(np.sqrt(yvari_pca_pred / xvari_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pca_results(tser_pca/1000, true_pca, learnt_pca, align_error_ser, off_diag_l_avg_abs)\n",
    "axes[-1].set_xlabel(\"Time (x1000 steps)\")\n",
    "fig.set_size_inches(fig.get_size_inches()[0], 3*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, ynorm_ser = plot_background_norm_inhibition(\n",
    "                                tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000 // skp\n",
    "norm_stats = compute_back_reduction_stats(bknorm_ser, ynorm_ser, trans=transient)\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['avg_reduction'] * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(norm_stats['std_reduction'] * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(norm_stats['std_reduction'] * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _ = plot_background_neurons_inhibition(tser_pca, bkvecser_pca, yser_pca, skp=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_w_matrix(tser_pca, wser_pca, skp=10)\n",
    "for i in range(n_dimensions):\n",
    "    axes.flat[0].axhline(w_pca_pred[0, i], ls=\"--\", zorder=-i, lw=0.5)\n",
    "fig.set_size_inches(plt.rcParams[\"figure.figsize\"])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second moment of s, simulation vs prediction\n",
    "sts_pca_ser = np.sum(yser_pca*yser_pca, axis=1)\n",
    "sts_pca_pred = np.sum(yser_pca_pred*yser_pca_pred, axis=1)\n",
    "fig, axes = plt.subplots(2, sharex=True)\n",
    "axes = axes.flatten()\n",
    "axes[0].plot(tser_pca/1000, sts_pca_ser, color=\"k\")\n",
    "axes[0].plot(tser_pca/1000, sts_pca_pred, color=\"blue\", alpha=0.9)\n",
    "axes[0].set(ylabel=r\"$\\vec{s}^T \\vec{s}$\")\n",
    "# Plot the difference\n",
    "axes[1].plot(tser_pca/1000, sts_pca_pred - sts_pca_ser, color=\"k\")\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Pred. $-$ sim. $\\vec{s}^T \\vec{s}$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export BioPCA simulation for plotting\n",
    "We have a good analytical understanding of BioPCA's W weights on the toy model. Show that in a supplementary figure too. Maybe combine with the toy model figure of IBCM? TBD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = os.path.join(\"..\", \"results\", \"for_plots\", \"sample_2d_simulation_biopca.npz\")\n",
    "save_skp = 10\n",
    "np.savez_compressed(\n",
    "    results_filename, \n",
    "    tser=tser_pca[::save_skp], \n",
    "    nuser=nuser_pca[::save_skp], \n",
    "    mser=mser_pca[::save_skp],\n",
    "    m_pred=m_pca_pred,\n",
    "    lser=lser_pca[::save_skp],\n",
    "    l_pred=l_pca_pred,\n",
    "    wser=wser_pca[::save_skp], \n",
    "    w_pred=w_pca_pred,\n",
    "    yser=yser_pca[::save_skp], \n",
    "    yser_pred=yser_pca_pred[::save_skp],\n",
    "    yvari_pred=yvari_pca_pred,\n",
    "    true_pca_vals=true_pca[0],\n",
    "    true_pca_vecs=true_pca[1], \n",
    "    learnt_pca_vals=learnt_pca[0][::save_skp],\n",
    "    learnt_pca_vecs=learnt_pca[1][::save_skp],\n",
    "    align_error=align_error_ser[::save_skp],\n",
    "    back_vecs=back_components,\n",
    "    save_skp=np.ones(1, dtype=int)*save_skp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average background subtraction simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average subtraction model parameters\n",
    "avg_options = {\"activ_fct\": activ_function}\n",
    "\n",
    "# Initial synaptic weights: dummy\n",
    "init_synapses_avg = np.zeros([1, n_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = integrate_inhib_average_sub_skip(\n",
    "                init_synapses_avg, update_fct, init_back_list, \n",
    "                [], inhib_rates, back_params, duration, deltat,\n",
    "                seed=simul_seed, noisetype=\"normal\", skp=skp, **avg_options\n",
    ")\n",
    "tser_avg, bkser_avg, bkvecser_avg, wser_avg, yser_avg = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal inhibition\n",
    "The component parallel to the background is reduced to beta / (2*alpha + beta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_projector = find_projector(back_components.T)\n",
    "ideal_factor = inhib_rates[1] / (2*inhib_rates[0] + inhib_rates[1])\n",
    "yser_ideal = bkvecser_ibcm * ideal_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for background inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynorm_series = {\n",
    "    \"ibcm\": l2_norm(yser_ibcm), \n",
    "    \"biopca\": l2_norm(yser_pca), \n",
    "    \"avgsub\": l2_norm(yser_avg), \n",
    "    \"none\": l2_norm(bkvecser_ibcm), \n",
    "    \"ideal\": l2_norm(yser_ideal)\n",
    "}\n",
    "std_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "mean_options = dict(kernelsize=2001, boundary=\"free\")\n",
    "std_series = {\n",
    "    a: np.sqrt(moving_var(ynorm_series[a], **std_options)) for a in ynorm_series\n",
    "} \n",
    "mean_series = {\n",
    "    a: moving_average(ynorm_series[a], **mean_options) for a in ynorm_series\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes = axes.flatten()\n",
    "for model in std_series.keys():\n",
    "    props = dict(label=model_nice_names[model], color=model_colors[model])\n",
    "    axes[0].plot(tser_ibcm / 1000, mean_series[model], **props)\n",
    "    axes[1].plot(tser_ibcm / 1000, std_series[model], **props)\n",
    "ynorm_string = r\"$\\|\\vec{s}\\|$\"\n",
    "axes[0].set_ylabel(r\"PN activity norm, \" + ynorm_string)\n",
    "axes[1].set(xlabel=\"Time (x1000 steps)\", ylabel=r\"Standard deviation \" + ynorm_string)\n",
    "axes[0].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(4.5, 2.5*2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison for new odor recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snap_index(dt, skip, times):\n",
    "    \"\"\" Find nearest multiple of dt*skip to each time in times \"\"\"\n",
    "    return np.around(times / (dt*skip)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "n_kc = 1000\n",
    "projection_arguments = {\n",
    "    \"kc_sparsity\": 0.05,\n",
    "    \"adapt_kc\": True,\n",
    "    \"n_pn_per_kc\": 3,\n",
    "    \"project_thresh_fact\": 0.1\n",
    "}\n",
    "proj_mat = create_sparse_proj_mat(n_kc, n_dimensions, rgen_meta)\n",
    "\n",
    "yser_dict = {\n",
    "    \"ibcm\": yser_ibcm, \n",
    "    \"biopca\": yser_pca, \n",
    "    \"avgsub\": yser_avg, \n",
    "    \"none\": bkvecser_ibcm, \n",
    "    \"ideal\": yser_ideal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new odors, select test times, etc.\n",
    "# New odors tested\n",
    "n_new = 100\n",
    "new_odors = generate_odorant([n_new, n_dimensions], rgen_meta, lambda_in=0.1)\n",
    "new_odors /= l2_norm(new_odors)[:, None]\n",
    "\n",
    "# Test times\n",
    "n_test_times = 10\n",
    "start_test_t = duration - n_test_times * 2000.0\n",
    "test_times = np.linspace(start_test_t, duration, n_test_times)\n",
    "test_times -= deltat*skp\n",
    "test_idx = find_snap_index(deltat, skp, test_times)\n",
    "\n",
    "# New odor concentrations\n",
    "new_test_concs = np.asarray([0.5, 1.0])\n",
    "avg_whiff_conc = 0.5\n",
    "print(\"Average whiff concentration: {:.4f}\".format(avg_whiff_conc))\n",
    "new_test_concs *= avg_whiff_conc\n",
    "n_new_concs = len(new_test_concs)\n",
    "\n",
    "# Background samples, indexed [time, sample, n_orn]\n",
    "n_back_samples = 10\n",
    "# Steady-state distribution of nu: normal distribution, mean 0, variance sigma2\n",
    "conc_samples = rgen_meta.normal(loc=0.0, scale=np.sqrt(sigma2_nu), size=n_test_times*(n_back_samples-1))\n",
    "# Clip between -0.5 and 0.5\n",
    "conc_samples = np.clip(conc_samples, a_min=-0.5, a_max=0.5).reshape(-1, 1)\n",
    "back_samples = (0.5+conc_samples)*back_components[0:1] + (0.5-conc_samples)*back_components[1:2]\n",
    "back_samples = back_samples.reshape([n_test_times, n_back_samples-1, -1])\n",
    "back_samples = np.concatenate([bkvecser_ibcm[test_idx, None, :], back_samples], axis=1)\n",
    "\n",
    "# Containers for s vectors of each model\n",
    "mixture_yvecs = {a: np.zeros([n_new, n_test_times,  n_new_concs,  \n",
    "                    n_back_samples, n_dimensions]) for a in yser_dict.keys()}\n",
    "mixture_tags = {a: SparseNDArray((n_new, n_test_times, n_new_concs,\n",
    "                    n_back_samples, n_kc), dtype=bool) for a in yser_dict.keys()}\n",
    "new_odor_tags = sparse.lil_array((n_new, n_kc), dtype=bool)\n",
    "jaccard_scores = {a: np.zeros([n_new, n_test_times, n_new_concs,  n_back_samples]) \n",
    "                  for a in yser_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ideal reduction factor for each concentration\n",
    "dummy_rgen = np.random.default_rng(0x6e3e2886c30163741daaaf7c8b8a00e6)\n",
    "ideal_factors = [compute_ideal_factor_toy(c, sigma2_nu, n_dimensions, \n",
    "                    generate_odorant, (dummy_rgen,)) for c in new_test_concs]\n",
    "for i in range(n_new):\n",
    "    # Compute neural tag of the new odor alone, without inhibition\n",
    "    new_tag = project_neural_tag(\n",
    "                    new_odors[i], new_odors[i],\n",
    "                    proj_mat, **projection_arguments\n",
    "                )\n",
    "    new_odor_tags[i, list(new_tag)] = True\n",
    "    # Parallel and orthogonal components\n",
    "    x_new_par = find_parallel_component(new_odors[i], \n",
    "                        back_components, back_projector)\n",
    "    x_new_ort = new_odors[i] - x_new_par\n",
    "    # Now, loop over snapshots, mix the new odor with the back samples,\n",
    "    # compute the PN response at each test concentration,\n",
    "    # compute tags too, and save results\n",
    "    for j in range(n_test_times):\n",
    "        jj = test_idx[j]\n",
    "        for k in range(n_new_concs):\n",
    "            mixtures = (back_samples[j]\n",
    "                + new_test_concs[k] * new_odors[i])\n",
    "            # odors, mlx, wmat, \n",
    "            # Compute for each model\n",
    "            mixture_yvecs[\"ibcm\"][i, j, k] = ibcm_respond_new_odors(\n",
    "                mixtures, mser_ibcm[jj], wser_ibcm[jj], \n",
    "                ibcm_rates, options=ibcm_options\n",
    "            )\n",
    "            mixture_yvecs[\"biopca\"][i, j, k] = biopca_respond_new_odors(\n",
    "                mixtures, [mser_pca[jj], lser_pca[jj], xser_pca[jj]], \n",
    "                wser_pca[jj], biopca_rates, options=pca_options\n",
    "            )\n",
    "            mixture_yvecs[\"avgsub\"][i, j, k] = average_sub_respond_new_odors(\n",
    "                mixtures, wser_avg[jj], options=avg_options\n",
    "            )\n",
    "            mixture_yvecs[\"none\"][i, j, k] = mixtures\n",
    "            mixture_yvecs[\"ideal\"][i, j, k] = ideal_linear_inhibitor(\n",
    "                x_new_par, x_new_ort, mixtures, new_test_concs[k], \n",
    "                ideal_factors[k], **avg_options\n",
    "            )\n",
    "            for l in range(n_back_samples):\n",
    "                for mod in mixture_yvecs.keys():\n",
    "                    mix_tag = project_neural_tag(\n",
    "                        mixture_yvecs[mod][i, j, k, l], mixtures[l],\n",
    "                        proj_mat, **projection_arguments\n",
    "                    )\n",
    "                    try:\n",
    "                        mixture_tags[mod][i, j, k, l, list(mix_tag)] = True\n",
    "                    except ValueError as e:\n",
    "                        print(mix_tag)\n",
    "                        print(mixture_yvecs[mod][i, j, k, l])\n",
    "                        print(proj_mat.dot(mixture_yvecs[mod][i, j, k, l]))\n",
    "                        raise e\n",
    "                    jaccard_scores[mod][i, j, k, l] = jaccard(mix_tag, new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_jacs = jaccard_scores[m]\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_jacs[:, :, i, :].flatten(),\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            np.median(all_jacs[:, :, i, :]), ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    ax = axes[i]\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(\"Jaccard similarity (higher is better)\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/detection/compare_models_one_odor_habituation_{}.pdf\".format(activ_function),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to new odor\n",
    "# Plot model histogram results\n",
    "# One plot per new odor concentration\n",
    "fig, axes = plt.subplots(1, n_new_concs, sharex=True)\n",
    "fig.set_size_inches(9.5, 4)\n",
    "axes = axes.flatten()\n",
    "models = [\"none\", \"ideal\", \"avgsub\", \"biopca\", \"ibcm\"]\n",
    "all_medians = []\n",
    "for m in models:  # Plot IBCM last\n",
    "    all_distances = (mixture_yvecs[m] \n",
    "         - new_test_concs[None, None, :, None, None]*new_odors[:, None, None, None, :])\n",
    "    all_norms = l2_norm(all_distances.reshape(-1, n_dimensions))\n",
    "    all_medians.append(np.median(all_norms))\n",
    "    for i in range(n_new_concs):\n",
    "        hist_outline(\n",
    "            axes[i], all_norms,\n",
    "            bins=\"doane\", density=True, label=model_nice_names.get(m, m),\n",
    "            color=model_colors.get(m), alpha=1.0\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            all_medians[-1], ls=\"--\",\n",
    "            color=model_colors.get(m)\n",
    "        )\n",
    "# Labeling the graphs, etc.\n",
    "for i in range(n_new_concs):\n",
    "    axes[i].set_xlim([0.0, 2.0*max(all_medians)])\n",
    "    axes[i].set_title(\"New conc. = {:.1f}\".format(new_test_concs[i]))\n",
    "    axes[i].set_xlabel(r\"Distance to new odor, $\\|\\vec{s} - \\vec{x}_{\\mathrm{new}}\\|$\")\n",
    "    axes[i].set_ylabel(\"Probability density\")\n",
    "axes[1].legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0), frameon=False)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/detection/compare_models_onerun_ynorm_{}.pdf\".format(activ_fct),\n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary panels: convergence time\n",
    "Considering one IBCM neuron, comparing analytical predictions to numerical simulations of how long it takes to reach steady-state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm_analytics import find_convergence_time, analytical_convergence_times_2d\n",
    "from modelfcts.ibcm import integrate_ibcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the convergence prediction with the previous IBCM parameters\n",
    "# but with a new simulation for a single neuron. Still with n_ORN=25\n",
    "def example_convergence_time(input_vecs, learning_mu, tau_theta, moments_nu, tau_nu, rng):\n",
    "    n_orn = input_vecs.shape[1]\n",
    "    tmax = 160000\n",
    "    dt = 1.0\n",
    "    ## Recompute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "    average_nu, sigma2_nu = moments_nu\n",
    "    update_coefs_mean = np.exp(-dt/tau_nu)\n",
    "    update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*dt/tau_nu)))\n",
    "    init_back_vec = np.sum(input_vecs, axis=0) / 2\n",
    "    bk_update_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]\n",
    "    \n",
    "    # Show an example\n",
    "    m_init = 0.025*rng.standard_normal(size=n_orn)\n",
    "    simul_seed = seed_from_gen(rng)\n",
    "    tser, mser, nuser, cser, _, bkvecser = integrate_ibcm(m_init, update_ou_2inputs, \n",
    "                        [np.zeros(1), init_back_vec], bk_update_params, tmax, dt, \n",
    "                        learnrate=learning_mu, seed=simul_seed, noisetype=\"normal\", tavg=tau_theta)\n",
    "    \n",
    "    mdota_ser = np.dot(mser, input_vecs[0])\n",
    "    mdotb_ser = np.dot(mser, input_vecs[1])\n",
    "    mdotd_ser = 0.5*(mdota_ser + mdotb_ser)\n",
    "    mdots_ser = mdota_ser - mdotb_ser\n",
    "\n",
    "    initial_mdotds = [mdotd_ser[0], mdots_ser[0]]\n",
    "    norms_vecds = [0.25*np.sum((input_vecs[0] + input_vecs[1])**2), \n",
    "                   np.sum((input_vecs[0] - input_vecs[1])**2)]\n",
    "    predict_td, predict_ts = analytical_convergence_times_2d(initial_mdotds, \n",
    "                                        norms_vecds, learning_mu, sigma2_nu)\n",
    "    \n",
    "    mdotsers = np.stack([mdota_ser, mdotb_ser, mdotd_ser, mdots_ser])\n",
    "    predicts_tds = np.asarray([predict_td, predict_ts])\n",
    "    analytical_mdotab = np.asarray([1.0 + 1.0 / (2.0 * np.sqrt(sigma2_nu)), \n",
    "                                    1.0 - 1.0 / (2.0 * np.sqrt(sigma2_nu))])\n",
    "    analytical_mdotds = np.asarray([1.0, 1.0 / np.sqrt(sigma2_nu), -1.0 / np.sqrt(sigma2_nu)])\n",
    "    \n",
    "    return tser, mdotsers, predicts_tds, analytical_mdotab, analytical_mdotds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments_conc = [average_nu, sigma2_nu]\n",
    "rgen_time = np.random.default_rng(seed=0x2e56695e3b22bb595b16e645a2e9402)\n",
    "res = example_convergence_time(back_components, learnrate_ibcm, \n",
    "                        tau_avg_ibcm, moments_conc, tau_nu, rgen_time)\n",
    "tser_t, mdotsers, tdspreds, mdotabpreds, mdotdspreds = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the example\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes = axes.flatten()\n",
    "ax = axes[0]\n",
    "# Plot dot products with vectors x_a and x_b\n",
    "skpt = 20\n",
    "ax.plot(tser_t[::skpt], mdotsers[0][::skpt], color=\"red\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_a$\")\n",
    "ax.plot(tser_t[::skpt], mdotsers[1][::skpt], color=\"pink\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_b$\")\n",
    "\n",
    "# Analytical predictions\n",
    "ax.axhline(mdotabpreds[0], label=r\"$1 + 1/(2 \\sigma)$\", ls=\"--\", color=\"k\")\n",
    "ax.axhline(mdotabpreds[1], label=r\"$1 - 1/(2 \\sigma)$\", ls=\"-.\", color=\"k\")\n",
    "ax.set(xlabel=\"Time\", ylabel=r\"Dot products $\\vec{m} \\cdot \\vec{x}_{a, b}$\")\n",
    "\n",
    "# Plot dot products with x_s and x_d\n",
    "ax = axes[1]\n",
    "ax.plot(tser_t[::skpt], mdotsers[3][::skpt], color=\"orange\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_s$\")\n",
    "ax.plot(tser_t[::skpt], mdotsers[2][::skpt], color=\"blue\", label=r\"$\\vec{m}(t) \\cdot \\vec{x}_d$\")\n",
    "\n",
    "ax.axhline(mdotdspreds[1], label=r\"$\\vec{m} \\cdot \\vec{x}_s = \\pm 1/\\sigma$\", ls=\"-.\", \n",
    "           color=\"xkcd:orangey brown\")\n",
    "ax.axhline(mdotdspreds[0], label=r\"$\\vec{m} \\cdot \\vec{x}_d = 1$\", color=\"xkcd:marine\", ls=\"--\")\n",
    "ax.set(xlabel=\"Time\", ylabel=r\"Dot products $\\vec{m} \\cdot \\vec{x}_{s, d}$\")\n",
    "# Convergence times predicted\n",
    "for ax in axes:\n",
    "    ax.axvline(tdspreds[0], color=\"cyan\", ls=\":\", label=r\"Conv. time $c_d$\")\n",
    "    ax.axvline(tdspreds[1], color=\"orange\", ls=\"--\", label=r\"Conv. time $c_s$\")\n",
    "    ax.legend(fontsize=8, loc='center right')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([0, 40000, 80000, 120000, 160000])\n",
    "fig.set_size_inches(7.5, 3.)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_convergence_time(*args, seed_sequence=None, n_tries=2):\n",
    "    \"\"\" Check how ts and td scale as a function of initial x_d and x_s\n",
    "    n_tries x n_tries values of initial x_s and x_d are tried\n",
    "    Plot t_d - t_s, which should depend on eps_d only once this leading\n",
    "    order behaviour of t_s is removed. \n",
    "    \n",
    "    Same arguments as example_convergence_time, except the rng one. \n",
    "    \"\"\"\n",
    "    input_vecs, learning_mu, tau_theta, moments_nu, tau_nu = args\n",
    "    \n",
    "    # Random number generation business\n",
    "    if seed_sequence is None:\n",
    "        seed_sequence = np.random.SeedSequence()\n",
    "    all_seeds = list(seed_sequence.spawn(n_tries*n_tries))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_orn = input_vecs.shape[1]\n",
    "    tmax = 160000\n",
    "    dt = 1.0\n",
    "    ## Recompute the coefficients in the Ornstein-Uhlenbeck update equation\n",
    "    average_nu, sigma2_nu = moments_nu\n",
    "    update_coefs_mean = np.exp(-dt/tau_nu)\n",
    "    update_coefs_noise = np.sqrt(sigma2_nu*(1 - np.exp(-2*dt/tau_nu)))\n",
    "    init_back_vec = np.sum(input_vecs, axis=0) / 2\n",
    "    bk_update_params = [average_nu, update_coefs_mean, update_coefs_noise, back_components]\n",
    "    \n",
    "    # Loop over pairs of x_s, x_d values\n",
    "    epsd_axis = np.geomspace(0.02, 0.2, n_tries)*0.5\n",
    "    epss_axis = np.geomspace(0.01, 0.1, n_tries)*0.5\n",
    "    td_grid = np.zeros([2, n_tries, n_tries])  # Should only depend on epss\n",
    "    ts_grid = np.zeros([2, n_tries, n_tries])  \n",
    "    x_d, x_s = np.sum(input_vecs, axis=0)/2, input_vecs[0] - input_vecs[1]    \n",
    "    norms_vecds = [np.sum(x_d**2), np.sum(x_s**2)]\n",
    "    \n",
    "    for i, epsd in enumerate(epsd_axis):\n",
    "        for j, epss in enumerate(epss_axis):\n",
    "            # Combine x_s and x_d to form initial m vector (x_s, x_d are orthogonal, so easy)\n",
    "            m_init = epsd * x_d / norms_vecds[0] + epss * x_s / norms_vecds[1]\n",
    "            tser, mser, nuser, cser, _, bkvecser = integrate_ibcm(m_init, update_ou_2inputs, \n",
    "                        [np.zeros(1), init_back_vec], bk_update_params, tmax, dt, \n",
    "                        learnrate=learning_mu, seed=all_seeds.pop(), noisetype=\"normal\", tavg=tau_theta)\n",
    "    \n",
    "            mdota_ser = np.dot(mser, input_vecs[0])\n",
    "            mdotb_ser = np.dot(mser, input_vecs[1])\n",
    "            mdotd_ser = 0.5*(mdota_ser + mdotb_ser)\n",
    "            mdots_ser = mdota_ser - mdotb_ser\n",
    "            initial_mdotds = [mdotd_ser[0], mdots_ser[0]]  # Should equal epss, epsd\n",
    "    \n",
    "            td_grid[0, i, j], ts_grid[0, i, j] = analytical_convergence_times_2d(initial_mdotds, \n",
    "                                                            norms_vecds, learning_mu, sigma2_nu)\n",
    "            td_grid[1, i, j], ts_grid[1, i, j] = find_convergence_time(tser, mdotd_ser, mdots_ser, sigma2_nu)\n",
    "        print(\"Completed {} points at eps_d = {}\".format(n_tries, epsd))\n",
    "    \n",
    "    return np.stack([epsd_axis, epss_axis]), np.stack([td_grid, ts_grid])\n",
    "\n",
    "def plot_time_analysis_results(eps_axes, time_grids):\n",
    "    n_tries = eps_axes[0].size\n",
    "    epsd_axis, epss_axis = eps_axes\n",
    "    td_grid, ts_grid = time_grids\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes = axes.flatten()\n",
    "    # td as a function of epsd; should not depend on eps_s\n",
    "    ax = axes[0]\n",
    "    colors = sns.color_palette(\"mako\", n_colors=n_tries)\n",
    "    for j in range(epsd_axis.size):\n",
    "        ax.plot(epsd_axis, td_grid[1, :, j], color=colors[j], ls=\"--\",\n",
    "                label=r\"$\\epsilon_s = {:.3f}$\".format(epss_axis[j]), marker=\"o\")\n",
    "    # Plot theory, independent of eps_s\n",
    "    ax.plot(epsd_axis, td_grid[0, :, 0], label=\"Analytical\", color=\"k\", ls=\"-\", lw=2.0)\n",
    "    ax.set(xlabel=r\"$\\epsilon_d$ (initial $c_d = \\vec{m} \\cdot \\vec{x}_d$)\", \n",
    "           ylabel=r\"$t_d$ ($\\vec{m} \\cdot \\vec{x}_d$ convergence time)\", \n",
    "           title=\"First phase\", xscale=\"log\", yscale=\"log\")\n",
    "    ax.legend(title=\"Simulation\", fontsize=9)\n",
    "    \n",
    "    # ts - td as a function of epss\n",
    "    ax = axes[1]\n",
    "    colors = sns.color_palette(\"flare\", n_colors=n_tries)\n",
    "    # Plot simulations\n",
    "    for i in range(epsd_axis.size):\n",
    "        ax.plot(epss_axis, ts_grid[1, i] - td_grid[1, i], color=colors[i], ls=\"--\",\n",
    "                label=r\"$\\epsilon_d = {:.3f}$\".format(epsd_axis[i]), marker=\"o\")\n",
    "    # Plot theory, independent of eps_s\n",
    "    ax.plot(epss_axis, ts_grid[0, 0] - td_grid[0, 0], label=\"Analytical\", color=\"k\", ls=\"-\", lw=2.0)\n",
    "    ax.set(xlabel=r\"$\\epsilon_s$ (initial $c_s = \\vec{m} \\cdot \\vec{x}_s$)\", \n",
    "           ylabel=r\"$t_s$ ($\\vec{m} \\cdot \\vec{x}_s$ convergence time)\", \n",
    "           title=\"Second phase\", xscale=\"log\", yscale=\"log\")\n",
    "    ax.legend(title=\"Simulation\", fontsize=9)\n",
    "    \n",
    "    fig.set_size_inches(7.5, 3.5)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_axes, time_grids = scaling_convergence_time(back_components, learnrate_ibcm,\n",
    "                        tau_avg_ibcm, moments_conc, tau_nu, \n",
    "                        seed_sequence=np.random.SeedSequence(0xf06a864ae983b9fa690fa682a26aa05), n_tries=4)\n",
    "\n",
    "fig, axes = plot_time_analysis_results(eps_axes, time_grids)\n",
    "# fig.savefig(\"../figures/two_odors/ibcm_neuron_2d_convergence_time_scaling.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results_filename = os.path.join(\"..\", \"results\", \"for_plots\", \"convergence_time_2d_simulation.npz\")\n",
    "save_skp = 20\n",
    "np.savez_compressed(results_filename, \n",
    "    tser=tser_t[::save_skp], \n",
    "    mdotsers=mdotsers[:, ::save_skp], \n",
    "    tds_predictions=tdspreds, \n",
    "    mdotab_predictions=mdotabpreds, \n",
    "    mdotds_predictions=mdotdspreds,\n",
    "    eps_axes=eps_axes,\n",
    "    time_grids=time_grids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
