{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys_byteorder = sys.byteorder\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(1, \"..\")\n",
    "from utils.statistics import seed_from_gen\n",
    "from modelfcts.distribs import truncexp1_inverse_transform, truncexp1_density\n",
    "from modelfcts.backgrounds import (update_tc_odor, logof10, sample_background_powerlaw,\n",
    "                        sample_ss_conc_powerlaw, decompose_nonorthogonal_basis, update_alternating_inputs)\n",
    "from modelfcts.checktools import check_conc_samples_powerlaw_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimensions = 4  # Number of ORNs = number of recorded mixtures\n",
    "n_components = 3  # Number of independent components\n",
    "\n",
    "# Random number generator\n",
    "rgen_meta = np.random.default_rng(seed=0xcd905cb60bb4152747d486201735b136)\n",
    "\n",
    "back_params_sym = [\n",
    "    np.asarray([1.0] * n_components),        # whiff_tmins\n",
    "    np.asarray([100.] * n_components),       # whiff_tmaxs\n",
    "    np.asarray([2.0] * n_components),        # blank_tmins\n",
    "    np.asarray([200.0] * n_components),      # blank_tmaxs\n",
    "    np.asarray([0.3] * n_components),        # c0s\n",
    "    np.asarray([0.5] * n_components),        # alphas\n",
    "]\n",
    "back_params_sym = np.asarray(back_params_sym).T\n",
    "\n",
    "# Choose background odor components (mixing matrix columns)\n",
    "back_components_sym = np.ones([n_components, n_dimensions]) * 0.2\n",
    "#back_components_sym = rgen_meta.exponential(size=[n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    back_components_sym[i, i] = 0.8\n",
    "    back_components_sym[i] /= np.sqrt(np.sum(back_components_sym[i]**2))\n",
    "mixing_matrix = back_components_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time series of odor concentrations\n",
    "# Initial conditions\n",
    "n_steps = 100000\n",
    "deltat = 1.0\n",
    "\n",
    "tc_current = np.asarray([[2.0, 0.0], [1.0, 1.0], [0.0, 0.0]])\n",
    "c_series = np.zeros([n_steps+1, n_components])\n",
    "c_series[0] = tc_current[:, 1]\n",
    "unif_noises = rgen_meta.uniform(size=[n_steps, n_components, 2])\n",
    "for i in range(n_steps):\n",
    "    for j in range(n_components):\n",
    "        tc_current[j] = update_tc_odor(tc_current[j], deltat, unif_noises[i, j], *back_params_sym[j])\n",
    "    c_series[i] = tc_current[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that the generated time series have the right distribution\n",
    "fig, axes = check_conc_samples_powerlaw_exp1(c_series.T, *(back_params_sym.T))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check that the generated time series have the right distribution\n",
    "fig, axes = plt.subplots(n_components, sharex=True)\n",
    "trange = range(80000, 82000, 1)\n",
    "tslice = slice(80000, 82000, 1)\n",
    "for i in range(n_components):\n",
    "    axes[i].plot(trange, c_series[tslice, i], color=\"k\")\n",
    "    axes[i].set(xlabel=\"Time\", ylabel=r\"Concentration $\\nu$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent component analysis of the mixtures at the ORNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtures = np.dot(c_series, mixing_matrix)  # Indexed time, ORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ica = FastICA(n_components=n_components, algorithm=\"parallel\", whiten=\"unit-variance\", \n",
    "                  random_state=seed_from_gen(rgen_meta, nbits=32)).fit(mixtures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmixing matrix: apply to mixtures.T to recover sources\n",
    "unmix_results = fast_ica.components_.dot(mixtures.T)  # [source, ORN]x[time, ORN].T\n",
    "fast_ica.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovered components: normalize and compensate unmixing results\n",
    "recovered_components = fast_ica.mixing_.copy().T  # [source, ORN]\n",
    "for i in range(recovered_components.shape[0]):\n",
    "    norm_of_comp = np.sqrt(np.sum(recovered_components[i]**2))\n",
    "    recovered_components[i] /= norm_of_comp\n",
    "    unmix_results[i] *= norm_of_comp\n",
    "    if np.all(recovered_components[i] < 0):\n",
    "        recovered_components[i] *= -1\n",
    "        unmix_results[i] *= -1\n",
    "print(recovered_components)\n",
    "print(back_components_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_permutation(shuffled, reference):\n",
    "    \"\"\" Find permutation indices that reorder shuffled into reference. \n",
    "    I.e. find the indices where each row of reference is in shuffled, \n",
    "    so calling shuffled[indices] puts it back in order. \n",
    "    Do it by minimizing the squared difference between rows. \n",
    "    Greedy algorithm, may not be optimal if reconstruction\n",
    "    is not very accurate. \n",
    "    \"\"\"\n",
    "    indices = np.arange(shuffled.shape[0]).astype(int)\n",
    "    sum_axes = tuple(range(1, shuffled.ndim))\n",
    "    for i in range(len(indices)):\n",
    "        squared_dists = np.sum((shuffled - reference[i:i+1])**2, axis=sum_axes)\n",
    "        indices[i] = np.argmin(squared_dists)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut_indices = find_permutation(recovered_components, back_components_sym)\n",
    "recovered_components_ordered = recovered_components[permut_indices]\n",
    "unmix_results_ordered = unmix_results[permut_indices]\n",
    "print(recovered_components_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, so ICA seems to recover the components and the concentrations over time, up to a sign. \n",
    "# Can I find out how it recovers the components (fast_ica.mixing_ matrix) and apply\n",
    "# the same strategy to IBCM demixing? \n",
    "# Check that the generated time series have the right distribution\n",
    "fig, axes = plt.subplots(n_components, sharex=True)\n",
    "trange = range(80000, 82000, 1)\n",
    "tslice = slice(80000, 82000, 1)\n",
    "for i in range(n_components):\n",
    "    axes[i].plot(trange, unmix_results_ordered[i, tslice], color=\"xkcd:purple\")\n",
    "    axes[i].plot(trange, c_series[tslice, i], color=\"xkcd:grey\", ls=\":\")\n",
    "    axes[i].set(xlabel=\"Time\", ylabel=r\"Concentration $\\nu$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.linalg.pinv(fast_ica.components_))  # The mixing matrix A and the demixing matrix W are pseudo-inverses\n",
    "print(fast_ica.mixing_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA works extremely well to decompose the background\n",
    "This is true for independent odors, probably not for odors with some correlation (but I'm not sure how to code that for turbulent environments anyways for the moment). \n",
    "\n",
    "Two questions:\n",
    "- Can it work equally well online? In real-time, can it identify the current concentration of each source?\n",
    "    - Answer: yes, just apply the demixing matrix learnt up to now. \n",
    "- What does it do when a new source is presented?\n",
    "    - It tries to interpret it as part of the signal of the background. But the demixing matrix is not going to project into the orthogonal subspace: it only reconstructs a projection in the background subspace. So it should achieve what we want. \n",
    "    \n",
    "## How to use this\n",
    "Two options. \n",
    " 1. Figure out how, from IBCM, we can extract the mixing matrix A for the concentration-like IBCM activations c found from the IBCM network. \n",
    " 2. Use someone else's algorithm, BioNICA, to inhibit the olfactory background.\n",
    "\n",
    "Ideally, compare the two, and also online PCA. Hopefully, IBCM can work best when starting to combine odors non-linearly and with some correlation between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of a new odor\n",
    "\n",
    "- Learn an unmixing matrix from a long time series of background. \n",
    "- Then, try to unmix new samples that combine the background with a new odor. Also project back to the mixed space. \n",
    "- Compare to a projection in the background subspace: they should agree. \n",
    "- This means that by subtracting the demixed-remixed background+new odor, we are subtracting only the component int he background subspace. This is what we wanted to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mixtures of the existing background with new odors\n",
    "new_odor = np.roll(back_components_sym[0], shift=-1)  # Should be a new vector\n",
    "typical_conc = np.mean(back_params_sym[:, -2]) * np.mean(back_params_sym[:, -1])\n",
    "\n",
    "mix_samples, _ = sample_background_powerlaw(back_components_sym, *back_params_sym.T, size=1000, rgen=rgen_meta)\n",
    "# mix_samples indexed [n_samples, n_orn]\n",
    "mix_samples = mix_samples + 0.2 * new_odor.reshape(1, -1) * typical_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try demixing and projecting back\n",
    "demixed_concs = fast_ica.components_.dot(mix_samples.T)  # indexed [source, n_sample]\n",
    "recomposed_mixes = fast_ica.mixing_.dot(demixed_concs)  # [ORN, source]x[source, n_sample]\n",
    "\n",
    "# inhibit\n",
    "inhib_fraction = 5/6\n",
    "inhibited_mix = mix_samples - inhib_fraction*recomposed_mixes.T\n",
    "print(mix_samples)\n",
    "print(inhibited_mix)\n",
    "print(0.2*new_odor*typical_conc)\n",
    "#print(recomposed_mixes)\n",
    "\n",
    "# Check if only orthogonal part left: yes indeed! \n",
    "print(inhibited_mix[0].dot(back_components_sym[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(vecs):\n",
    "    \"\"\" Computes l2 norm of vectors stored along the last axis of vecs. \n",
    "    Args:\n",
    "        vecs can be either a single vector (1d) or an  arbitrary array of vectors, \n",
    "            where the last dimension indexes elements of vectors. \n",
    "    \n",
    "    Returns: if vecs is of shape (K x L x ... M x N), \n",
    "        returns an array of distances of shape (K x L x ... x M) \n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(vecs**2, axis=-1))\n",
    "\n",
    "def l1_norm(vecs):\n",
    "    \"\"\" |x| = \\sum_i |x_i|\"\"\"\n",
    "    return np.sum(np.abs(vecs), axis=-1)\n",
    "\n",
    "def linf_norm(vecs):\n",
    "    \"\"\" |x| = max_i(|x_i|) \"\"\"\n",
    "    return np.max(np.abs(vecs), axis=-1)\n",
    "\n",
    "def cosine_dist(x, y):\n",
    "    \"\"\" d(x, y) = 1 - (x \\cdot y)/(|x| |y|)\"\"\"\n",
    "    xnorm, ynorm = l2_norm(x), l2_norm(y)\n",
    "    return 1.0 - x.dot(np.moveaxis(y, -1, 0)) / xnorm / ynorm\n",
    "\n",
    "def distance_panel_target(mixes, target):\n",
    "    \"\"\" Compute a panel of distances between the pure (target) new odor and mixtures \n",
    "    (which can be without inhibition, with average inhibition, IBCM inhibition, etc.). \n",
    "    \n",
    "    Four distances included, in order: l2, l1, linf, cosine_dist\n",
    "    \n",
    "    Args:\n",
    "        mixes (np.ndarray): mixtures of odors to compute distance from target, \n",
    "            the last axis should have the size of target, \n",
    "            while other axes are arbitrary.  \n",
    "        target (np.1darray): target odor vector, same length as\n",
    "            last axis of mixes. \n",
    "    Returns:\n",
    "        dist_panel (np.ndarray): shape of pure, except the last axis, \n",
    "            which has length 4 (for the number of distances computed). \n",
    "    \"\"\"\n",
    "    # Make axis 0 the axis indexing distance metrics, to begin with\n",
    "    # And move it to the last axis before returning\n",
    "    dist_array = np.zeros([4] + list(mixes.shape[:-1]))\n",
    "    # No need to add axes to target vector; if it is 1d, it is broadcasted\n",
    "    # along the last axis of mixes, which indexes elements of each vector. \n",
    "    dist_array[0] = l2_norm(target - mixes)\n",
    "    dist_array[1] = l1_norm(target - mixes)\n",
    "    dist_array[2] = linf_norm(target - mixes)\n",
    "    dist_array[3] = cosine_dist(target, mixes)\n",
    "    \n",
    "    return np.moveaxis(dist_array, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "dist_pure_inhib_none = distance_panel_target(mix_samples, 0.2* typical_conc*new_odor )\n",
    "#dist_pure_inhib_avg = distance_panel_target(inhib_avg_samples, new_odor)\n",
    "dist_pure_inhib_ica = distance_panel_target(inhibited_mix, 0.2*typical_conc*new_odor)\n",
    "\n",
    "median_distances_none = np.median(dist_pure_inhib_none, axis=0)\n",
    "#median_distances_avg = np.median(dist_pure_inhib_avg, axis=0)\n",
    "median_distances_ica = np.median(dist_pure_inhib_ica, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_none = \"xkcd:navy blue\"\n",
    "clr_ibcm = \"xkcd:turquoise\"\n",
    "clr_ica = \"xkcd:cherry\"\n",
    "clr_avg = \"xkcd:orangey brown\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none[i], color=clr_none, ls=\"--\", lw=2.0)\n",
    "    #ax.hist(dist_pure_inhib_avg[:, i], label=\"Average inhibition\", facecolor=clr_avg, alpha=0.6, \n",
    "    #    edgecolor=clr_avg, density=True)\n",
    "    #ax.axvline(median_distances_avg[i], color=clr_avg, ls=\"--\", lw=1.0)\n",
    "    ax.hist(dist_pure_inhib_ica[:, i], label=\"ICA inhibition\", facecolor=clr_ica, alpha=0.6, \n",
    "        edgecolor=clr_ica, density=True) \n",
    "    ax.axvline(median_distances_ica[i], color=clr_ica, ls=\"--\", lw=2.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "ICA, offline, works really well. It does exactly what I expected.\n",
    "Offline, other algorithms would probably work in the same manner: it's easy to compute some basis for the background vector subspace, and then decompose the incoming odor on that subspace. PCA would do the same; any orthogonal basis of a set of samples would do the same, really. \n",
    "    \n",
    "What isn't obvious is how to then decompose the incoming odor on the basis; we need the 'demixing' matrix. Offile, that's a matrix inversion; what biological algorithm can do that online? \n",
    "\n",
    "If I can find a way, even approximate, to find the basis (A matrix, to project back) and the demixing matrix (basically the inverse of A), with IBCM, this is a worthy finding. \n",
    "\n",
    "Then, I could compare to BioNICA in terms of performance. I'm sure IBCM isn't going to do great, because it is not very efficient when there are long temporal correlations, it tends to diverge, it takes a long time to converge, etc. But at least in principle, it would be a meaningful finding. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to PCA\n",
    "PCA probably does not learn the odor components, but I'm curious to see how it can be used anyways to inhibit a background. Test the offline case in this notebook. For an online version, could use IncrementalPCA from sklearn. \n",
    "\n",
    "\n",
    "#### Assuming column vectors\n",
    "Take the PCs as \"odor\" vectors, A. Then, \"concentrations\" of those odors are obtained by decomposing an ORN vector on that basis, which is done by inverting the equation \n",
    "$$ \\vec{x} = A \\vec{c} \\Rightarrow \\vec{c} = A^+ \\vec{x} $$\n",
    "\n",
    "Then, the decomposition can be reassembled on the basis of PCs by taking the dot product with $A$ again; in one step, this means that a vector $\\vec{x}$ can be projected to the vector subspace spanned by $A$ using the projection $P = AA^+$: $A^+$ decomposes into 'concentrations' and $A$ combines back the basis vectors. \n",
    "\n",
    "#### With row vectors\n",
    "Matrices act from the right, so reverse the order of matrices. Projector is $A^+ A$, so the row vector corresponding to the projection of a column vector $\\vec{x}$ is obtained by taking $\\vec{x}^T A^+ A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_obj = PCA(n_components='mle').fit(mixtures)\n",
    "# Here, since rows contain vectors, the matrix A, A^+ act from the right, e.g., conc.dot(A) assembles odors. \n",
    "pca_basis = pca_obj.components_  # mixing matrix: indexed [component, orn]: each row is a basis vector\n",
    "pca_demixing = np.linalg.pinv(pca_basis)  # demixing matrix, pseudo-inverse of A. indexed [orn, component]\n",
    "pca_projector = pca_demixing.dot(pca_basis)  # Will act from the right, shape [orn, orn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try demixing and projecting back with PCA\n",
    "demixed_pca_concs = mix_samples.dot(pca_demixing)  # indexed [n_sample, component]\n",
    "recomposed_pca_mixes = demixed_pca_concs.dot(pca_basis)  # [n_sample, component]x[component, orn]\n",
    "\n",
    "# inhibit\n",
    "inhibited_pca_mix = mix_samples - inhib_fraction*recomposed_pca_mixes\n",
    "print(inhibited_pca_mix)\n",
    "print(0.2*new_odor*typical_conc)\n",
    "#print(recomposed_mixes)\n",
    "\n",
    "# Check if only orthogonal part left: yes indeed if inhib_fraction=1 ! \n",
    "print(inhibited_pca_mix[0].dot(back_components_sym[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "dist_pure_inhib_pca = distance_panel_target(inhibited_pca_mix, 0.2*typical_conc*new_odor)\n",
    "median_distances_pca = np.median(dist_pure_inhib_pca, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of distance to pure odor, for each distance\n",
    "# Overlay histogram for mix without and with inhibition\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "axes = axes.flatten()\n",
    "clr_pca = \"xkcd:green\"\n",
    "dist_names = [r\"$L^2$ distance\", r\"$L^1$ distance\", r\"$L^{\\infty}$ distance\", \"Cosine distance\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(dist_pure_inhib_none[:, i], label=\"No inhibition\", facecolor=clr_none, alpha=0.6, \n",
    "        edgecolor=clr_none, density=True)\n",
    "    ax.axvline(median_distances_none[i], color=clr_none, ls=\"--\", lw=2.0)\n",
    "    ax.hist(dist_pure_inhib_pca[:, i], label=\"ICA inhibition\", facecolor=clr_pca, alpha=0.6, \n",
    "        edgecolor=clr_pca, density=True) \n",
    "    ax.axvline(median_distances_pca[i], color=clr_pca, ls=\"--\", lw=2.0)\n",
    "    ax.set(xlabel=\"Distance to new odor\", ylabel=\"Probability density\", title=dist_names[i])\n",
    "axes[0].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion on PCA\n",
    "Both PCA and ICA give identical background inhibition in the learnt subspace. However, they learn different bases: PCA learns an orthogonal basis (principal components, obviously), while ICA can actually recover odor vectors and their concentrations, at least in the symmetric case and with independent concentrations, even though this basis is not orthogonal. Therefore, ICA is more interesting for its concrete relation to the problem studied, while PCA is mathematically simpler but more abstract. \n",
    "\n",
    "The big problem: both require computation of $A^+$ from $A$, or vice-versa, to demix or remix the concentrations into ORN activity. That's not a trivial operation to perform online with a neural network. \n",
    "But with the IBCM model, we would also need to do the same with the basis of synaptic weight vectors $\\vec{m}$. We easily obtain concentrations by projecting $\\vec{x}$ on the various $\\vec{m}$, corresponding to rows of the matrix $W = A^+$, but we would need to invert to find the actual vector components to recombine in order to inhibit odors in ORN space. \n",
    "\n",
    "So, this means we need neurons computing the pseudo-inverse of the synaptic weights $\\vec{m}$: not clear how. That's what I was trying with my $\\vec{w}$ vectors (notational mishap: my $\\vec{w}$ is really the $A$ matrix in Hyvarinen and Oja, not the $W$ matrix; their $W$ is my $M$, really. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
