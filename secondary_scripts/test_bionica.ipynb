{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test biologically plausible online non-negative ICA (BioNICA) network\n",
    "Two algorithms implementing non-negative ICA from Lipshutz, Pehlevan, Chklovskii, 2022. \n",
    "\n",
    "Their implementation is available on Github: https://github.com/flatironinstitute/bio-nica/\n",
    "I wrote my own implementation here.\n",
    "\n",
    "Both algorithms are tested. \n",
    "\n",
    "Note that NICA only works when sources have a finite, non-zero probability to be exactly zero. So this would probably not work very well with gaussian or near-gaussian sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from quadprog import solve_qp\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(0, \"../\")\n",
    "    \n",
    "from nica_algorithms import bio_nica_indirect, bio_nica_direct\n",
    "    \n",
    "import multiprocessing\n",
    "from psutil import cpu_count\n",
    "n_cpu = cpu_count(logical=False)\n",
    "\n",
    "from utils.statistics import seed_from_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm implementation\n",
    "Embedded in an online simulation where random samples are successively presented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate as a function of time. \n",
    "def learn_decay(t, eta0, delta):\n",
    "    return eta0 / (1.0 + delta * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Github implementation of algorithm 1\n",
    "def integrate_bionica1(m_init, l_init, update_bk, bk_init, bionica_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"uniform\"):\n",
    "    # Extract info about dimensionalities (notation K, D switched vs Lipshutz\n",
    "    # to match the one used for IBCM).\n",
    "    n_neu = m_init.shape[0]  # Number of ICA neurons N_K: number of extracted sources\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D: number of mixtures\n",
    "    n_int = l_init.shape[0]  # Number of interneurons N_I\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    learnrate0, decayrate, tau_avg = bionica_params\n",
    "    \n",
    "    # Initialize NICA algorithm object\n",
    "    \"\"\" \n",
    "    s_dim         -- Dimension of sources\n",
    "    x_dim         -- Dimension of mixtures\n",
    "    n_dim         -- Dimension of interneurons\n",
    "    P0            -- Initial guess for the lateral weight matrix P, must be of size s_dim by s_dim\n",
    "    W0            -- Initial guess for the forward weight matrix W, must be of size s_dim by x_dim\n",
    "    learning_rate -- Learning rate as a function of t\n",
    "    tau           -- Learning rate factor for M (multiplier of the W learning rate)\n",
    "    \"\"\"\n",
    "    nica_obj = bio_nica_indirect(n_neu, n_dim, n_int, eta0=learnrate0, decay=decayrate)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_int, n_neu])  # series of L (N_NxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of output projections\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))  # Sources (odor concentrations)\n",
    "    \n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L, n)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    bk_series[0] = bk_vari\n",
    "\n",
    "    # Generate required noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "\n",
    "        ## Update synaptic weights and background to time t+1 for next iteration\n",
    "        cbar = nica_obj.fit_next(bkvec)\n",
    "        cbar_series[k] = cbar\n",
    "        \n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bk_vari, bkvec = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        \n",
    "        t += dt\n",
    "\n",
    "        # Save synaptic weights at time step k+1\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = nica_obj.W\n",
    "        l_series[knext] = nica_obj.P.T\n",
    "        bkvec_series[knext] = bkvec\n",
    "        bk_series[knext] = bk_vari\n",
    "    \n",
    "    # Compute final neuronal activities with latest matrices and inputs\n",
    "    cbar = nica_obj.fit_next(bkvec)\n",
    "    cbar_series[-1] = cbar\n",
    "    \n",
    "    return tseries, bk_series, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Github implementation of algorithm 1\n",
    "def integrate_bionica2(m_init, l_init, update_bk, bk_init, bionica_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"uniform\"):\n",
    "    # Extract info about dimensionalities (notation K, D switched vs Lipshutz\n",
    "    # to match the one used for IBCM).\n",
    "    n_neu = m_init.shape[0]  # Number of ICA neurons N_K: number of extracted sources\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D: number of mixtures\n",
    "    n_int = l_init.shape[0]  # Number of interneurons N_I\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    learnrate0, decayrate, tau_avg, tau_l = bionica_params\n",
    "    \n",
    "    # Initialize NICA algorithm object\n",
    "    \"\"\" \n",
    "    s_dim         -- Dimension of sources\n",
    "    x_dim         -- Dimension of mixtures\n",
    "    M0            -- Initial guess for the lateral weight matrix M, must be of size s_dim by s_dim\n",
    "    W0            -- Initial guess for the forward weight matrix W, must be of size s_dim by x_dim\n",
    "    learning_rate -- Learning rate as a function of t\n",
    "    tau           -- Learning rate factor for M (multiplier of the W learning rate)\n",
    "    \"\"\"\n",
    "    nica_obj = bio_nica_direct(n_neu, n_dim, eta0=learnrate0, decay=decayrate, tau=tau_l)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_int, n_neu])  # series of L (N_NxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of output projections\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))  # Sources (odor concentrations)\n",
    "    \n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L, n)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    bk_series[0] = bk_vari\n",
    "\n",
    "    # Generate required noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "\n",
    "        ## Update synaptic weights and background to time t+1 for next iteration\n",
    "        cbar = nica_obj.fit_next(bkvec)\n",
    "        cbar_series[k] = cbar\n",
    "        \n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bk_vari, bkvec = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        \n",
    "        t += dt\n",
    "\n",
    "        # Save synaptic weights at time step k+1\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = nica_obj.W\n",
    "        l_series[knext] = nica_obj.M\n",
    "        bkvec_series[knext] = bkvec\n",
    "        bk_series[knext] = bk_vari\n",
    "    \n",
    "    # Compute final neuronal activities with latest matrices and inputs\n",
    "    cbar = nica_obj.fit_next(bkvec)\n",
    "    cbar_series[-1] = cbar\n",
    "    \n",
    "    return tseries, bk_series, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My own implementations, tabled for now, some problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1 with interneurons. Assume P = L^T. \n",
    "# Modifications to Github's implementation:\n",
    "#    - Using tau_avg averaging time rather than 1/current time: more realistic since no absolute zero time\n",
    "#    - Working with L instead of P (P = L^T)\n",
    "#    - Not checking for singular values of M, L every 100 steps\n",
    "def integrate_bionica1(m_init, l_init, update_bk, bk_init, bionica_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"uniform\"):\n",
    "    # Extract info about dimensionalities (notation K, D switched vs Lipshutz\n",
    "    # to match the one used for IBCM).\n",
    "    n_neu = m_init.shape[0]  # Number of ICA neurons N_K: number of extracted sources\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D: number of mixtures\n",
    "    n_int = l_init.shape[0]  # Number of interneurons N_I\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    learnrate0, decayrate, tau_avg = bionica_params\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_int, n_neu])  # series of L (N_NxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of output projections\n",
    "    # No need to save interneuron activities, easily recovered as n = Ly\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))  # Sources (odor concentrations)\n",
    "    \n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities (before applying L)\n",
    "    n = np.zeros(n_int)  # interneuron activities\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L, n)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    \n",
    "    # Running averages\n",
    "    x_avg = bk_vec_init.copy()\n",
    "    cbar_avg = cbar.copy()\n",
    "    n_avg = n.copy()\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    bk_series[0] = bk_vari\n",
    "\n",
    "    # Generate required noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    \n",
    "    # Constant matrices used often (don't re-create every iteration)\n",
    "    identity_neu = np.eye(n_neu)\n",
    "    zeros_neu = np.zeros(n_neu)\n",
    "    newax = np.newaxis\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ## Compute neuronal activity at time t with background and matrices at t\n",
    "        # Input projection with M\n",
    "        c = mmat.dot(bkvec)\n",
    "        \n",
    "        # Fast iterations for cbar, n: quasi-static approximation, use quadprog\n",
    "        # to optimize 1/2*cbar^T PL cbar - c^T cbar\n",
    "        # subject to cbar \\geq 0 at each element\n",
    "        # Solution without constraint: cbar = (L^T L)^{-1} c\n",
    "        cbar = solve_qp(G=lmat.T.dot(lmat), a=c, C=identity_neu, b=zeros_neu)[0]\n",
    "        n = lmat.dot(cbar)\n",
    "        \n",
    "        # Save neuronal activities at time t (we don't save interneurons n)\n",
    "        cbar_series[k] = cbar\n",
    "        \n",
    "        ## Update running averages of x, n, cbar up to time t\n",
    "        x_avg += dt * (bkvec - x_avg) / tau_avg\n",
    "        n_avg += dt * (n - n_avg) / tau_avg\n",
    "        cbar_avg += dt * (cbar - cbar_avg) / tau_avg\n",
    "        \n",
    "        ## Update synaptic weights and background to time t+1 for next iteration\n",
    "        # Based on neuronal activities, inputs, and running averages at time t\n",
    "        # Learning rate at time t\n",
    "        eta = learn_decay(t, learnrate0, decayrate)\n",
    "        # Synaptic plasticity: update mmat, lmat to k+1 based on cbar, n at k\n",
    "        mmat = mmat + dt*eta*((cbar - cbar_avg)[:, newax].dot((bkvec - x_avg)[newax, :]) - mmat)\n",
    "        lmat = lmat + dt*eta*((n - n_avg)[:, newax].dot((cbar - cbar_avg)[newax, :]) - lmat)\n",
    "\n",
    "        #if k % 100 == 0:\n",
    "        #    for i in range(n_neu):\n",
    "        #        if np.linalg.norm(mmat[i,:]) < .1:\n",
    "        #            print(f'iteration {k}: M row {i} small: {mmat[i,:]}')\n",
    "        #            mmat[i,:] = rng.standard_normal(n_dim)/np.sqrt(n_dim)\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bk_vari, bkvec = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        \n",
    "        t += dt\n",
    "\n",
    "        # Save synaptic weights at time step k+1\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        bkvec_series[knext] = bkvec\n",
    "        bk_series[knext] = bk_vari\n",
    "    \n",
    "    # Compute final neuronal activities with latest matrices and inputs\n",
    "    c = mmat.dot(bkvec)\n",
    "    cbar = solve_qp(G=lmat.T.dot(lmat), a=c, C=identity_neu, b=zeros_neu)[0]\n",
    "    cbar_series[-1] = cbar\n",
    "    \n",
    "    return tseries, bk_series, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 with two-compartment neurons\n",
    "# Modifications to Github's implementation:\n",
    "#    - Using tau_avg averaging time rather than 1/current time: more realistic since no absolute zero time\n",
    "#    - Not checking for singular values of M every 100 steps\n",
    "def integrate_bionica2(m_init, l_init, update_bk, bk_init, bionica_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"uniform\"):\n",
    "    # Extract info about dimensionalities (notation K, D switched vs Lipshutz\n",
    "    # to match the one used for IBCM).\n",
    "    n_neu = m_init.shape[0]  # Number of ICA neurons N_I: number of extracted sources\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D: number of mixtures\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    learnrate0, decayrate, tau_avg, tau_l = bionica_params\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of output projections\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "    bk_series = np.zeros([tseries.shape[0]] + list(bk_vari_init.shape))  # Sources (odor concentrations)\n",
    "    \n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities (before applying L)\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L, n)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    \n",
    "    # Running averages\n",
    "    x_avg = bk_vec_init.copy()\n",
    "    c_avg = c.copy()\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "    bk_series[0] = bk_vari\n",
    "\n",
    "    # Generate required noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "    \n",
    "    # Constant matrices used often (don't re-create every iteration)\n",
    "    identity_neu = np.eye(n_neu)\n",
    "    zeros_neu = np.zeros(n_neu)\n",
    "    newax = np.newaxis\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ## Compute neuronal activity at time t with background and matrices at t\n",
    "        # Input projection with M\n",
    "        c = mmat.dot(bkvec)\n",
    "        \n",
    "        # Fast iterations for cbar: quasi-static approximation, use quadprog\n",
    "        # to optimize 1/2*cbar^T L cbar - c^T cbar\n",
    "        # subject to cbar \\geq 0 at each element\n",
    "        # Solution without constraint: cbar = L^{+} c\n",
    "        try:\n",
    "            cbar = solve_qp(G=lmat, a=c, C=identity_neu, b=zeros_neu)[0]\n",
    "        except:\n",
    "            return tseries, bk_series, bkvec_series, m_series, l_series, cbar_series\n",
    "        \n",
    "        # Save neuronal activities at time t (we don't save interneurons n)\n",
    "        cbar_series[k] = cbar\n",
    "        \n",
    "        ## Update running averages of x, n, cbar up to time t\n",
    "        x_avg += dt * (bkvec - x_avg) / tau_avg\n",
    "        c_avg += dt * (c - c_avg) / tau_avg\n",
    "        \n",
    "        ## Update synaptic weights and background to time t+1 for next iteration\n",
    "        # Based on neuronal activities, inputs, and running averages at time t\n",
    "        # Learning rate at t\n",
    "        eta = learn_decay(t, learnrate0, decayrate)\n",
    "        # Synaptic plasticity: update mmat, lmat to k+1 based on cbar, n at k\n",
    "        mmat = mmat + 2.0*dt*eta*(cbar[:, newax].dot(bkvec[newax, :]) \n",
    "                                    - (c - c_avg)[:, newax].dot((bkvec - x_avg)[newax, :]))\n",
    "        lmat = lmat + dt*eta/tau_l*(cbar[:, newax].dot(cbar[newax, :]) - lmat)\n",
    "    \n",
    "        # check to see if M is close to degenerate\n",
    "        #if k%100==0:\n",
    "        #    lam, v = np.linalg.eig(lmat)\n",
    "        #    \n",
    "        #    for i in range(n_neu):\n",
    "        #        if lam[i]<1e-4:\n",
    "        #            print(f'iteration {t}: close to degenerate')\n",
    "        #            lam[i] = 1\n",
    "        #        \n",
    "        #    lmat = v@np.diag(lam)@v.T\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bk_vari, bkvec = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        \n",
    "        t += dt\n",
    "\n",
    "        # Save synaptic weights at time step k+1\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        bkvec_series[knext] = bkvec\n",
    "        bk_series[knext] = bk_vari\n",
    "    \n",
    "    # Compute final neuronal activities with latest matrices and inputs\n",
    "    c = mmat.dot(bkvec)\n",
    "    cbar = solve_qp(G=lmat, a=c, C=identity_neu, b=zeros_neu)[0]\n",
    "    cbar_series[-1] = cbar\n",
    "    \n",
    "    return tseries, bk_series, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the test conditions\n",
    "\n",
    "### Sources\n",
    "Each source is statistically independent. At every step, it is either set to zero with probability $\\frac12$, or sampled uniformly from the interval $(0, \\sqrt{48/5})$, with probability $\\frac12$. \n",
    "\n",
    "Two synthetic test cases were considered: 3 sources and 10 sources. \n",
    "\n",
    "\n",
    "### Mixing matrix\n",
    "Random square matrix with elements that are independent standard normal random variables.\n",
    "\n",
    "They report a specific test instance in the appendix, but it's unclear whether this is actually what they used, because their Github code is not seeded and the mixing matrix isn't printed anywhere. So I will just generate my own random matrix anyways. \n",
    "\n",
    "### Initial values\n",
    "Not specified in the paper, but based on the code on Github:\n",
    " - $M$ ($N_D \\times N_K$) is a diagonal $D$ of ones stopping after $N_K \\leq N_D$ columns, transformed by two random orthogonal matrices $R$ of appropriate dimensions: $M = R_{N_D} D R_{N_K}$. \n",
    " - $L$ ($N_I \\times N_K$) is a diagonal $D$ of ones stopping after $N_K \\leq N_I$ columns\n",
    "\n",
    "\n",
    "### Model parameters and rates\n",
    "Specified in the appendix of the paper and hardcoded in algorithm class definitions on the Github repo. \n",
    "The learning rate is made time-dependent:\n",
    "\n",
    "$$ \\eta(t) = \\frac{\\eta_0}{1 + \\delta t} $$\n",
    "\n",
    "For the 3-source case and the 10-source case, algorithm 1:\n",
    " - Learning rate $\\eta_0 = 0.01$\n",
    " - Decay rate: $\\delta = 0.001$\n",
    " \n",
    "For the 3-source case, algorithm 2:\n",
    " - Learning rate $\\eta_0 = 0.1$\n",
    " - Decay rate: $\\delta = 0.01$\n",
    " - $\\tau$ for $L$ dynamics: $\\tau = 0.8$\n",
    "\n",
    "For the 10-source case, algorithm 2:\n",
    " - Learning rate $\\eta_0 = 0.001$\n",
    " - Decay rate: $\\delta = 10^{-4}$\n",
    " - $\\tau$ for $L$ dynamics: $\\tau = 0.03$\n",
    " \n",
    "I will be using $\\tau_{avg} = 150$, as in the IBCM model. \n",
    "\n",
    "\n",
    "### Error metric\n",
    "Mean-squared error between the recovered sources, $\\vec{\\bar{c}}(t)$, and the original ones, $\\vec{s}(t)$, with the best possible permutation of sources being performed. Assumes $N_I = N_K$, the number of sources is known and equal to the number of neurons/recovered sources. \n",
    "\n",
    "$$ E(t) = \\frac{1}{t N_K} \\sum_{t'=1}^{t} \\|\\vec{s}(t) - P \\vec{\\bar{c}}(t)  \\|^2 $$\n",
    "\n",
    "where $P$ is a permutation matrix optimized over all times $t$ (i.e. to find which source corresponds to which recovered source) to minimize the difference with $\\vec{s}(t)$. \n",
    "\n",
    "The error is then computed as a function of time, as a moving exponential average, from the equation\n",
    "\n",
    "$$ dE/dt = \\frac{1}{1000}(E - err_t) $$\n",
    "\n",
    "where $err_t$ is the error contribution of time point $t$. \n",
    "\n",
    "For this function, I simply copy-pasted the code available on Github from the bio-nica repository, adding a few comments for my own understanding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate a multivariate sample from N(0, 1) samples (stdnorm_vec)\n",
    "def update_sources(bk_vari, bk_params, unif_vec, dt):\n",
    "    # dt is an argument for compatibility, not used\n",
    "    # smax should broadcast to the shape of bk_vari\n",
    "    # zeroprob is assumed to be a float: the same max conc. for all sources\n",
    "    mixmat, smax, zeroprob = bk_params\n",
    "    # For each source, choose whether it is zero or non-zero\n",
    "    # Reuse the uniform normal samples, conditioned on knowing they are in [zeroprob, 1.0)\n",
    "    # to select the concentration of non-zero sources. \n",
    "    if zeroprob < 1.0:\n",
    "        bk_vari = np.where(unif_vec < zeroprob, 0.0, (unif_vec - zeroprob) / (1.0 - zeroprob) * smax)\n",
    "    else:  # This would be strange, but anyways. \n",
    "        bk_vari = np.zeros(mixmat.shape[1])\n",
    "    return bk_vari, mixmat.dot(bk_vari)\n",
    "\n",
    "# Generate a random orthogonal matrix\n",
    "def random_orthogonal_mat(n, rng):\n",
    "    # Tested in test_biopca notebook: this does give orthogonal matrices\n",
    "    # Github code of bio-nica uses scipy.stats.ortho_group\n",
    "    q, r = np.linalg.qr(rng.standard_normal(size=[n, n]), mode=\"complete\")\n",
    "    return q.dot(np.diagflat(np.sign(np.diagonal(r))))\n",
    "\n",
    "def generate_mixing_mat(dim, rng):\n",
    "    return rng.standard_normal(size=(dim, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found in https://github.com/flatironinstitute/bio-nica/blob/master/util.py\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "def permutation_error(S_perm, Y):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ====================\n",
    "    S_perm   -- The data matrix of permuted sources\n",
    "    Y   -- The data matrix of recovered sources\n",
    "    \n",
    "    Output:\n",
    "    ====================\n",
    "    err -- the (relative) Frobenius norm error\n",
    "    \"\"\"\n",
    "    msg = (\"The shape of the permuted sources S_perm\"\n",
    "            + \"must equal the shape of the recovered sources Y\")\n",
    "\n",
    "    assert S_perm.shape==Y.shape, msg\n",
    "    s_dim = S_perm.shape[0]\n",
    "    iters = S_perm.shape[1]\n",
    "    \n",
    "    err = np.zeros(iters)\n",
    "    \n",
    "    # Determine the optimal permutation at the final time point.\n",
    "    # We solve the linear assignment problem using the linear_sum_assignment package\n",
    "    \n",
    "    # Calculate cost matrix:\n",
    "    C = np.zeros((s_dim, s_dim))\n",
    "    \n",
    "    # Compare every pair of recovered, original sources. \n",
    "    for i in range(s_dim):\n",
    "        for j in range(s_dim):\n",
    "            C[i,j] = ((S_perm[i] - Y[j])**2).sum()\n",
    "    \n",
    "    # Find the optimal assignment for the cost matrix C\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    \n",
    "    # Compute the error after each time point, moving exp. average over 1000 frames\n",
    "    for t in range(iters):\n",
    "\n",
    "        diff_t = (S_perm[row_ind[:],t] - Y[col_ind[:],t])**2\n",
    "        error_t = diff_t.sum()/s_dim\n",
    "        \n",
    "        if t==0:\n",
    "            err[t] = error_t\n",
    "        elif t>0:\n",
    "            # err[t] = err[t-1] + (error_t - err[t-1])/t\n",
    "            err[t] = err[t-1] + (error_t - err[t-1])/1000\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce tests cases\n",
    "Both algorithms, both dimensionalities, with the prescribed parameters. \n",
    "\n",
    "Report average and ci across 10 trials with the same mixing matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_onetest_twoalgos(init_m, init_l, update_fct, init_bk, nica_params1, nica_params2, back_params,  \n",
    "                    duration, dt, seeds,  noisetype, tst):\n",
    "    # Run simulation and compute error with algorithm 1\n",
    "    # back_params: contains mixmat, smax, probzero\n",
    "    res = integrate_bionica1(init_m, init_l, update_fct, init_bk, nica_params1, back_params,  \n",
    "                duration, dt, seed=seeds[0], noisetype=noisetype)\n",
    "    tser, bkser, bkvecser, mser, lser, cbarser = res\n",
    "    err_series1 = permutation_error(cbarser.T, bkser.T)  # Each column is a time point\n",
    "    \n",
    "    # Run simulation and compute error with algorithm 2\n",
    "    res = integrate_bionica2(init_m, init_l, update_fct, init_bk, nica_params2, back_params,  \n",
    "                duration, dt, seed=seeds[1], noisetype=noisetype)\n",
    "    tser, bkser, bkvecser, mser, lser, cbarser = res\n",
    "    \n",
    "    err_series2 = permutation_error(cbarser.T, bkser.T)\n",
    "    \n",
    "    print(\"Plotting\")\n",
    "    fig, axes = plt.subplots(3)\n",
    "    n_pts = tser.size\n",
    "    axes[0].plot(bkser[n_pts//2:, 1], bkser[n_pts//2:, 2], ls=\"none\", marker=\"o\", ms=2)\n",
    "    axes[1].plot(bkvecser[n_pts//2:, 1], bkvecser[n_pts//2:, 2], ls=\"none\", marker=\"o\", ms=2)\n",
    "    axes[2].plot(cbarser[n_pts//2:, 1], cbarser[n_pts//2:, 2], ls=\"none\", marker=\"o\", ms=2)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Finished plot\")\n",
    "    \n",
    "    # Check distribution of samples\n",
    "    #msg = \"Samples are not zero with probability {}\".format(back_params[-1])\n",
    "    #assert abs(np.count_nonzero(bkser)/bkser.size/(1-back_params[-1]) - 1.0) < 2.0 / np.sqrt(tser.size), msg\n",
    "    \n",
    "    print(\"Finished test ntst = {}\".format(tst))\n",
    "    \n",
    "    return tser, err_series1, err_series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallelized version of the tests\n",
    "# Run ntst tests of both algorithms (1 and 2) for a choice of N_D (n mix), N_K (n sources)\n",
    "# Keep the duration of each test to 1e4 by default, to make things quicker. \n",
    "def run_tests_nkchoice_parallel(ntst, nica_params1, nica_params2, mix_params, \n",
    "            duration=1e4, rng=None, njob=n_cpu):\n",
    "    # Extract a few parameters\n",
    "    #mix_mat, s_max, zero_prob = mix_params\n",
    "    #learnrate0, decayrate, tau_avg, tau_l = nica_params\n",
    "    nd = mix_params[0].shape[0]  # n_dim = N_D = number of mixtures\n",
    "    nk = mix_params[0].shape[1]  # n_sources = N_K = number of ICA neurons\n",
    "    nn = nk  # Use a number of interneurons equal to the number of sources n_k\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: diagonal of ones stopping after N_K columns\n",
    "    # and transformed by two random orthogonal matrices\n",
    "    init_m = (random_orthogonal_mat(nk, rng)\n",
    "              .dot(np.eye(nk, nd))\n",
    "              .dot(random_orthogonal_mat(nd, rng)))\n",
    "    # L: identity matrix\n",
    "    init_l1 = np.eye(nn, nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # To generate a new seed for each simulation\n",
    "    seed_sequence = np.random.SeedSequence(seed_from_gen(rng))\n",
    "    \n",
    "    # Pool of workers, to parallelize\n",
    "    pool = multiprocessing.Pool(njob)\n",
    "    res_objs = []\n",
    "    \n",
    "    for tst in range(ntst):\n",
    "        \n",
    "        # Initialize background: use update_sources\n",
    "        init_bk = list(update_sources(None, mix_params, rng.random(size=nk), dt))\n",
    "        \n",
    "        # Get new seeds\n",
    "        seeds_tst = seed_sequence.spawn(2)\n",
    "        \n",
    "        # Launch this test in parallel of others\n",
    "        res = pool.apply_async(run_onetest_twoalgos, \n",
    "                            args=(init_m, init_l1, update_sources, init_bk, nica_params1, nica_params2,\n",
    "                            mix_params, duration, dt, seeds_tst, \"uniform\", tst)\n",
    "                            )\n",
    "        res_objs.append(res)\n",
    "    \n",
    "    # Get the results\n",
    "    res_objs_finished = [a.get() for a in res_objs]\n",
    "    \n",
    "    # Don't forget to close the Pool! \n",
    "    pool.close()\n",
    "    \n",
    "    # Store results in appropriate containers\n",
    "    chosen_t = res_objs_finished[0][0]\n",
    "    err_algo1 = np.zeros([ntst, len(chosen_t)])\n",
    "    err_algo2 = np.zeros([ntst, len(chosen_t)])\n",
    "    for tst in range(ntst):\n",
    "        err_algo1[tst] = res_objs_finished[tst][1]\n",
    "        err_algo2[tst] = res_objs_finished[tst][2]\n",
    "        \n",
    "    return chosen_t, err_algo1, err_algo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test runs for small $N_D$, $N_K$\n",
    "$N_K = 3$, $N_D = 4$. \n",
    "\n",
    "Reminder of parameters:\n",
    "\n",
    "For the 3-source case and the 10-source case, algorithm 1:\n",
    " - Learning rate $\\eta_0 = 0.01$\n",
    " - Decay rate: $\\delta = 0.001$\n",
    " \n",
    "For the 3-source case, algorithm 2:\n",
    " - Learning rate $\\eta_0 = 0.1$\n",
    " - Decay rate: $\\delta = 0.01$\n",
    " - $\\tau$ for $L$ dynamics: $\\tau = 0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgen = np.random.default_rng(seed=0x508f6659e430c228a2bdeababc96b8de)\n",
    "\n",
    "# Dimensionalities\n",
    "n_n = 3  # input: number of mixtures\n",
    "n_k = 3  # output: number of sources\n",
    "\n",
    "mix_params_smallk = [\n",
    "    rgen.standard_normal(size=[n_n, n_k]),   # mix_mat\n",
    "    np.sqrt(48/5),                           # s_max\n",
    "    0.5                                      # zero_prob\n",
    "]\n",
    "\n",
    "#learnrate0, decayrate, tau_avg, tau_l = nica_params\n",
    "bionica_params1_smallk = [0.01, 0.001, 150.0]\n",
    "bionica_params2_smallk = [0.1, 0.01, 150.0, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run tests\n",
    "n_tests = 1\n",
    "duration = 1e5\n",
    "\n",
    "try:\n",
    "    res_file = np.load(\"outputs/test_bionica_k3_permutation_errors.npz\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Launching test...\")\n",
    "    #ntst, nica_params1, nica_params2, mix_params, \n",
    "            #duration=1e4, rng=None, njob=n_cpu\n",
    "    chosen_times, errors_algo1, errors_algo2 = run_tests_nkchoice_parallel(\n",
    "                    n_tests, bionica_params1_smallk, bionica_params2_smallk, \n",
    "                    mix_params_smallk, duration=duration, rng=rgen, njob=n_cpu)\n",
    "    # Write test results to disk so we don't have to run code every time\n",
    "    #np.savez(\"outputs/test_bionica_k3_permutation_errors.npz\", \n",
    "    #     chosen_times=chosen_times, \n",
    "    #     errors_algo1=errors_algo1, \n",
    "    #     errors_algo2=errors_algo2\n",
    "    #    )\n",
    "    print(\"New test results saved.\")\n",
    "else:\n",
    "    print(\"Managed to load existing test results.\")\n",
    "    chosen_times = res_file[\"chosen_times\"]\n",
    "    errors_algo1 = res_file[\"errors_algo1\"]\n",
    "    errors_algo2 = res_file[\"errors_algo2\"]\n",
    "    res_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the median alignment error as a function of iteration number\n",
    "mederr_algo1 = np.median(errors_algo1, axis=0)\n",
    "mederr_algo2 = np.median(errors_algo2, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "start_t = find_nearest_idx(chosen_times, 1e2)\n",
    "li1, = ax.plot(chosen_times[start_t:], mederr_algo1[start_t:], label=\"Bio-NICA 1\")\n",
    "li2, = ax.plot(chosen_times[start_t:], mederr_algo2[start_t:], label=\"Bio-NICA 2\", ls=\"--\")\n",
    "ax.set(xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", \n",
    "       yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/tests/test_ifpsp_n10_k3_align_subspace_error.pdf\", \n",
    "#           transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test runs for large N, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionalities\n",
    "n_n2 = 10  # input: mixtures\n",
    "n_k2 = 10  # output: sources\n",
    "\n",
    "mix_params_largek = [\n",
    "    rgen.standard_normal(size=[n_n2, n_k2]),   # mix_mat\n",
    "    np.sqrt(48/5),                           # s_max\n",
    "    0.5                                      # zero_prob\n",
    "]\n",
    "\n",
    "#learnrate0, decayrate, tau_avg, tau_l = nica_params\n",
    "bionica_params1_largek = [0.01, 0.001, 150.0]\n",
    "bionica_params2_largek = [0.001, 0.0001, 150.0, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "n_tests = 1\n",
    "duration = 1e5\n",
    "\n",
    "try:\n",
    "    res_file = np.load(\"outputs/test_bionica_k10_permutation_errors.npz\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Launching test...\")\n",
    "    #ntst, nica_params1, nica_params2, mix_params, \n",
    "            #duration=1e4, rng=None, njob=n_cpu\n",
    "    chosen_times2, errors_algo1_large, errors_algo2_large = run_tests_nkchoice_parallel(\n",
    "                    n_tests, bionica_params1_largek, bionica_params2_largek, \n",
    "                    mix_params_largek, duration=duration, rng=rgen, njob=n_cpu)\n",
    "    # Write test results to disk so we don't have to run code every time\n",
    "    #np.savez(\"outputs/test_bionica_k10_permutation_errors.npz\", \n",
    "    #     chosen_times=chosen_times, \n",
    "    #     errors_algo1=errors_algo1, \n",
    "    #     errors_algo2=errors_algo2\n",
    "    #    )\n",
    "    print(\"New test results saved.\")\n",
    "else:\n",
    "    print(\"Managed to load existing test results.\")\n",
    "    chosen_times2 = res_file[\"chosen_times\"]\n",
    "    errors_algo1_large = res_file[\"errors_algo1\"]\n",
    "    errors_algo2_large = res_file[\"errors_algo2\"]\n",
    "    res_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the median alignment error as a function of iteration number\n",
    "mederr_algo1_large = np.median(errors_algo1_large, axis=0)\n",
    "mederr_algo2_large = np.median(errors_algo2_large, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "li1, = ax.plot(chosen_times2, mederr_algo1_large, label=\"BioNICA 1\")\n",
    "li2, = ax.plot(chosen_times2, mederr_algo2_large, label=\"BioNICA 2\", ls=\"--\")\n",
    "ax.set(xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", \n",
    "       yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/tests/test_bionica_n100_k10_align_subspace_error.pdf\", \n",
    "#           transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
