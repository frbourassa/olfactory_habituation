{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test biologically plausible online PCA network\n",
    "On simple gaussian and non-gaussian inputs. \n",
    "\n",
    "## Test case studied in Minden et al., 2018\n",
    "\n",
    "Multivariate gaussian with random orthogonal transformation of a well-defined covariance matrix. $N$ dimensions, $K$ projections learnt. \n",
    "\n",
    "Two cases: $N=10$, $K=3$, and $N=100$, $K=10$\n",
    "\n",
    "Two algorithms: inverse-free principal subspace projection (ifPSP) and PSP with complete inverse calculation. \n",
    "\n",
    "With each algorithm, the projection matrix to compute based on the learnt weights is different: for ifPSP, must use the approximate inverse of $L$, while for PSP, the exact inverse is used (because it is also used in the algorithm). \n",
    "\n",
    "### I would need the original code to compare\n",
    "I can't reproduce the subspace alignment error results exactly. \n",
    "Not sure whether it comes from \n",
    "1. How I implemented the algorithm (but it seems not, because I get identical results with their Similarity Matching code from Giovannucci, Minden et al., 2018 and with my implementation of that algorithm. \n",
    "2. How I implemented the test input data itself, although intermediate checks seem OK. \n",
    "But these are small differences, overall, my implementation still gives the PCA decomposition of the input process, as it should, and is overall correct. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.insert(0, \"../\")\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"fork\")\n",
    "from psutil import cpu_count\n",
    "n_cpu = cpu_count(logical=False)\n",
    "\n",
    "from utils.statistics import principal_component_analysis, seed_from_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca_meankept(samp, do_proj=False, vari_thresh=1.0, force_svd=False):\n",
    "    \"\"\" Given an array of samples, compute the empirical covariance and\n",
    "    diagonalize it to obtain the principal components and principal values,\n",
    "    which are what is returned.\n",
    "\n",
    "    If less than 10*d samples, take SVD of the sample matrix directly\n",
    "    divided by 1/sqrt(N-1), because this amounts to eigendecomposition of\n",
    "    the covariance matrix, but with better numerical stability and accuracy\n",
    "    (but it's a lot slower).\n",
    "\n",
    "    Args:\n",
    "        samp (np.array): nxp matrix for n samples of p dimensions each.\n",
    "            Pass the values of a dataframe for proper slicing.\n",
    "        do_proj (bool): if True, also project the sample points\n",
    "        vari_thresh (float in [0., 1.]): include principal components until\n",
    "            a fraction vari_thresh of the total variance is explained.\n",
    "        force_svd (bool): if True, use SVD of the data matrix directly.\n",
    "    Returns:\n",
    "        p_values (np.ndarray): 1d array of principal values, descending order.\n",
    "        p_components (np.ndarray): 2d array of principal components.\n",
    "            p_components[:, i] is the vector for p_values[i]\n",
    "        samp_proj (np.ndarray): of shape (samp.shape[0], n_comp) where n_comp\n",
    "            is the number of principal components needed to explain\n",
    "            vari_thresh of the total variance.\n",
    "    \"\"\"\n",
    "    # Few samples: use SVD on the de-meaned data directly.\n",
    "    if force_svd or samp.shape[0] <= 10*samp.shape[1]:\n",
    "        svd_res = np.linalg.svd(samp.T / np.sqrt(samp.shape[0] - 1))\n",
    "        # U, Sigma, V. Better use transpose so small first dimension,\n",
    "        # because higher accuracy in eigenvectors in U\n",
    "        # Each column of U is an eigenvector of samp^T*samp/(N-1)\n",
    "        p_components = svd_res[0]\n",
    "        p_values = svd_res[1]**2  # Singular values are sqrt of eigenvalues\n",
    "\n",
    "    # Many samples are available; use covariance then eigen decomposition\n",
    "    else:\n",
    "        covmat = np.dot(samp.T, samp) / (samp.shape[0] - 1)\n",
    "        p_values, p_components = np.linalg.eigh(covmat)\n",
    "        # Sort in decreasing order; eigh returns increasing order\n",
    "        p_components = p_components[:, ::-1]\n",
    "        p_values = p_values[::-1]\n",
    "\n",
    "    if do_proj:\n",
    "        vari_explained = 0.\n",
    "        total_variance = np.sum(p_values)\n",
    "        n_comp = 0\n",
    "        while vari_explained < total_variance*vari_thresh:\n",
    "            vari_explained += p_values[n_comp]\n",
    "            n_comp += 1\n",
    "        samp_proj = samp.dot(p_components[:, :n_comp])\n",
    "\n",
    "    else:\n",
    "        samp_proj = None\n",
    "\n",
    "    return p_values, p_components, samp_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm implementation\n",
    "Embedded in an online simulation where random samples are successively presented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha(t) for small N case\n",
    "def alpha_t_smallN(t):\n",
    "    return 10 / (250 + t)\n",
    "\n",
    "# Alpha(t) for large N case\n",
    "def alpha_t_largeN(t):\n",
    "    return 1.1e-3 if t < 1e4 else 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version that uses the approximate inverse L\n",
    "def integrate_biopca_ifpsp(m_init, l_init, update_bk, bk_init, biopca_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"normal\"):\n",
    "    # Note: keep lambda matrix as 1d diagonal only, replace dot products by:\n",
    "    # Lambda.dot(A): Lambda_ii applied to row i, replace by Lambda_diag[:, None]*A element-wise\n",
    "    # A.dot(Lambda): Lambda_ii applied to column i, just A*Lambda broadcasts right\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    mrate, lrate, lambda_diag = biopca_params\n",
    "    # mrate is a callable: function of time. \n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of projections\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities (before applying L)\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    \n",
    "    # Inverse of diagonal of L is used a few times per iteration\n",
    "    # Indices to access diagonal and off-diagonal elements of L\n",
    "    # Will be used often, so prepare in advance. Replace dot product\n",
    "    # with diagonal matrix by element-wise products.\n",
    "    diag_idx = np.diag_indices(l_init.shape[0])\n",
    "    inv_l_diag = 1.0 / l_init[diag_idx]  # 1d flattened diagonal\n",
    "    # Use this difference the only time M_d is needed per iteration\n",
    "    # Faster to re-invert inv_l_diag than to slice lmat again\n",
    "    # l_offd = lmat - dflt(1.0 / inv_l_diag)\n",
    "    newax = np.newaxis\n",
    "    dflt = np.diagflat\n",
    "\n",
    "    # Initialize neuron activity with m and background at time zero\n",
    "    c = inv_l_diag*(mmat.dot(bkvec))  # L_d^(-1) M^T x_0\n",
    "    # Lateral inhibition between neurons\n",
    "    cbar = c - inv_l_diag*np.dot(lmat-dflt(1.0 / inv_l_diag), c)\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    cbar_series[0] = cbar\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "\n",
    "    # Generate N(0, 1) noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ### Online PCA weights\n",
    "        # Synaptic plasticity: update mmat, lmat to k+1 based on cbar at k\n",
    "        # Adding new length-1 axes is faster than np.outer for c.x^T, c.c^T\n",
    "        mmat = mmat + dt * mrate(t) * (cbar[:, newax].dot(bkvec[newax, :]) - mmat)\n",
    "        lmat = lmat + dt * mrate(t) * lrate * (cbar[:, newax].dot(cbar[newax, :])\n",
    "                        - lambda_diag[:, newax] * lmat * lambda_diag)\n",
    "        # Update too the variable saving the inverse of the diagonal of L\n",
    "        inv_l_diag = 1.0 / lmat[diag_idx]\n",
    "        \n",
    "        t += dt\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "\n",
    "        # Neural dynamics (two-step) at time k+1, to be used in next step\n",
    "        # Not using Lambda[:, newax] trick here: this is a product with a vector M.x\n",
    "        # so the result has to be a vector too. \n",
    "        c = inv_l_diag*(mmat.dot(bkvec))  # L_d^(-1) M^T x_0\n",
    "        # Lateral inhibition between neurons\n",
    "        cbar = c - inv_l_diag*np.dot(lmat - dflt(1.0/inv_l_diag), c)\n",
    "\n",
    "        # Save current state\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        bkvec_series[knext] = bkvec\n",
    "        cbar_series[knext] = cbar  # Save activity of neurons at time k+1\n",
    "\n",
    "    return tseries, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version that numerically inverts matrix L: online PSP\n",
    "def integrate_biopca_psp(m_init, l_init, update_bk, bk_init, biopca_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"normal\"):\n",
    "    # Note: keep lambda matrix as 1d diagonal only, replace dot products by:\n",
    "    # Lambda.dot(A): Lambda_ii applied to row i, replace by Lambda_diag[:, None]*A element-wise\n",
    "    # A.dot(Lambda): Lambda_ii applied to column i, just A*Lambda broadcasts right\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    mrate, lrate, lambda_diag = biopca_params\n",
    "    # mrate is a callable: function of time. \n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    cbar_series = np.zeros([tseries.shape[0], n_neu])  # series of projections\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    c = np.zeros(n_neu)  # un-inhibited neuron activities (before applying L)\n",
    "    cbar = np.zeros(n_neu)  # inhibited neuron activities (after applying L)\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    linv = np.linalg.inv(lmat)\n",
    "    newax = np.newaxis\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "\n",
    "    # Generate N(0, 1) noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ### Online PCA weights\n",
    "        # Neural dynamics (two-step) at time k, to be used in this\n",
    "        cbar = linv.dot(mmat).dot(bkvec)\n",
    "        \n",
    "        # Synaptic plasticity: update mmat, lmat to k+1 based on cbar at k\n",
    "        mmat = mmat + dt * mrate(t) * (cbar[:, newax].dot(bkvec[newax, :]) - mmat)\n",
    "        lmat = lmat + dt * mrate(t) * lrate * (cbar[:, newax].dot(cbar[newax, :])\n",
    "                        - lambda_diag[:, newax] * lmat * lambda_diag)\n",
    "        # Update too the variable saving the inverse of the diagonal of L\n",
    "        linv = np.linalg.inv(lmat)\n",
    "\n",
    "\n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k], dt) \n",
    "        t += dt\n",
    "        \n",
    "        # Save current state\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        bkvec_series[knext] = bkvec\n",
    "        cbar_series[k] = cbar  # Save activity of neurons at time k+1\n",
    "    \n",
    "    cbar = linv.dot(mmat).dot(bkvec)\n",
    "    cbar_series[-1] = cbar\n",
    "    return tseries, bkvec_series, m_series, l_series, cbar_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input process\n",
    "\n",
    "Following Minden et al., 2018, each sample $\\vec{x}_t$ is drawn from a normal distribution with zero mean and covariance matrix\n",
    "\n",
    "$$ G = R \\tilde{G} R^T $$\n",
    "\n",
    "where $\\tilde{G}$ is diagonal and $R$ is a random orthogonal matrix, uniform in the Haar measure. Not sure what that means, but anyways, it is generated by taking the QR decomposition of a random matrix with standard normal elements. See https://nhigham.com/2020/04/22/what-is-a-random-orthogonal-matrix/. \n",
    "\n",
    "To match how background samples are generated in my code, I use the Cholesky decomposition trick: a vector $\\vec{x}$ with covariance matrix G can be generated from standard normal samples in $\\vec{u}$ by taking $\\vec{x} = \\Psi \\vec{u}$, where $G = \\Psi \\Psi^T$ for some $\\Psi$. So, the update of the background simply consists in taking pre-generated normal samples and applying $\\Psi$ on them. \n",
    "\n",
    "Now, note that we can get something equivalent to $\\Psi$ (although not lower triangular) by defining $\\psi \\psi^T = \\tilde{G}$, such that $G = (R \\psi)(R \\psi)^T$ and we can take $\\Psi = R \\psi$. Since $\\tilde{G}$ is diagonal, $\\psi$ is just a diagonal matrix with standard deviations instead of variances on the diagonal. So, we get an acceptable $\\Psi = R \\tilde{G}^{1/2}$ directly from our random $R$ and from the standard deviations on the diagonal of $\\tilde{G}^{1/2}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate a multivariate sample from N(0, 1) samples (stdnorm_vec)\n",
    "def update_mvnormal(bk_vari, bk_params, stdnorm_vec, dt):\n",
    "    # bk_vari and dt are arguments for general compatibility: not used. \n",
    "    psi = bk_params[0]  # Only parameter is the Cholesky of the covariance! \n",
    "    return psi.dot(stdnorm_vec), stdnorm_vec\n",
    "\n",
    "# Generate a random orthogonal matrix\n",
    "def random_orthogonal_mat(n, rng):\n",
    "    # Scipy or my version both give similar test results\n",
    "    #return sp.stats.ortho_group.rvs(dim=n, size=1, random_state=rng)\n",
    "    q, r = np.linalg.qr(rng.standard_normal(size=[n, n]), mode=\"complete\")\n",
    "    return q.dot(np.diagflat(np.sign(np.diagonal(r))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether our trick with $\\Psi$ generates\n",
    "# samples with the right distribution\n",
    "# and that we really generate a random orthogonal $R$. \n",
    "def test_construct(std, r, psi, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    # Check r is an orthogonal matrix: R^T R is the identity matrix\n",
    "    n = std.shape[0]\n",
    "    assert np.allclose(r.T.dot(r), np.eye(n)), \"\\n\" + str(r)\n",
    "    \n",
    "    # Check that the covariance matrix constructed from psi \n",
    "    # and the intended matrix are the same\n",
    "    cov_mat = psi.dot(psi.T)\n",
    "    cov_mat_target = r.dot(std**2).dot(r.T)\n",
    "    assert np.allclose(cov_mat, cov_mat_target), str(cov_mat)\n",
    "    \n",
    "    # Quick check: random samples have the desired covariance matrix G\n",
    "    # Each column is a sample\n",
    "    n_test_samp = int(1e6)\n",
    "    test_samples = psi.dot(rng.standard_normal(size=[n, n_test_samp]))\n",
    "    \n",
    "    # Relative tolerance of 1/sqrt(n_samples)\n",
    "    test_cov_mat = test_samples.dot(test_samples.T) / (n_test_samp - 1)\n",
    "    \n",
    "    assert np.allclose(cov_mat, test_cov_mat, \n",
    "                atol=4.0/np.sqrt(n_test_samp)), str(test_cov_mat) + \"\\n\" + str(cov_mat)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the output metric\n",
    "Frobenius norm: sum of squared norm of all columns:\n",
    "\n",
    "$$ ||A||^2 = \\mathrm{Tr}\\left[ A^T A \\right] = \\sum_{j} \\vec{a}^T_j \\vec{a}_j $$\n",
    "\n",
    "where $\\vec{a}_j$ is the $j$th column of matrix $A$. \n",
    "\n",
    "Subspace alignment error between $A$ (matrix) and $B$ (target): \n",
    "$$ E_B(A) = \\min_Q \\frac{||QA - B||^2}{||B||^2} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mat):\n",
    "    \"\"\" Compute Frobenius norm of matrix A, \n",
    "    ||A||^2 = Tr(A^T A). \"\"\"\n",
    "    return np.trace(mat.T.dot(mat))\n",
    "\n",
    "def subspace_align_error(mat, target):\n",
    "    \"\"\" Compute min_Q ||Q.dot(mat) - target||^2 / ||target||^2. \n",
    "    The solution to that orthogonal Procrustes problem is \n",
    "        Q = U V^T, where USV^T is the SVD of target.dot(mat.T)\n",
    "    according to Wikipedia, citing the solution of Schönemann, 1966. \n",
    "    (https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem)\n",
    "    \"\"\"\n",
    "    # Solve Procrustes problem\n",
    "    u, s, vh = np.linalg.svd(target.dot(mat.T))\n",
    "    q = u.dot(vh)\n",
    "    # Compute alignment error\n",
    "    return frobnorm(q.dot(mat) - target) / frobnorm(target)\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the subspace alignment error metric. \n",
    "def test_error_metric(n, rng):\n",
    "    for i in range(10):\n",
    "        # Error should be zero when comparing matrices\n",
    "        # that differ by an orthogonal matrix\n",
    "        a = rng.standard_normal(size=[n,  n])\n",
    "        rmat = random_orthogonal_mat(n, rng)\n",
    "        b = rmat.dot(a)\n",
    "        assert subspace_align_error(a, b) <= 1e-14\n",
    "        # Change a column of a, check that the error between a and aprime\n",
    "        # is the same whether rmat is applied or not, i.e. check that\n",
    "        # we do optimize over Q properly in the subspace error. \n",
    "        aprime = a.copy()\n",
    "        aprime[:, -1] = rng.standard_normal(size=n)\n",
    "        errdiff = abs(subspace_align_error(a, aprime) \n",
    "                      - subspace_align_error(a, rmat.dot(aprime)))\n",
    "        assert errdiff <= 1e-14\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given simulation results, compute the subspace alignment error after\n",
    "# various numbers of iterations (at npts log-spaced iteration numbers)\n",
    "def compute_error_series(ts, bkvecs, ms, ls, lamb_mat, inv_kind=\"exact\", npts=21):\n",
    "    \"\"\" inv_kind: either 'exact' or 'taylor', for PSP or ifPSP, respectively. \"\"\"\n",
    "    # Compute offline PCA solution\n",
    "    nk = ls.shape[1]\n",
    "    pvals, uk_offline, _ = compute_pca_meankept(bkvecs, do_proj=False)\n",
    "    uk_offline = uk_offline[:, :nk]\n",
    "\n",
    "    # Compute online PCA solution at a few different times\n",
    "    chosen_times = np.logspace(2.0, np.log10(ms.shape[0]), npts)\n",
    "    chosen_indices = np.asarray([find_nearest_idx(ts, t) for t in chosen_times])\n",
    "\n",
    "    m_series_sub = ms[chosen_indices]\n",
    "    l_series_sub = ls[chosen_indices]\n",
    "    # See what happens with random matrices\n",
    "    #m_series_sub = np.random.standard_normal(size=m_series_sub.shape)\n",
    "    #l_series_sub = np.random.standard_normal(size=l_series_sub.shape)\n",
    "    if inv_kind == \"exact\": \n",
    "        linv_series_sub = np.linalg.inv(l_series_sub)\n",
    "    \n",
    "    elif inv_kind == \"taylor\":\n",
    "        linv_series_sub = np.zeros(l_series_sub.shape)\n",
    "        for i in range(npts):\n",
    "            ldinv = 1.0 / np.diagonal(l_series_sub[i])\n",
    "            ldoff = l_series_sub[i] - np.diagflat(1.0 / ldinv)\n",
    "            linv_series_sub[i] = ldinv[:, None] * (np.eye(nk) - ldoff*ldinv[None, :])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown inverse kind: {}\".format(inv_kind))\n",
    "\n",
    "    # Compute the uk projector from M and L at each of those times\n",
    "    # and the error compared to uk_offline\n",
    "    uk_online_sub = np.zeros([len(chosen_indices)] + list(uk_offline.shape))\n",
    "    pro_error = []\n",
    "    for i in range(npts):\n",
    "        uk_online_sub[i] = (1.0/lamb_mat[:, None] * linv_series_sub[i].dot(m_series_sub[i])).T\n",
    "        pro_error.append(subspace_align_error(uk_online_sub[i], uk_offline))\n",
    "    # Dot product of u_K^T u_K should be identity matrix\n",
    "    # Check that the unscaled UK projector F = L^{-1} M has norms FF^T = Lambda^2, \n",
    "    # So that U_K itself has norm 1\n",
    "    #print(uk_online_sub[-1].T.dot(uk_online_sub[-1]))\n",
    "    return chosen_times, pro_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce test cases\n",
    "Both algorithms, both dimensionalities, with the prescribed parameters. \n",
    "\n",
    "Report medians across 100 trials of randomly generated $R$ matrices -- will take some time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_onetest_twoalgos(init_m, init_l, update_fct, init_bk, biopca_params, back_params,  \n",
    "                    duration, dt, seeds, psi_mat, noisetype, npts, tst):\n",
    "    # Run simulation and compute error with ifPSP\n",
    "    lambd = biopca_params[-1]\n",
    "    res = integrate_biopca_ifpsp(init_m, init_l, update_fct, init_bk, biopca_params, back_params,  \n",
    "                duration, dt, seed=seeds[0], noisetype=noisetype)\n",
    "    tser, bkvecser, mser, lser, cbarser = res\n",
    "    chosen_t, align_ifpsp = compute_error_series(tser, bkvecser, mser, lser, lambd, \n",
    "                                                  inv_kind=\"taylor\", npts=npts)\n",
    "    #print(\"ifPSP:\\n\", lser[-1])\n",
    "    # Run simulation and compute error with PSP\n",
    "    res = integrate_biopca_psp(init_m, init_l, update_fct, init_bk, biopca_params, back_params,  \n",
    "                duration, dt, seed=seeds[1], noisetype=noisetype)\n",
    "    tser, bkvecser, mser, lser, cbarser = res\n",
    "    chosen_t, align_psp = compute_error_series(tser, bkvecser, mser, lser, lambd, \n",
    "                                                  inv_kind=\"exact\", npts=npts)     \n",
    "    #print(\"PSP:\\n\", lser[-1])\n",
    "    \n",
    "    # Check covariance matrix of background samples, make sure the input is what we think\n",
    "    #cov_mat_samples = np.dot(bkvecser.T, bkvecser) / (tser.size - 1)\n",
    "    #errmsg = \"Problem with the empirical covariance of the generated samples\"\n",
    "    #assert np.allclose(cov_mat_samples, psi_mat.dot(psi_mat.T), atol=4e-2), errmsg\n",
    "    \n",
    "    print(\"Finished test ntst = {}\".format(tst))\n",
    "    \n",
    "    return chosen_t, align_ifpsp, align_psp\n",
    "    \n",
    "    \n",
    "\n",
    "# Run ntst tests of both algorithms (PSP and ifPSP) for a choice of N, K\n",
    "# Keep the duration of each test to 1e4 by default, to make things quicker. \n",
    "def run_tests_nkchoice(ntst, nn, nk, gtilde, lambd, alphat, duration=1e4, tau=0.5, npts=21, rng=None):\n",
    "    # Check a few things\n",
    "    assert gtilde.shape[0] == nn\n",
    "    assert lambd.shape[0] == nk\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: normally-distributed, mean 0 and variance 1/N\n",
    "    init_m = rng.standard_normal(size=[nk, nn]) / np.sqrt(nn)\n",
    "    # L: identity matrix\n",
    "    init_l = np.eye(nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # Algorithm parameters\n",
    "    biopca_params = [alphat, 1.0/tau, lambd]\n",
    "    \n",
    "    # Containers for the error as a function of the number\n",
    "    # of iterations, for each test run. \n",
    "    err_psp = np.zeros([ntst, npts])\n",
    "    err_ifpsp = np.zeros([ntst, npts])\n",
    "    \n",
    "    # To generate a new seed for each simulation\n",
    "    seed_sequence = np.random.SeedSequence(seed_from_gen(rng))\n",
    "    \n",
    "    for tst in range(ntst):\n",
    "        # Generate a random orthogonal matrix R for this test run\n",
    "        r_mat = random_orthogonal_mat(nn, rng)\n",
    "        \n",
    "        # Square root decomposition of the actual covariance matrix\n",
    "        psi_mat = r_mat.dot(gtilde)\n",
    "        if not test_construct(gtilde, r_mat, psi_mat, rng=rng):\n",
    "            print(r\"Problem with the $\\Psi$ matrix trick!!!\")\n",
    "\n",
    "        # Background parameters with current psi_mat\n",
    "        init_bk = [np.zeros(nn, dtype=bool), psi_mat.dot(rng.standard_normal(size=nn))]\n",
    "        back_params = [psi_mat]\n",
    "        \n",
    "        # Get new seeds\n",
    "        seeds_tst = seed_sequence.spawn(2)\n",
    "        \n",
    "        res = run_onetest_twoalgos(init_m, init_l, update_mvnormal, init_bk, biopca_params,\n",
    "                        back_params, duration, dt, seeds_tst, psi_mat, \"normal\", npts, tst)\n",
    "        chosen_t, err_ifpsp[tst], err_psp[tst] = res\n",
    "        \n",
    "    return chosen_t, err_ifpsp, err_psp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallelized version of the tests\n",
    "# Run ntst tests of both algorithms (PSP and ifPSP) for a choice of N, K\n",
    "# Keep the duration of each test to 1e4 by default, to make things quicker. \n",
    "def run_tests_nkchoice_parallel(ntst, nn, nk, gtilde, lambd, alphat, \n",
    "            duration=1e4, tau=0.5, npts=21, rng=None, njob=n_cpu):\n",
    "    # Check a few things\n",
    "    assert gtilde.shape[0] == nn\n",
    "    assert lambd.shape[0] == nk\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: normally-distributed, mean 0 and variance 1/N\n",
    "    init_m = rng.standard_normal(size=[nk, nn]) / np.sqrt(nn)\n",
    "    # L: identity matrix\n",
    "    init_l = np.eye(nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # Algorithm parameters\n",
    "    biopca_params = [alphat, 1.0/tau, lambd]\n",
    "    \n",
    "    # Containers for the error as a function of the number\n",
    "    # of iterations, for each test run. \n",
    "    err_ifpsp = np.zeros([ntst, npts])\n",
    "    err_psp = np.zeros([ntst, npts])\n",
    "    \n",
    "    # To generate a new seed for each simulation\n",
    "    seed_sequence = np.random.SeedSequence(seed_from_gen(rng))\n",
    "    \n",
    "    # Pool of workers, to parallelize\n",
    "    pool = multiprocessing.Pool(njob)\n",
    "    res_objs = []\n",
    "    \n",
    "    for tst in range(ntst):\n",
    "        # Generate a random orthogonal matrix R for this test run\n",
    "        r_mat = random_orthogonal_mat(nn, rng)\n",
    "        \n",
    "        # Square root decomposition of the actual covariance matrix\n",
    "        psi_mat = r_mat.dot(gtilde)\n",
    "        if not test_construct(gtilde, r_mat, psi_mat, rng=rng):\n",
    "            print(r\"Problem with the $\\Psi$ matrix trick!!!\")\n",
    "\n",
    "        # Background parameters with current psi_mat\n",
    "        init_bk = [np.zeros(nn, dtype=bool), psi_mat.dot(rng.standard_normal(size=nn))]\n",
    "        back_params = [psi_mat]\n",
    "        \n",
    "        # Get new seeds\n",
    "        seeds_tst = seed_sequence.spawn(2)\n",
    "        \n",
    "        # Launch this test in parallel of others\n",
    "        res = pool.apply_async(run_onetest_twoalgos, \n",
    "                            args=(init_m, init_l, update_mvnormal, init_bk, biopca_params,\n",
    "                            back_params, duration, dt, seeds_tst, psi_mat, \"normal\", npts, tst)\n",
    "                            )\n",
    "        res_objs.append(res)\n",
    "    \n",
    "    # Get the results\n",
    "    res_objs_finished = [a.get() for a in res_objs]\n",
    "    \n",
    "    # Don't forget to close the Pool! \n",
    "    pool.close()\n",
    "    \n",
    "    # Store results in appropriate containers\n",
    "    chosen_t = res_objs_finished[0][0]\n",
    "    for tst in range(ntst):\n",
    "        err_ifpsp[tst] = res_objs_finished[tst][1]\n",
    "        err_psp[tst] = res_objs_finished[tst][2]\n",
    "        \n",
    "    return chosen_t, err_ifpsp, err_psp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test runs for small N, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgen = np.random.default_rng(seed=0x88977399c6600332f6b98129df9126c1)\n",
    "\n",
    "# Dimensionalities\n",
    "n_n = 10  # input\n",
    "n_k = 3  # output\n",
    "\n",
    "# Check alignment subspace metric\n",
    "if test_error_metric(n_n, rgen):\n",
    "    print(\"Subspace alignment metric seems to work\")\n",
    "\n",
    "# Standard deviation matrix diagonalized\n",
    "gtilde_std = np.sqrt(np.diagflat([1.0, 0.75, 0.5] + [0.2]*(n_n - 3)))\n",
    "\n",
    "# Lambda matrix\n",
    "lambd_mat = np.asarray([1, 0.85, 0.7])\n",
    "\n",
    "tau_const = 0.5  # Use value reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run tests\n",
    "n_tests = 50\n",
    "n_checkpts = 11\n",
    "duration = 1e5\n",
    "chosen_times, errors_ifpsp, errors_psp = run_tests_nkchoice_parallel(\n",
    "            n_tests, n_n, n_k, gtilde_std, lambd_mat, alpha_t_smallN, \n",
    "            duration=duration, tau=tau_const, npts=n_checkpts, rng=rgen, njob=n_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the median alignment error as a function of iteration number\n",
    "mederr_ifpsp = np.median(errors_ifpsp, axis=0)\n",
    "mederr_psp = np.median(errors_psp, axis=0)\n",
    "maxerr_ifpsp, minerr_ifpsp = np.quantile(errors_ifpsp, q=[0.95, 0.1], axis=0)\n",
    "maxerr_psp, minerr_psp = np.quantile(errors_psp, q=[0.95, 0.1], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "li1, = ax.plot(chosen_times, mederr_ifpsp, label=\"ifPSP\")\n",
    "ax.fill_between(chosen_times, minerr_ifpsp, maxerr_ifpsp, color=li1.get_color(), \n",
    "                alpha=0.3, label=\"ifPSP CI 90 %\")\n",
    "li2, = ax.plot(chosen_times, mederr_psp, label=\"PSP\", ls=\"--\")\n",
    "ax.fill_between(chosen_times, minerr_psp, maxerr_psp, color=li2.get_color(), \n",
    "                alpha=0.3, label=\"PSP CI 90 %\")\n",
    "ax.set(xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", \n",
    "       yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "# Add reported values\n",
    "reported_errs_t = [1e3, 1e4, 1e5]\n",
    "ax.plot(reported_errs_t, [2.1e-2, 1.5e-4, 1.7e-5], label=\"ifPSP (reported)\", \n",
    "        marker=\"o\", ls=\"None\", ms=10, color=li1.get_color())\n",
    "ax.plot(reported_errs_t, [1.9e-2, 4.1e-4, 5.5e-5], label=\"PSP (reported)\", \n",
    "        marker=\"s\", ls=\"None\", ms=10, color=li2.get_color())\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/tests/test_ifpsp_n10_k3_align_subspace_error.pdf\", \n",
    "#           transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test runs for large N, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionalities\n",
    "n_n2 = 100  # input\n",
    "n_k2 = 10  # output\n",
    "\n",
    "# Check alignment subspace metric\n",
    "if test_error_metric(n_n2, rgen):\n",
    "    print(\"Subspace alignment metric seems to work\")\n",
    "\n",
    "# Standard deviation matrix diagonalized\n",
    "gtilde_std2 = np.sqrt(np.diagflat([1 - k/(2*(n_k2-1)) for k in range(10)] + [0.02]*(n_n2 - 10)))\n",
    "\n",
    "# Lambda matrix\n",
    "lambd_mat2 = np.asarray([1 - 3*k / (10*(n_k2-1)) for k in range(n_k2)])\n",
    "\n",
    "tau_const2 = 0.5  # Use value reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run tests\n",
    "n_tests = 50\n",
    "n_checkpts = 11\n",
    "duration = 1e5\n",
    "\n",
    "chosen_times2, errors_ifpsp2, errors_psp2 = run_tests_nkchoice_parallel(\n",
    "        n_tests, n_n2, n_k2, gtilde_std2, lambd_mat2, alpha_t_largeN, \n",
    "        duration=duration, tau=tau_const, npts=n_checkpts, rng=rgen, njob=n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the median alignment error as a function of iteration number\n",
    "mederr_ifpsp2 = np.median(errors_ifpsp2, axis=0)\n",
    "mederr_psp2 = np.median(errors_psp2, axis=0)\n",
    "maxerr_ifpsp2, minerr_ifpsp2 = np.quantile(errors_ifpsp2, q=[0.95, 0.1], axis=0)\n",
    "maxerr_psp2, minerr_psp2 = np.quantile(errors_psp2, q=[0.95, 0.1], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "li1, = ax.plot(chosen_times2, mederr_ifpsp2, label=\"ifPSP\")\n",
    "ax.fill_between(chosen_times2, minerr_ifpsp2, maxerr_ifpsp2, color=li1.get_color(), \n",
    "                alpha=0.3, label=\"ifPSP CI 90 %\")\n",
    "li2, = ax.plot(chosen_times2, mederr_psp2, label=\"PSP\", ls=\"--\")\n",
    "ax.fill_between(chosen_times2, minerr_psp2, maxerr_psp2, color=li2.get_color(), \n",
    "                alpha=0.3, label=\"PSP CI 90 %\")\n",
    "ax.set(xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", \n",
    "       yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "# Add reported values\n",
    "reported_errs_t = [1e3, 1e4, 1e5]\n",
    "ax.plot(reported_errs_t, [1.0, 3.1e-3, 5.4e-4], label=\"ifPSP (reported)\", \n",
    "        marker=\"o\", ls=\"None\", ms=10, color=li1.get_color())\n",
    "ax.plot(reported_errs_t, [1.3, 1.5e-3, 1.4e-4], label=\"PSP (reported)\", \n",
    "        marker=\"s\", ls=\"None\", ms=10, color=li2.get_color())\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/tests/test_ifpsp_n100_k10_align_subspace_error.pdf\", \n",
    "#           transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one trial and visualize learnt components\n",
    "Even if we are not reproducing the exact error metrics reported in the paper, are we at least capturing the right components?\n",
    "\n",
    "Try a simple case with $N=2$, $K=2$ so that the full data can be plotted and the PCs overlaid on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_details_ifpsp(nn, nk, gtilde, lambd, alphat, duration=1e4, tau=0.5, rng=None):\n",
    "    # Assuming gtilde is diagonal\n",
    "    # Check a few things\n",
    "    assert gtilde.shape[0] == nn\n",
    "    assert lambd.shape[0] == nk\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: normally-distributed, mean 0 and variance 1/N\n",
    "    init_m = rng.standard_normal(size=[nk, nn]) / np.sqrt(nn)\n",
    "    # L: identity matrix\n",
    "    init_l = np.eye(nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # Rotation matrix transforming gtilde\n",
    "    r_mat = random_orthogonal_mat(nn, rng)\n",
    "    # Square root decomposition of the actual covariance matrix\n",
    "    psi_mat = r_mat.dot(gtilde)\n",
    "    if not test_construct(gtilde, r_mat, psi_mat, rng=rng):\n",
    "        print(r\"Problem with the $\\Psi$ matrix trick!!!\") \n",
    "    \n",
    "    # Exact PCA: eigenvalue decomposition of psi psi^T, \n",
    "    # since we know the eigenvalues on the diagonal of G\n",
    "    # the eigenvectors are columns in the r_mat\n",
    "    #eigvals, eigvecs = np.linalg.eigh(psi_mat.dot(psi_mat.T))\n",
    "    eigvecs = r_mat\n",
    "    eigvals = np.diagonal(gtilde**2)\n",
    "        \n",
    "    # Algorithm parameters\n",
    "    biopca_params = [alphat, 1.0/tau, lambd]\n",
    "\n",
    "    # Background parameters with current psi_mat\n",
    "    init_bk = [np.zeros(nn, dtype=bool), psi_mat.dot(rng.standard_normal(size=nn))]\n",
    "    back_params = [psi_mat]\n",
    "    \n",
    "    # Run simulation and compute error with ifPSP\n",
    "    res = integrate_biopca_ifpsp(init_m, init_l, update_mvnormal, init_bk, biopca_params, back_params,  \n",
    "                duration, dt, seed=seed_from_gen(rng), noisetype=\"normal\")\n",
    "    tser, bkvecser, mser, lser, cbarser = res\n",
    "    \n",
    "    # Determine basis learnt by algorithm and return\n",
    "    ldiag_inv = 1.0 / np.diagonal(lser[-1])\n",
    "    ldoff = lser[-1] - np.diagflat(1.0 / ldiag_inv)\n",
    "    linv = ldiag_inv[:, None] * (np.eye(nk) - ldoff*ldiag_inv[None, :])\n",
    "    learntvecs = (1.0/lambd[:, None] * linv.dot(mser[-1])).T\n",
    "    # Each column of learntvecs is an eigenvector learnt. \n",
    "    \n",
    "    # Values on the diagonal of L are supposed to be eigenvalues\n",
    "    learntvals = 1.0 / ldiag_inv\n",
    "    \n",
    "    # Compute\n",
    "    \n",
    "    return bkvecser, [eigvals, eigvecs], [learntvals, learntvecs], [tser, mser, lser, cbarser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionalities\n",
    "rgen_0 = np.random.default_rng(seed=0x2977c572e42f79c4c4b3f60de4687e08)\n",
    "n_n0 = 3  # input\n",
    "n_k0 = 2  # output\n",
    "\n",
    "# Check alignment subspace metric\n",
    "if test_error_metric(n_n0, rgen):\n",
    "    print(\"Subspace alignment metric seems to work\")\n",
    "\n",
    "# Standard deviation matrix diagonalized\n",
    "gtilde_std0 = np.sqrt(np.diagflat([1.0, 0.75, 0.5]))\n",
    "\n",
    "# Lambda matrix\n",
    "lambd_mat0 = np.asarray([1, 0.7])\n",
    "\n",
    "tau_const = 0.5  # Use value reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk_samples, true_pca, learnt_pca, learnser = run_details_ifpsp(n_n0, n_k0, gtilde_std0, lambd_mat0, \n",
    "                                                    alpha_t_smallN, duration=1e5, tau=tau_const, rng=rgen_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learnser[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(bk_samples[::10, 0], bk_samples[::10, 1], bk_samples[::10, 2], \n",
    "           color=\"grey\", s=1, alpha=0.5, zorder=1)\n",
    "scale = max(ax.get_xlim()[1], ax.get_ylim()[1], ax.get_zlim()[1])\n",
    "for i in range(n_k0):\n",
    "    ax.text(x=true_pca[1][0, i]*scale, y=true_pca[1][1, i]*scale, z=true_pca[1][2, i]*scale, \n",
    "            s=\"{:.2f}\".format(true_pca[0][i]), fontsize=8, zorder=6+4*i, va=\"top\")\n",
    "    ax.quiver(0, 0, 0, *(true_pca[1][:, i]*scale), color=\"k\", zorder=7+4*i)\n",
    "    ax.text(x=learnt_pca[1][0, i]*scale, y=learnt_pca[1][1, i]*scale, z=learnt_pca[1][2, i]*scale, \n",
    "            s=\"{:.2f}\".format(learnt_pca[0][i]), fontsize=8, ha=\"right\", va=\"top\", zorder=8+4*i)\n",
    "    ax.quiver(0, 0, 0, *(scale*learnt_pca[1][:, i]), color=\"b\", zorder=9+4*i)\n",
    "ax.view_init(elev=50, azim=340)\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n",
    "#fig.savefig(\"../figures/tests/online_pca_ifpsp_learnt_vectors_3d_example.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_t, align_psp = compute_error_series(learnser[0], bk_samples, learnser[1], learnser[2], lambd_mat0, \n",
    "                                                  inv_kind=\"taylor\", npts=201)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(chosen_t, align_psp)\n",
    "ax.set(xlabel=\"Time (steps)\", ylabel=\"Subspace alignment error\", yscale=\"log\", xscale=\"log\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online PSP version from another paper\n",
    "Giovannucci, Minden et al., 2018. Code is available on Github for this one, so I can test even more directly their algorithm (copy-pasted their code). Slightly different version, using matrix inversion and no $\\Lambda$ matrix (so we should compare with our code when setting $\\Lambda = 1$. \n",
    "\n",
    "As it turns out, the algorithm is really the same as the ``biopca_inv`` that I tested above, with the exception of an added factor 2 in the $M$ learning rate. This is actually important, because Minden 2018 reports using $\\tau < 1$, which seems to make the performance very poor; the factor $2$ added in Giovannucci and Minden compensates this difference and improves the error a lot. \n",
    "\n",
    "For an exact comparison of my PSP and theirs, I set $\\tau$ twice as small ($\\frac{1}{\\tau}$) twice as large for their version, so the 2 in $M$'s equation is compensated in $L$'s equation by this $\\tau$, and I set $\\alpha_t$ twice as small to compensate the overall added factor of two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_sm_smallN(t):\n",
    "    return 5 / (250 + t)\n",
    "\n",
    "def alpha_sm_largeN(t):\n",
    "    return 5.5e-4 if t <= 1e4 else 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-pasted code from \n",
    "# https://github.com/flatironinstitute/online_psp/blob/master/online_psp/similarity_matching.py\n",
    "class SM:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ====================\n",
    "    K             -- Dimension of PCA subspace to learn\n",
    "    D             -- Dimensionality of data\n",
    "    M0            -- Initial guess for the lateral weight matrix M, must be of size K-by-K\n",
    "    W0            -- Initial guess for the forward weight matrix W, must be of size K-by-D\n",
    "    learning_rate -- Learning rate as a function of t\n",
    "    tau           -- Learning rate factor for M (multiplier of the W learning rate)\n",
    "    Methods:\n",
    "    ========\n",
    "    fit_next()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, K, D, M0=None, W0=None, learning_rate=alpha_sm_smallN, tau=0.25):\n",
    "\n",
    "        if M0 is not None:\n",
    "            assert M0.shape == (K, K), \"The shape of the initial guess Minv0 must be (K,K)=(%d,%d)\" % (K, K)\n",
    "            M = M0\n",
    "        else:\n",
    "            M = np.eye(K)\n",
    "\n",
    "        if W0 is not None:\n",
    "            assert W0.shape == (K, D), \"The shape of the initial guess W0 must be (K,D)=(%d,%d)\" % (K, D)\n",
    "            W = W0\n",
    "        else:\n",
    "            W = np.random.normal(0, 1.0 / np.sqrt(D), size=(K, D))\n",
    "\n",
    "\n",
    "        self.eta = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.tau = tau\n",
    "        self.M = M\n",
    "        self.W = W\n",
    "\n",
    "        # Storage variables to allocate memory and optimize outer product time\n",
    "        self.outer_W = np.empty_like(W)\n",
    "        self.outer_M = np.empty_like(M)\n",
    "\n",
    "    def fit_next(self, x):\n",
    "\n",
    "        assert x.shape == (self.D,)\n",
    "\n",
    "        t, tau, W, M, K = self.t, self.tau, self.W, self.M, self.K\n",
    "\n",
    "        y = np.linalg.solve(M, W.dot(x))\n",
    "\n",
    "        # Plasticity, using gradient ascent/descent\n",
    "        # TODO: the factor of 2 can go away probably...\n",
    "        # W <- W + 2 eta(t) * (y*x' - W)\n",
    "        step = self.eta(t)\n",
    "\n",
    "        np.outer(2 * step * y, x, self.outer_W)\n",
    "        W = (1 - 2 * step) * W + self.outer_W\n",
    "\n",
    "        # M <- M + eta(self.t)/tau * (y*y' - M)\n",
    "        step = step / tau\n",
    "        np.outer(step * y, y, self.outer_M)\n",
    "        M = (1 - step) * M + self.outer_M\n",
    "\n",
    "        self.M = M\n",
    "        self.W = W\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "    def get_components(self, orthogonalize=True):\n",
    "        '''\n",
    "        Extract components from object\n",
    "        orthogonalize: bool\n",
    "            whether to orthogonalize the components before returning\n",
    "        Returns\n",
    "        -------\n",
    "        components: ndarray\n",
    "        '''\n",
    "\n",
    "        components = np.asarray(np.linalg.solve(self.M, self.W).T)\n",
    "        if orthogonalize:\n",
    "            components, _ = np.linalg.qr(components)\n",
    "\n",
    "        return components\n",
    "    \n",
    "# Using the class above, call fit_next repeatedly and save attributes\n",
    "# each time. \n",
    "def integrate_opca_sm(m_init, l_init, update_bk, bk_init, opca_params, bk_params, \n",
    "                     tmax, dt, seed=None, noisetype=\"normal\"):\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D\n",
    "    bk_vari_init, bk_vec_init = bk_init\n",
    "    assert n_dim == bk_vec_init.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    mrate, lrate = opca_params\n",
    "    # mrate is a callable: function of time. \n",
    "    \n",
    "    # Create relevant SM object\n",
    "    smobj = SM(n_neu, n_dim, l_init, m_init, mrate, 1.0 / lrate)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    tseries = np.arange(0, tmax, dt)\n",
    "\n",
    "    # Containers for the solution over time\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    bkvec_series = np.zeros([tseries.shape[0], n_dim])  # Input vecs, convenient to compute inhibited output\n",
    "\n",
    "    ## Initialize running variables, separate from the containers above to avoid side effects.\n",
    "    bk_vari = bk_vari_init.copy()\n",
    "    bkvec = bk_vec_init.copy()\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "\n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    bkvec_series[0] = bkvec\n",
    "\n",
    "    # Generate N(0, 1) noise samples in advance\n",
    "    if (tseries.shape[0]-1)*bk_vari.size > 1e7:\n",
    "        raise ValueError(\"Too much memory needed; consider calling multiple times for shorter times\")\n",
    "    if noisetype == \"normal\":\n",
    "        noises = rng.normal(0, 1, size=(tseries.shape[0]-1,*bk_vari.shape))\n",
    "    elif noisetype == \"uniform\":\n",
    "        noises = rng.random(size=(tseries.shape[0]-1, *bk_vari.shape))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Noise option {} not implemented\".format(noisetype))\n",
    "\n",
    "    t = 0\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        # Update synaptic weights with background from previous step\n",
    "        smobj.fit_next(bkvec)\n",
    "        \n",
    "        # Update background to time k+1, to be used in next time step (k+1)\n",
    "        bkvec, bk_vari = update_bk(bk_vari, bk_params, noises[k], dt)\n",
    "        t += dt\n",
    "\n",
    "        # Save current state\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = smobj.W\n",
    "        l_series[knext] = smobj.M\n",
    "        bkvec_series[knext] = bkvec\n",
    "\n",
    "    return tseries, bkvec_series, m_series, l_series, smobj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test of the PSP algorithm obtained from Github\n",
    "# And compare to my PSP algorithm\n",
    "def run_onetest_sm_github(init_m, init_l, update_fct, init_bk, opca_params, biopca_params, \n",
    "                    back_params, duration, dt, seed, psi_mat, noisetype, npts, tst):\n",
    "    lambd = biopca_params[-1]\n",
    "    # Run simulation and compute error with SM PSP\n",
    "    # m_init, l_init, update_bk, bk_init, opca_params, bk_params, \n",
    "                     #tmax, dt, seed=None, noisetype=\"normal\"\n",
    "    res = integrate_opca_sm(init_m, init_l, update_fct, init_bk, opca_params, back_params,  \n",
    "                duration, dt, seed=seed, noisetype=noisetype)\n",
    "    tser, bkvecser, mser, lser, cbarser = res\n",
    "    chosen_t, align_smpsp = compute_error_series(tser, bkvecser, mser, lser, lambd, \n",
    "                                                  inv_kind=\"exact\", npts=npts)\n",
    "\n",
    "    # Run simulation and compute error with PSP\n",
    "    res = integrate_biopca_psp(init_m, init_l, update_fct, init_bk, biopca_params, back_params,  \n",
    "                duration, dt, seed=seed, noisetype=noisetype)\n",
    "    tser, bkvecser, mser, lser, cbarser = res\n",
    "    chosen_t, align_mypsp = compute_error_series(tser, bkvecser, mser, lser, lambd, \n",
    "                                                  inv_kind=\"exact\", npts=npts)     \n",
    "\n",
    "    # Check covariance matrix of background samples, make sure the input is what we think\n",
    "    cov_mat_samples = np.dot(bkvecser.T, bkvecser) / (tser.size - 1)\n",
    "    errmsg = \"Problem with the empirical covariance of the generated samples\"\n",
    "    assert np.allclose(cov_mat_samples, psi_mat.dot(psi_mat.T), atol=4e-2), errmsg\n",
    "    \n",
    "    print(\"Finished test ntst = {}\".format(tst))\n",
    "    \n",
    "    return chosen_t, align_smpsp, align_mypsp\n",
    "    \n",
    "    \n",
    "\n",
    "# Run ntst tests of PSP algorithm obtained from Github\n",
    "def run_tests_sm_github_parallel(ntst, nn, nk, gtilde, lambd, alphat_both, duration=1e4, \n",
    "                        tau=0.5, npts=21, rng=None, njob=n_cpu):\n",
    "    # Check a few things\n",
    "    assert gtilde.shape[0] == nn\n",
    "    assert lambd.shape[0] == nk\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: normally-distributed, mean 0 and variance 1/N\n",
    "    init_m = rng.standard_normal(size=[nk, nn]) / np.sqrt(nn)\n",
    "    # L: identity matrix\n",
    "    init_l = np.eye(nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # Algorithm parameters\n",
    "    # For Github version, alpha twice smaller, tau twice smaller. \n",
    "    opca_params = [alphat_both[0], 2.0/tau]\n",
    "    biopca_params = [alphat_both[1], 1.0/tau, lambd]\n",
    "\n",
    "    \n",
    "    # Containers for the error as a function of the number\n",
    "    # of iterations, for each test run. \n",
    "    err_smpsp = np.zeros([ntst, npts])\n",
    "    err_mypsp = np.zeros([ntst, npts])\n",
    "    \n",
    "    # To generate a new seed for each simulation\n",
    "    seed_sequence = np.random.SeedSequence(seed_from_gen(rng))\n",
    "    \n",
    "    # Pool of workers, to parallelize\n",
    "    pool = multiprocessing.Pool(njob)\n",
    "    res_objs = []\n",
    "    \n",
    "    for tst in range(ntst):\n",
    "        # Generate a random orthogonal matrix R for this test run\n",
    "        r_mat = random_orthogonal_mat(nn, rng)\n",
    "        \n",
    "        # Square root decomposition of the actual covariance matrix\n",
    "        psi_mat = r_mat.dot(gtilde)\n",
    "        if not test_construct(gtilde, r_mat, psi_mat, rng=rng):\n",
    "            print(r\"Problem with the $\\Psi$ matrix trick!!!\")\n",
    "\n",
    "        # Background parameters with current psi_mat\n",
    "        init_bk = [np.zeros(nn, dtype=bool), psi_mat.dot(rng.standard_normal(size=nn))]\n",
    "        back_params = [psi_mat]\n",
    "        \n",
    "        # Get new seeds\n",
    "        seed_tst = seed_sequence.spawn(1)\n",
    "        \n",
    "        # Launch this test in parallel of others\n",
    "        res = pool.apply_async(run_onetest_sm_github, \n",
    "                            args=(init_m, init_l, update_mvnormal, init_bk, opca_params, biopca_params,\n",
    "                            back_params, duration, dt, seed_tst[0], psi_mat, \"normal\", npts, tst)\n",
    "                            )\n",
    "        res_objs.append(res)\n",
    "    \n",
    "    # Get the results\n",
    "    res_objs_finished = [a.get() for a in res_objs]\n",
    "    \n",
    "    # Don't forget to close the Pool! \n",
    "    pool.close()\n",
    "    \n",
    "    # Store results in appropriate containers\n",
    "    chosen_t = res_objs_finished[0][0]\n",
    "    for tst in range(ntst):\n",
    "        err_smpsp[tst] = res_objs_finished[tst][1]\n",
    "        err_mypsp[tst] = res_objs_finished[tst][2]\n",
    "        \n",
    "    return chosen_t, err_smpsp, err_mypsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run tests\n",
    "n_tests = 50\n",
    "n_checkpts = 11\n",
    "duration = 1e5\n",
    "lambd_mat = np.ones(n_k)\n",
    "\n",
    "chosen_times, errors_smpsp, errors_mypsp = run_tests_sm_github_parallel(\n",
    "        n_tests, n_n, n_k, gtilde_std, lambd_mat, [alpha_sm_smallN, alpha_t_smallN], \n",
    "        duration=duration, tau=tau_const, npts=n_checkpts, rng=rgen, njob=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the median alignment error as a function of iteration number\n",
    "mederr_smpsp = np.median(errors_smpsp, axis=0)\n",
    "mederr_mypsp = np.median(errors_mypsp, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "li1, = ax.plot(chosen_times, mederr_smpsp, label=\"Github PSP\")\n",
    "li2, = ax.plot(chosen_times, mederr_mypsp, label=\"My PSP\", ls=\"--\")\n",
    "ax.set(xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", \n",
    "       yscale=\"log\", xscale=\"log\")\n",
    "\n",
    "# Add reported values\n",
    "reported_errs_t = [1e3, 1e4, 1e5]\n",
    "#ax.plot(reported_errs_t, [2.1e-2, 1.5e-4, 1.7e-5], label=\"ifPSP (reported)\", \n",
    "#        marker=\"o\", ls=\"None\", ms=10, color=li1.get_color())\n",
    "ax.plot(reported_errs_t, [1.9e-2, 4.1e-4, 5.5e-5], label=r\"PSP (reported, different $\\mathbf{\\Lambda}$)\", \n",
    "        marker=\"s\", ls=\"None\", ms=6)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"../figures/tests/test_smpsp_n10_k3_align_subspace_error.pdf\", \n",
    "#           transparent=\"True\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-zero average inputs\n",
    "For some reason, the online algorithm seems to fail. The outputs do not follow the properties derived in the paper (e.g. Lemma 3) even though the proofs should hold even when the average $\\vec{x}$ is non-zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate multivariate samples from N(0, 1) samples (stdnorm_vecs)\n",
    "def generate_mvnormal_avg(bk_params, stdnorm_vecs):\n",
    "    # stdnorm_vecs: indexed [dimension, samples]\n",
    "    # psi: indexed [dimension, dimension]\n",
    "    # Return: indexed [dimension, sample]\n",
    "    psi = bk_params[0]\n",
    "    avg = bk_params[1]\n",
    "    return psi.dot(stdnorm_vecs) + avg[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline version of the algorithm, for comparison\n",
    "Maybe the offline algorithm converges and the problem just comes from the online version being affected too much by a non-zero average $\\vec{x}$. But I think both will fail if the online does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_alpha(t):\n",
    "    return 0.1\n",
    "\n",
    "def taylor_inverse(invdiag, offdiag):\n",
    "    return invdiag[:, np.newaxis] * (np.eye(invdiag.shape[0]) - offdiag*invdiag)\n",
    "\n",
    "def integrate_offline_ifpsp(m_init, l_init, bk_samples, biopca_params, tmax):\n",
    "    # Note: keep lambda matrix as 1d diagonal only, replace dot products by:\n",
    "    # Lambda.dot(A): Lambda_ii applied to row i, replace by Lambda_diag[:, None]*A element-wise\n",
    "    # A.dot(Lambda): Lambda_ii applied to column i, just A*Lambda broadcasts right\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D\n",
    "    assert n_dim == bk_samples.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    mrate, lrate, lambda_diag = biopca_params\n",
    "    # mrate is a callable: function of time. \n",
    "\n",
    "    tseries = np.arange(0, tmax, 1)\n",
    "\n",
    "    # Containers for the solution over iterations\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    f_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of projectors F = L^{-1} M\n",
    "    \n",
    "    ## Initialize running variables\n",
    "    xx_cov = bk_samples.dot(bk_samples.T) / (bk_samples.shape[1] - 1)\n",
    "    yx_cov = np.zeros([n_neu, n_dim])  # F.dot(xx_cov) at every iteration\n",
    "    yy_cov = np.zeros([n_neu, n_neu])  # F.dot(xx_cov).dot(F.T) at every iteration\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    \n",
    "    diag_idx = np.diag_indices(l_init.shape[0])\n",
    "    inv_l_diag = 1.0 / l_init[diag_idx]  # 1d flattened diagonal\n",
    "    # Use this difference the only time M_d is needed per iteration\n",
    "    # Faster to re-invert inv_l_diag than to slice lmat again\n",
    "    # l_offd = lmat - dflt(1.0 / inv_l_diag)\n",
    "    newax = np.newaxis\n",
    "    dflt = np.diagflat\n",
    "    #fmat = np.linalg.inv(lmat).dot(mmat)\n",
    "    fmat = taylor_inverse(inv_l_diag, lmat - dflt(1.0 / inv_l_diag)).dot(mmat)\n",
    "    \n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    f_series[0] = fmat\n",
    "\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ### Neural activity, averages over data given current projector. \n",
    "        yx_cov = fmat.dot(xx_cov)\n",
    "        yy_cov = yx_cov.dot(fmat.T)\n",
    "        \n",
    "        ### Synaptic plasticity: update mmat, lmat to k+1 based on covariances at k\n",
    "        mmat = mmat + mrate(k) * (yx_cov - mmat)\n",
    "        lmat = lmat + mrate(k) * lrate * (yy_cov - lambda_diag[:, newax]*lmat*lambda_diag)\n",
    "        # Update too the variable saving the inverse of the diagonal of L\n",
    "        inv_l_diag = 1.0 / lmat[diag_idx]\n",
    "        # and the projector F\n",
    "        fmat = taylor_inverse(inv_l_diag, lmat - dflt(1.0 / inv_l_diag)).dot(mmat)\n",
    "        #fmat = np.linalg.inv(lmat).dot(mmat)\n",
    "        \n",
    "        # Save current state\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        f_series[knext] = fmat\n",
    "    \n",
    "    return tseries, m_series, l_series, f_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_offline_psp(m_init, l_init, bk_samples, biopca_params, tmax):\n",
    "    # Note: keep lambda matrix as 1d diagonal only, replace dot products by:\n",
    "    # Lambda.dot(A): Lambda_ii applied to row i, replace by Lambda_diag[:, None]*A element-wise\n",
    "    # A.dot(Lambda): Lambda_ii applied to column i, just A*Lambda broadcasts right\n",
    "    n_neu = m_init.shape[0]  # Number of neurons N_I\n",
    "    n_dim = m_init.shape[1]  # Number of input neurons N_D\n",
    "    assert n_dim == bk_samples.shape[0], \"Mismatch between dimension of m and background\"\n",
    "    mrate, lrate, lambda_diag = biopca_params\n",
    "    # mrate is a callable: function of time. \n",
    "\n",
    "    tseries = np.arange(0, tmax, 1)\n",
    "\n",
    "    # Containers for the solution over iterations\n",
    "    m_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of M^T (N_IxN_D)\n",
    "    l_series = np.zeros([tseries.shape[0], n_neu, n_neu])  # series of L (N_IxN_I)\n",
    "    f_series = np.zeros([tseries.shape[0], n_neu, n_dim])  # series of projectors F = L^{-1} M\n",
    "    \n",
    "    ## Initialize running variables\n",
    "    xx_cov = bk_samples.dot(bk_samples.T) / (bk_samples.shape[1] - 1)\n",
    "    yx_cov = np.zeros([n_neu, n_dim])  # F.dot(xx_cov) at every iteration\n",
    "    yy_cov = np.zeros([n_neu, n_neu])  # F.dot(xx_cov).dot(F.T) at every iteration\n",
    "    mmat = m_init.copy()\n",
    "    lmat = l_init.copy()\n",
    "    \n",
    "    newax = np.newaxis\n",
    "    dflt = np.diagflat\n",
    "    fmat = np.linalg.inv(lmat).dot(mmat)\n",
    "    \n",
    "    # Store back some initial values in containers\n",
    "    m_series[0] = m_init\n",
    "    l_series[0] = l_init\n",
    "    f_series[0] = fmat\n",
    "\n",
    "    for k in range(0, len(tseries)-1):\n",
    "        ### Neural activity, averages over data given current projector. \n",
    "        yx_cov = fmat.dot(xx_cov)\n",
    "        yy_cov = yx_cov.dot(fmat.T)\n",
    "        \n",
    "        ### Synaptic plasticity: update mmat, lmat to k+1 based on covariances at k\n",
    "        mmat = mmat + mrate(k) * (yx_cov - mmat)\n",
    "        lmat = lmat + mrate(k) * lrate * (yy_cov - lambda_diag[:, newax]*lmat*lambda_diag)\n",
    "        # and the projector F\n",
    "        fmat = np.linalg.inv(lmat).dot(mmat)\n",
    "        \n",
    "        # Save current state\n",
    "        knext = (k+1)\n",
    "        m_series[knext] = mmat\n",
    "        l_series[knext] = lmat\n",
    "        f_series[knext] = fmat\n",
    "    \n",
    "    return tseries, m_series, l_series, f_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_product(a):\n",
    "    return a.dot(a.T)\n",
    "\n",
    "def run_offline_psp(algo, nn, nk, gtilde, lambd, alphat, nsamples=int(1e5), \n",
    "                      niter=int(1e4), tau=0.5, rng=None):\n",
    "    \"\"\" algo is either integrate_offline_ifpsp or integrate_offline_psp\"\"\"\n",
    "    # Assuming gtilde is diagonal\n",
    "    # Check a few things\n",
    "    assert gtilde.shape[0] == nn\n",
    "    assert lambd.shape[0] == nk\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    \n",
    "    # Initial matrices. M: normally-distributed, mean 0 and variance 1/N\n",
    "    init_m = rng.standard_normal(size=[nk, nn]) / np.sqrt(nn)\n",
    "    # L: identity matrix\n",
    "    init_l = np.eye(nk)\n",
    "    dt = 1.0\n",
    "    \n",
    "    # Rotation matrix transforming gtilde\n",
    "    r_mat = random_orthogonal_mat(nn, rng)\n",
    "    # Square root decomposition of the actual covariance matrix\n",
    "    psi_mat = r_mat.dot(gtilde)\n",
    "    if not test_construct(gtilde, r_mat, psi_mat, rng=rng):\n",
    "        print(r\"Problem with the $\\Psi$ matrix trick!!!\") \n",
    "    \n",
    "    # Average of the input samples\n",
    "    avg_vec = 0*rng.random(size=nn)\n",
    "    \n",
    "    # Generate samples in advance\n",
    "    bk_samples_offline = generate_mvnormal_avg([psi_mat, avg_vec], \n",
    "                            rng.standard_normal(size=[nn, nsamples]))\n",
    "\n",
    "    # Exact PCA: eigenvalue decomposition of xx^T / (n_samples-1)\n",
    "    eigvals, eigvecs = np.linalg.eigh(bk_samples_offline.dot(bk_samples_offline.T) / (nsamples-1))\n",
    "    eigvals = eigvals[::-1]  # Put largest eigenvalues first\n",
    "    eigvecs = eigvecs[:, ::-1]\n",
    "        \n",
    "    # Algorithm parameters\n",
    "    biopca_params = [alphat, 1.0/tau, lambd]\n",
    "    \n",
    "    # Run simulation and compute error with ifPSP\n",
    "    res = algo(init_m, init_l, bk_samples_offline, biopca_params, niter)\n",
    "    tser, mser, lser, fser = res\n",
    "    #print(cov_product(1.0/lambd[:, None] * fser[-1]))\n",
    "    \n",
    "    # Determine basis learnt by algorithm and return\n",
    "    learntvecs = ((1.0/lambd[:, None]) * fser[-1]).T\n",
    "    # Each column of learntvecs is an eigenvector learnt. \n",
    "    \n",
    "    # Values on the diagonal of L are supposed to be eigenvalues of XX^T / nsamples\n",
    "    learntvals = np.diagonal(lser[-1])\n",
    "    print(lser[-1])\n",
    "    \n",
    "    return bk_samples_offline, [eigvals, eigvecs], [learntvals, learntvecs], [mser, lser, fser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline ifPSP\n",
    "The offline version seems to always converge to all \"eigenvalues\" on the $L$ matrix diagonal being equal. \n",
    "\n",
    "Note that this is for zero average background; when the background average is non-zero, the eigenvalues are just crazy numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionalities\n",
    "rgen_off = np.random.default_rng(seed=0xb61d55bd547e030074bdfaefaf1fad45)\n",
    "n_n_off = 10  # input\n",
    "n_k_off = 3  # output\n",
    "\n",
    "# Check alignment subspace metric\n",
    "if test_error_metric(n_n_off, rgen_off):\n",
    "    print(\"Subspace alignment metric seems to work\")\n",
    "\n",
    "# Standard deviation matrix diagonalized\n",
    "gtilde_std_off = np.sqrt(np.diagflat([1.0, 0.75, 0.5] + [0.2] * (n_n_off - 3)))\n",
    "\n",
    "# Lambda matrix\n",
    "#lambd_mat_off = np.asarray([1, 0.85])\n",
    "lambd_mat_off = np.asarray([1, 0.85, 0.7])\n",
    "\n",
    "tau_const = 1.0  # Use value reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_offline_psp(integrate_offline_ifpsp, n_n_off, n_k_off, gtilde_std_off, lambd_mat_off, \n",
    "                offline_alpha, nsamples=int(1e5), niter=int(1e4), tau=tau_const, rng=rgen_off)\n",
    "bk_samples_off, true_pca_off, learnt_pca_off, learn_series = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Offline ifPSP\")\n",
    "print(\"Subspace align. error:\", subspace_align_error(learnt_pca_off[1], true_pca_off[1][:, :n_k_off]))\n",
    "\n",
    "print(\"Learnt eigenvalues:\", learnt_pca_off[0])\n",
    "print(\"True eigenvalues:\", true_pca_off[0][:n_k_off])\n",
    "#print(\"Learnt eigenvectors (columns):\\n\", learnt_pca_off[1])\n",
    "#print(\"True eigenvectors (columns):\\n\", true_pca_off[1][:, :n_k_off])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time course of l diagonal\n",
    "learntvalser = np.diagonal(learn_series[1], axis1=1, axis2=2)\n",
    "sort_learnt_pca = np.argsort(learntvalser[-1])[::-1]\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "ax = axes[0]\n",
    "for i in range(learntvalser.shape[1]):\n",
    "    li, = ax.plot(np.arange(1, learntvalser.shape[0]), learntvalser[1:, sort_learnt_pca[i]], \n",
    "                  label=\"Diagonal element ({},{})\".format(i, i), lw=2.0)\n",
    "    ax.axhline(true_pca_off[0][i], color=li.get_color(), ls=\"--\", lw=1.0)\n",
    "ax.set(ylabel=r\"Diagonal elements of $L$\", xscale=\"log\")\n",
    "ax.annotate(\"Principal values learning\", xy=(0.99, 0.98), ha=\"right\", va=\"top\", xycoords=\"axes fraction\")\n",
    "\n",
    "# Subspace error\n",
    "log_times = np.logspace(0, 4, 51).astype(int) - 1\n",
    "log_times = log_times.clip(1)\n",
    "learnt_pca_ser = [((1.0/lambd_mat_off[:, None]) * learn_series[2][i]).T for i in log_times]\n",
    "error_series = [subspace_align_error(true_pca_off[1][:, :n_k_off], v) for v in learnt_pca_ser]\n",
    "ax = axes[1]\n",
    "ax.plot(log_times, error_series, color=\"k\")\n",
    "ax.set(xscale=\"log\", xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", yscale=\"log\")\n",
    "ax.annotate(\"Principal components learning\", xy=(0.99, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline PSP \n",
    "With exact inverse, we sometimes get long-term convergence to distinct eigenvalues, sometimes to same eigenvalue. Seems completely degenerate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgen_off2 = np.random.default_rng(seed=0x839dcd7e6051b2b4b34d7785d4c5575e)\n",
    "res = run_offline_psp(integrate_offline_psp, n_n_off, n_k_off, gtilde_std_off, lambd_mat_off, \n",
    "                offline_alpha, nsamples=int(1e5), niter=int(1e4), tau=tau_const, rng=rgen_off2)\n",
    "bk_samples_off2, true_pca_off2, learnt_pca_off2, learn_series2 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Offline PSP\")\n",
    "print(\"Subspace align. error:\", subspace_align_error(learnt_pca_off2[1], true_pca_off2[1][:, :n_k_off]))\n",
    "\n",
    "print(\"Learnt eigenvalues:\", learnt_pca_off2[0])\n",
    "print(\"True eigenvalues:\", true_pca_off2[0][:n_k_off])\n",
    "#print(\"Learnt eigenvectors (columns):\\n\", learnt_pca_off2[1])\n",
    "#print(\"True eigenvectors (columns):\\n\", true_pca_off2[1][:, :n_k_off])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time course of l diagonal\n",
    "learntvalser = np.diagonal(learn_series2[1], axis1=1, axis2=2)\n",
    "sort_learnt_pca = np.argsort(learntvalser[-1])[::-1]\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "ax = axes[0]\n",
    "for i in range(learntvalser.shape[1]):\n",
    "    li, = ax.plot(np.arange(1, learntvalser.shape[0]), learntvalser[1:, sort_learnt_pca[i]], \n",
    "                  label=\"Diagonal element ({},{})\".format(i, i), lw=2.0)\n",
    "    ax.axhline(true_pca_off2[0][i], color=li.get_color(), ls=\"--\", lw=1.0)\n",
    "ax.set(ylabel=r\"Diagonal elements of $L$\", xscale=\"log\")\n",
    "ax.annotate(\"Principal values learning\", xy=(0.99, 0.98), ha=\"right\", va=\"top\", xycoords=\"axes fraction\")\n",
    "\n",
    "# Subspace error\n",
    "log_times = np.logspace(0, 4, 51).astype(int) - 1\n",
    "log_times = log_times.clip(1)\n",
    "learnt_pca_ser = [((1.0/lambd_mat_off[:, None]) * learn_series2[2][i]).T for i in log_times]\n",
    "error_series = [subspace_align_error(true_pca_off2[1][:, :n_k_off], v) for v in learnt_pca_ser]\n",
    "ax = axes[1]\n",
    "ax.plot(log_times, error_series, color=\"k\")\n",
    "ax.set(xscale=\"log\", xlabel=\"Iterations\", ylabel=\"Subspace alignment error\", yscale=\"log\")\n",
    "ax.annotate(\"Principal components learning\", xy=(0.99, 0.98), ha=\"right\", va=\"top\", xycoords=\"axes fraction\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
