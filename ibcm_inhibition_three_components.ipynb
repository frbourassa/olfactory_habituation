{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background subspace inhibition with IBCM neurons\n",
    "A layer of IBCM neurons is used to inhibit the activity of ORNs in response to a fluctuating olfactory background. Synaptic weights from the input layer to the inhibition layer, $M = (\\vec{m}_1, \\vec{m}_2, \\ldots)$, are learnt according to the IBCM rule. Synaptic weights for inhbition, from the inhibitory neurons to the projection layer, are learnt to minimize the squared norm of the projection neuron (PN) layer. In this way, the network of IBCM neurons is like an autoencoder applying feedforward inhibition. \n",
    "\n",
    "![test](figures/feedforward_inhibitory_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network of IBCM neurons\n",
    "\n",
    "Consider a feedforward network of IBCM  neurons. Each will be paired with an inhibitory neuron, but those inhibitory neurons will not interact with other neurons except their own IBCM neuron, so we can discuss them later. Each IBCM neuron has $N_D \\geq N_I$ input synapses, represented by the connectivity vector $\\vec{m}_i = (m^1_i, \\ldots, m^{N_D}_i)$. Its activation upon stimulation, before coupling to other IBCM neurons is considered, is given by $c = \\vec{m}_i \\cdot \\vec{x}$, where $\\vec{x}$ is a two-dimensional input vector. Its activity inhibited by other IBCM neurons in the network is \n",
    "\n",
    "$$ \\bar{c}_i = c_i - \\eta \\sum_{j \\neq i} c_j \\quad \\mathrm{where} \\, \\, c_i(t) = \\vec{m}_i(t) \\cdot \\vec{x}(t)  \\, \\, .$$\n",
    "\n",
    "The update equation of each IBCM neuron's weights uses this inhibited activity:\n",
    "\n",
    "$$ \\dot{m}_i = \\mu \\left[ \\bar{c}_i(\\bar{c}_i - \\bar{\\Theta}_i) \\vec{x} - \\eta \\sum_{j \\neq i} \\bar{c}_j(\\bar{c}_j - \\bar{\\Theta}_j) \\vec{x} \\right]  \\quad \\mathrm{where} \\, \\, \\bar{\\Theta}_i = \\mathbb{E}[\\bar{c}_i^2] $$\n",
    "\n",
    "The parameter $\\eta$ is the coupling strength. The thresholds $\\bar{\\Theta}_i$ are time-dependent averages of the IBCM neuron's inhibited activity squared, $\\bar{c}_i^2$; they evolve with $\\vec{m}_i$ according to the differential equation\n",
    "\n",
    "$$ \\dot{\\bar{\\Theta}}_i = \\frac{1}{\\tau_\\Theta} (\\bar{c}_i^2 - \\bar{\\Theta}_i)  \\,\\, .$$\n",
    "\n",
    "Hence, they effectively take the average of $\\bar{c}_i^2$ over a sliding exponential window with a time scale $\\tau_\\Theta$. This time scale should be a lot longer than the fluctuation time scale of the input, $\\tau_c$, but also a lot faster than the slow time scale of $\\vec{m}_i$ learning itself. In short, we should have $\\tau_c \\ll \\tau_{\\Theta} \\ll \\frac{1}{\\mu}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lateral inhibition with the IBCM neurons\n",
    "Use the IBCM neurons as lateral inhibitory neurons. There are weights $\\vec{w}_i$ going from IBCM neuron $i$ to projection neurons. We can store them in the matrix $W$, where each column is a vector $\\vec{w}_i$. The total inhibition received by projection neurons is therefore $\\sum_{j} \\bar{c}_j \\vec{w}_j = W \\vec{\\bar{c}}$; if these neurons have an element-wise activation function $R$, typically a ReLU function, they take the value\n",
    "\n",
    "$$ \\vec{s} = R\\left(\\vec{x}_{in}(t) - W \\vec{\\bar{c}}  \\right) $$\n",
    "\n",
    "By calculating the gradient of the cost function\n",
    "\n",
    "$$ C(\\vec{w}_j) = \\frac12 \\mathbb{E}\\left[\\vec{s}^T \\vec{s} \\right] + \\frac{\\beta}{2\\alpha} \\mathbb{E}\\left[ \\vec{w}_j^T \\vec{w}_j \\right] $$\n",
    "\n",
    "we find that the inhibition weights leaving neuron $j$ should evolve according to\n",
    "\n",
    "$$ \\frac{d \\vec{w}_j}{dt} = -\\alpha \\nabla_{\\vec{w}_j} C(\\vec{w}_j) = \\alpha \\bar{c}_j \\vec{s} R'\\left(\\vec{s}\\right) -  \\beta \\vec{w}_j $$\n",
    "\n",
    "after assuming that $\\alpha$ is small enough to take instantaneous values but still see $\\vec{w}$ converge on average to the optimum of the cost function. $R'\\left(\\vec{s}\\right)$ is the element-wise derivative of the activation function; for instance, it is $1$ if $R$ is the identity function, or a Heaviside function if $R$ is a ReLU. In those two cases, it can simply be omitted -- in the latter case, the ReLU applied on $\\vec{s}$ itself ensures the term is zero if the difference $\\vec{x} - W\\vec{\\bar{c}}$ is negative. \n",
    "Here, I will use a ReLU, so\n",
    "\n",
    "$$ \\frac{d\\vec{w}_j}{dt} = \\alpha \\bar{c}_j \\vec{s} -  \\beta \\vec{w}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "\n",
    "from utils.statistics import seed_from_gen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing different ReLU functions. \n",
    "# Methods from https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy\n",
    "def relu1(x):\n",
    "    return x * (x > 0.0)\n",
    "def relu2(x):\n",
    "    return (np.abs(x) + x) * 0.5\n",
    "def relu3(x):\n",
    "    x[x < 0.0] = 0.0\n",
    "    return x\n",
    "def relu4(x):   # Turns out to be fastest\n",
    "    return np.maximum(x, 0.0, x)\n",
    "def relu5(x):\n",
    "    return np.maximum(x, 0.0)\n",
    "    \n",
    "x_test = np.random.normal(size=10)\n",
    "relus = {\n",
    "    \"boolean\": relu1, \n",
    "    \"abs\": relu2, \n",
    "    \"indexing\": relu3,\n",
    "    \"max_inplace\": relu4, \n",
    "    \"max\": relu5\n",
    "}\n",
    "for name, relu in relus.items():\n",
    "    n_iter = 20\n",
    "    x = np.random.random((n_iter, 5000, 5000)) - 0.5\n",
    "\n",
    "    t1 = perf_counter()\n",
    "    for i in range(n_iter):\n",
    "        relu(x[i])\n",
    "    t2 = perf_counter()\n",
    "\n",
    "    print(\"{:>12s}  {:3.0f} ms\".format(name, (t2 - t1) / n_iter * 1000))\n",
    "\n",
    "# Results:\n",
    "#     boolean  228 ms\n",
    "#         abs  134 ms\n",
    "#    indexing  169 ms\n",
    "# max_inplace   33 ms\n",
    "#         max  109 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm import (\n",
    "    integrate_inhib_ibcm_network, \n",
    "    relu_inplace, \n",
    "    compute_mbars_cgammas_cbargammas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input process\n",
    "To have multiple input components fluctuating realistically, we write the input as\n",
    "$$ \\vec{x}(t) = \\sum_{\\alpha=1}^K \\nu_\\alpha \\vec{x}_\\alpha $$\n",
    "where the $\\nu_\\alpha$ are random variables with some correlation, to mimick the turbulent flow that carries all odorants together. Ideally, they would follow the distributions derived in (Celani, Villermaux and Vergassola 2014), but those are a bit tricky to simulate.\n",
    "\n",
    "### General case of the Ornstein-Uhlenbeck process\n",
    "The multivariate Langevin equation for the Ornstein-Uhlenbeck process is:\n",
    "\n",
    "$$ d\\vec{x} = -A \\vec{x}(t) dt + B dW(t) $$\n",
    "\n",
    "where $\\frac{dW}{dt} = \\vec{\\eta}(t)$, a vector of gaussian white noise (independent components), $A$ and $B$ are matrices. Assume the matrix $A$ is normal and can be diagonalized as $A = U D U^\\dagger$, where $D = \\mathrm{diag}(\\lambda^1, ..., \\lambda^n)$. For a deterministic initial condition $\\vec{x}(t_0) = \\vec{x}_0$, the general solution is that $\\vec{x}(t)$ follows a multivariate normal distribution, with mean and variance given by\n",
    "\n",
    "$$ \\langle \\vec{x}(t) \\rangle = U\\mathrm{e}^{-D(t-t_0)}U^{\\dagger} \\vec{x}_0 $$\n",
    "$$ \\langle \\vec{x}(t) \\vec{x}(t)^T \\rangle = U J(t, t_0) U^\\dagger $$\n",
    "\n",
    "where the components of $J$ are \n",
    "\n",
    "$$ J^{ij}(t, t_0) = \\left(\\frac{U^\\dagger B B^T U}{\\lambda^i + \\lambda^j} \\right)^{ij} \\left(1 - e^{-(\\lambda^i + \\lambda^j)(t - t_0)}  \\right) $$\n",
    "\n",
    "\n",
    "The stationary distribution of $\\vec{x}$ is thus\n",
    "\n",
    "$$ \\vec{x}^* \\sim \\mathcal{N} \\left(\\vec{0}, \\left(\\frac{B B^T}{\\lambda^k + \\lambda^l} \\right)^{kl} \\right) \\,\\, .$$\n",
    "\n",
    "For a non-zero mean, simulate the zero-mean process and add the average afterwards, it's simpler. \n",
    "\n",
    "For details, see Gardiner, chapter 4.2.6. \n",
    "\n",
    "### Exact numerical simulation, general case\n",
    "To simulate a realization of this process exactly, we use a trick suggested by Gillespie in the univariate case (which only works for the Ornstein-Uhlenbeck process because it's linear and gaussian). We iteratively take $\\vec{x}(t)$ as the initial condition of the evolution up to $\\vec{x}(t + \\Delta t)$, the distribution of which is\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) \\sim \\mathcal{N}\\left( U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) , U J(t + \\Delta t, t) U^\\dagger \\right) $$\n",
    "\n",
    "which can be rewritten using the following property of multivariate normal distributions: if $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$, then $\\vec{x} = \\vec{\\mu} + \\Psi \\vec{n} \\sim \\mathcal{N}(\\vec{\\mu}, \\Psi \\Psi^T)$ ($\\Psi$ is the Cholesky decomposition of the desired covariance matrix). This property is easily demonstrated by computing $\\langle \\vec{x} \\rangle$ and $\\langle \\vec{x} \\vec{x}^T \\rangle$ and using the linearity of multivariate normal distributions. For our update rule, this gives\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = U e^{-D \\Delta t}U^\\dagger \\vec{x}(t) + \\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right] \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n}$ is a vector of standard normal(0, 1) samples. The matrices $U e^{-D \\Delta t}U^\\dagger$ and $\\mathrm{Chol}\\left[U J(t + \\Delta t, t) U^\\dagger \\right]$ can be computed only once and applied repeatedly to the $\\vec{x}(t)$ obtained in sequence and the $\\vec{n}$ drawn at each iteration. The Cholesky decomposition of $UJU^\\dagger$ is not obviously expressed in terms of $B$, because the possibly different $\\lambda^i$ values mix up components. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple case and exact simulation of it\n",
    "If $A$ is diagonal, the $U$ matrices are just identity matrices and disappear, but the Cholesky decomposition of $J(t + \\Delta t, t)$ is still not obvious. More explicit expressions can be obtained in the simplifying case where $A$ is proportional to the identity matrix, i.e., all components of $\\vec{x}$ have the same fluctuation time scale. \n",
    "\n",
    "Let's say that $A =  \\frac{1}{\\tau} \\mathbb{1}$, where $\\tau$ is the fluctuation time scale ($\\lambda^i = \\tau \\,\\, \\forall i$). Then, the matrix $J$ simplifies to \n",
    "\n",
    "$$J(t, t_0) = \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  $$\n",
    "\n",
    "and its Cholesky decomposition is simply $\\sqrt{\\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right) } B$. Hence, the distribution of $\\vec{x}(t)$ at any time since $t_0$ (deterministic initial condition $\\vec{x}_0$) is\n",
    "\n",
    "$$ \\vec{x}(t) \\sim \\mathcal{N} \\left(e^{-(t-t_0)/\\tau} \\vec{x}_0, \\frac{\\tau}{2}\\left(1 - e^{-2(t - t_0)/\\tau} \\right)  BB^T  \\right) $$\n",
    "\n",
    "The stationary distribution is simply the above with the exponential factors set to 0. The update rule from $\\vec{x}(t)$ to $\\vec{x}(t + \\Delta t)$ to simulate a realization of the process is nicer as well:\n",
    "\n",
    "$$ \\vec{x}(t + \\Delta t) = e^{-\\Delta t / \\tau} \\vec{x}(t) + \\sqrt{\\frac{\\tau}{2} \\left(1 - e^{-2\\Delta t/\\tau}  \\right)} B \\cdot \\vec{n} $$\n",
    "\n",
    "where $\\vec{n} \\sim \\mathcal{N}(\\vec{0}, \\mathbb{1})$ is a vector of independent standard normal samples.\n",
    "\n",
    "As before, we can compute once the (scalar) factor $e^{-\\Delta t / \\tau}$ and the . This is exact for any $\\Delta t$, there is no increase in accuracy by decreasing $\\Delta t$. You just choose the $\\Delta t$ resolution at which you want to sample the realization of the process. \n",
    "\n",
    "### Symmetric choices for correlations\n",
    "We want all pairs of $\\nu_\\gamma$ to have the same correlation. More specifically, we want to force a Pearson correlation coefficient of $0 < \\rho < 1$ between any pair of $\\nu$s. We suppose all background components have the same individual variance $\\sigma^2$. The corresponding covariance matrix we want for the steady-state distribution is\n",
    "\n",
    "$$ \\Sigma = \\sigma^2 \\begin{pmatrix}\n",
    "    1 & \\rho & \\ldots & \\rho \\\\\n",
    "    \\rho & 1 & \\ldots & \\rho \\\\\n",
    "    \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "    \\rho & \\rho & \\ldots & 1\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we apply Cholesky decomposition to get $\\Sigma = \\Psi \\Psi^T$, then $\\sqrt{\\tau/2} B = \\Psi$, since the steady-state covariance of the Ornstein-Uhlenbeck process is, in this simplified case, $\\frac{\\tau}{2} BB^T$. The $M_B$ coefficient in the update rule is then\n",
    "\n",
    "$$ M_B = \\sqrt{\\tau/2(1 - e^{-2 \\Delta t/\\tau})}B = \\sqrt{(1 - e^{-2 \\Delta t/\\tau})} \\Psi $$\n",
    "\n",
    "The other coefficient is just\n",
    "\n",
    "$$ M_A = e^{-\\Delta t / \\tau} \\mathbb{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to update the fluctuating background variable\n",
    "from modelfcts.backgrounds import update_ou_kinputs, update_ou_2inputs, decompose_nonorthogonal_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simulation with gaussian background\n",
    "Select parameters, background components below to integrate the IBCM and inhibitory neuron equations while the background fluctuates. One simulation runs in ~10 s on my laptop (not very efficient Python code). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical fixed point prediction for IBCM model \n",
    "**Consider the case of gaussian $\\nu_{\\alpha}$ and $\\rho=0$**\n",
    "\n",
    "Calculations predicting a $K-2$ dimensional ensemble of fixed points in the subspace spanned by the $K$ background components. If $D > K$, the $D-K$ dimensions not spanned by the odor components have no dynamics happening in them,  since $\\frac{d\\vec{m}}{dt} \\propto \\vec{x}(t)$; initial conditions remain unchanged in that space. So, in the full $D$-dimensional space, the fixed points occupy a structure of $D-2$ dimensions, but only $K-2$ of them are not trivial. \n",
    "The two constraints satisfied at the fixed points are, in the $\\rho = 0$ case, which is just a linear transformation away from the more general case (by diagonalization of the covariance matrix):\n",
    "$$ \\sum_{\\alpha} \\bar{c}_{\\alpha} = 1 $$\n",
    "$$ \\sum_{\\alpha} \\bar{c}_{\\alpha}^2 = \\frac{1}{\\sigma^2} $$\n",
    "where\n",
    "$$ \\bar{c}_{i, \\alpha} = \\vec{m}_{i} \\cdot \\vec{x}_{\\alpha} - \\eta \\sum_{j \\neq i} \\vec{m}_j \\cdot \\vec{x}_{\\alpha} $$\n",
    "\n",
    "Note that the case $\\rho \\neq 0$ would be qualitatively the same as $\\rho = 0$: we could always rewrite the input mixture as a linear combination in the basis of the eigenvectors of the correlation matrix, where there are no correlations between components. Hence, posing $\\rho = 0$ to simplify calculations does not reduce the generality of the result. \n",
    "\n",
    "In the $\\rho \\neq 0$, one can derive the fixed point by transforming to normal coordinates, applying those two constraints on the new odor components, and transforming back. I did not take the time to do it explicity. Here, for comparison with the calculation, I set $\\rho = 0$ and compare to analytical predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General simulation parameters\n",
    "n_dimensions = 4\n",
    "n_components = 3\n",
    "n_neurons = 6\n",
    "\n",
    "# Simulation time scales\n",
    "duration = 100000.0\n",
    "deltat = 1.0\n",
    "learnrate = 0.003\n",
    "tau_avg = 150\n",
    "inhib_rates = [0.00025, 0.00005]  # alpha, beta\n",
    "\n",
    "# coupling strength\n",
    "coupling_eta = 0.2 / n_neurons\n",
    "ibcm_rates = [learnrate, tau_avg, coupling_eta]\n",
    "\n",
    "# Choose three LI vectors in (+, +, +) octant: [0.8, 0.1, 0.1], [0.1, 0.8, 0.1], etc.\n",
    "back_components = 0.1*np.ones([n_components, n_dimensions])\n",
    "for i in range(n_components):\n",
    "    if i < n_dimensions:\n",
    "        back_components[i, i] = 0.8\n",
    "    else:  # If there are more components than there are dimensions (ORNs)\n",
    "        back_components[i, i % n_dimensions] = 0.8 - i\n",
    "    # Normalize\n",
    "    back_components[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2))\n",
    "\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=0x8549e8fc7718e5fa3e7516addff73b86)\n",
    "init_synapses = 0.8*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "# Initial background params, ordered with nu first for the update_ou_kinputs function\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Correlation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0  # Set to zero for comparison with analytical prediction\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params = [update_mat_A, update_mat_B, back_components, averages_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_init, update_bk, bk_init, ibcm_params, inhib_params, bk_params, tmax, dt, seed=None, noisetype=\"normal\"\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_ou_kinputs, init_back_list, ibcm_rates, \n",
    "                inhib_rates, back_params, duration, deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "# tseries, bk_series, bkvec_series, m_series, cbar_series, w_series, s_series\n",
    "tser, nuser, bkvecser, mser, cbarser, _, wser, sser = sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the solution\n",
    "\n",
    "For a gaussian background, there is degeneracy in the fixed points of IBCM neurons. \n",
    "We plot a few different quantities to control how the model is behaving:\n",
    "- The sums $\\sum_{\\gamma} c_{\\gamma}$ and $\\sum_{\\gamma} c_{\\gamma}^2$, for which analytical values are $\\frac{1}{\\langle \\nu \\rangle}$ and $\\frac{1}{\\sigma^2}$, respectively. \n",
    "- The inhibitory weights $\\vec{w}^j$ leaving each IBCM neuron. The steady-states can't be predicted, really, because of the degeneracy (TODO: check if we can predict their sums or their norms anyways, etc.). \n",
    "\n",
    "\n",
    "Because of the degeneracy, the $\\bar{m}_{\\gamma}$s individuallyn can take arbitrary values as the $\\vec{\\bar{m}}$ land anywhere on the N-2 dimensional sphere of fixed points, so plotting them separately as a function of time is not very informative. However, a 3D plot of the $\\vec{\\bar{m}}$ is illustrative, so we keep doing that below. \n",
    "\n",
    "\n",
    "\n",
    "### IBCM neurons\n",
    "Dynamics, comparison to analytical fixed points, and plot of the time course of some selected components of $\\vec{m}$ of a few vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 50000\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser, c_gammas, cbars_gamma = compute_mbars_cgammas_cbargammas(mser, coupling_eta, back_components)\n",
    "\n",
    "sums_cbars_gamma = np.sum(cbars_gamma, axis=2)\n",
    "sums_cbars_gamma2 = np.sum(cbars_gamma*cbars_gamma, axis=2)\n",
    "\n",
    "# Constaint 1: sum of c_gammas for each neuron\n",
    "print(\"Comparison to analytical fixed points\")\n",
    "print(\"This should be all ones:\", np.mean(sums_cbars_gamma[transient:], axis=0) * averages_nu.mean())\n",
    "# Constraint 2: sum of c_gammas^2 for each neuron, copmared to 1/sigma^2\n",
    "print(\"This should be all zeros:\", np.mean(sums_cbars_gamma2[transient:], axis=0)*sigma2 - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_cbars_gammas_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_cbars_gammas_sums(tser, sums_cbars_gamma, sums_cbars_gamma2, skp=200, skp_lbl=1)\n",
    "axes[0].axhline(1.0 / averages_nu.mean(), ls=\"--\", color=\"k\", label=r\"$1 / \\langle \\nu \\rangle$\")\n",
    "axes[1].axhline(1.0 / sigma2, ls=\"-.\", color=\"k\", label=r'$1 / \\sigma^2$')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "# fig.savefig(\"figures/three_odors/sum_cgammas_squared_gaussian_background.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_cbars_gamma_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not interesting with gaussian degeneracy, not only two possible values of c_gamma. \n",
    "fig, ax, _ = plot_cbars_gamma_series(tser, cbars_gamma, skp=100, transient=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_3d_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_series(mbarser, dim_idx=[0, 1, 2], transient=10000, skp=1000)\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=45, elev=30)\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "# fig.savefig(\"figures/three_odors/points_fixes_ibcm_3_odeurs_gaussien.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp = 1\n",
    "tini = 0\n",
    "tmx = 100000# int(duration)#20000#int(duration)\n",
    "tslice = slice(tini, tmx, skp)\n",
    "fig, ax = plt.subplots()\n",
    "w1_palette = sns.color_palette(\"Blues\", n_colors=n_neurons)\n",
    "w2_palette = sns.color_palette(\"Purples\", n_colors=n_neurons)\n",
    "w3_palette = sns.color_palette(\"Greens\", n_colors=n_neurons)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 0], color=\"orange\", alpha=0.5)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 1], color=\"blue\", alpha=0.5)\n",
    "#ax.plot(tser3[tslice], bkvecser3[tslice, 0], color=\"red\", alpha=0.8)\n",
    "#ax.plot(tser[tslice], thetaser[tslice, -1], color=\"orange\", alpha=0.8)\n",
    "for i in range(n_neurons-1):\n",
    "    pass\n",
    "    #ax.plot(tser3[tslice], mser3[tslice, i, 0], color=w1_palette[i], alpha=0.8)\n",
    "    #ax.plot(tser3[tslice], mser3[tslice, i, 1], color=w2_palette[i], alpha=0.8)\n",
    "    #ax.plot(tser3[tslice], mser3[tslice, i, 2], color=w3_palette[i], alpha=0.8)\n",
    "ax.plot(tser[tslice], mser[tslice, -1, 0], color=w1_palette[-1], label=\"Neuron Component 0\", alpha=0.8)\n",
    "ax.plot(tser[tslice], mser[tslice, -1, 1], color=w2_palette[-1], label=\"Neuron Component 1\", alpha=0.8)\n",
    "ax.plot(tser[tslice], mser[tslice, -1, 2], color=w3_palette[-1], label=\"Neuron Component 2\", alpha=0.8)\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Inhibition neurons components\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the inhibitory neurons' weights $\\vec{w}_i$\n",
    "Analytically, for a toy model with two neurons and a 2D mixture, I find that $\\langle \\vec{w}^j \\rangle$ converges to $\\frac{\\alpha}{2\\alpha + \\beta}\\vec{x}(\\pm \\sigma)$, i.e. to either input vector one standard deviation away from the mean input. \n",
    "\n",
    "### Analytical prediction of $\\vec{w}$ for gaussian $\\vec{x}$\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_w_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the time course of the dot products -- not interesting with gaussian degeneracy\n",
    "# Unclear what it shows. \n",
    "fig, axes = plot_w_matrix(tser, wser, skp=500, lw=1.5)\n",
    "        \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background before and after inhibition\n",
    "\n",
    "Not sure what is best to plot. I will try plotting the norm of the background as a function of time, without and with inhibition, on the same plot. \n",
    "However, this is not a good metric for, say, a background that alternates between equal norm odors, or a saturated background that just rotates. \n",
    "In that case, we need to compute the standard deviation of each ORN/PN and combine them, because the total activity norm would not fluctuate much. \n",
    "\n",
    "I also try, again, making small plots, one per component. \n",
    "\n",
    "TODO: These are still not publication-quality plots, but they will do for now. We need a rough draft of the manuscript before I really know what to plot. \n",
    "\n",
    "### Analytical prediction of $\\vec{s}$ for gaussian $\\vec{x}$ and many neurons\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulfcts.plotting import plot_background_norm_inhibition, plot_background_neurons_inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser, snorm_ser = plot_background_norm_inhibition(tser, bkvecser, sser)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bknorm_ser[transient:])\n",
    "avg_snorm = np.mean(snorm_ser[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser[transient:])\n",
    "std_snorm = np.std(snorm_ser[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/three_odors/inhibition_gaussian_background_norm_3odors.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser, bkvecser, sser)\n",
    "axes[-1].legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.6), fontsize=8, handlelength=1.5)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/three_odors/inhibition_gaussian_background_neurons_3odors.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot of the original and inhibited odors, sampled sparsely in time\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Raw background\n",
    "skp = 1000\n",
    "tslice = slice(transient, None, skp)\n",
    "ax.scatter(bkvecser[tslice, 0], bkvecser[tslice, 1], bkvecser[tslice, 2], color=\"r\", label=\"Background\")\n",
    "# Compare to inhibition of the mean\n",
    "mean_inhibition = bkvecser - np.mean(bkvecser[transient:], axis=0)*inhib_rates[0]/sum(inhib_rates)\n",
    "ax.scatter(mean_inhibition[tslice, 0], mean_inhibition[tslice, 1], mean_inhibition[tslice, 2], \n",
    "           color=\"xkcd:light blue\", label=\"Average subtraction\")\n",
    "ax.scatter(sser[tslice, 0], sser[tslice, 1], sser[tslice, 2], \n",
    "           color=\"b\", label=\"IBCM inhibition\")\n",
    "ax.scatter(0, 0, 0, color=\"k\", s=200, alpha=1)\n",
    "ax.view_init(azim=150, elev=30)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.85))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response to a new odor\n",
    "This part of the code only runs if the simulation above had ``n_dimensions > n_components``. \n",
    "\n",
    "The goal is to see whether a new odor, not linearly dependent of the ones in the background, also gets repressed close to zero, or produces an inhibited output noticeably different from the inhibited background, and still similar to the new odor vector, at least its component perpendicular to the background subspace.\n",
    "\n",
    "I will test this property more carefully, over a statistical ensemble of new odors, once I have a more realistic model of odors themselves. Here, the goal is just to see if some component of the new odor is left after. To see this, I look at the mixture before and after inhibition, decomposed on the non-orthogonal basis of background + new odor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_new_odor_ibcm(odormix, typical_m, typical_w, coupling):\n",
    "    # Compute activation of neurons with this new odor (new+background)\n",
    "    # Given the IBCM and inhibitory neurons' current state \n",
    "    # (either latest or some average state of the neurons)\n",
    "    c = typical_m.dot(odormix)\n",
    "    cbar = c - coupling*(np.sum(c) - c)  # -c to cancel the subtraction of c[i] itself\n",
    "\n",
    "    # New odor after inhibition by the network, ReLU activation on s\n",
    "    # Inhibit with the mean cbar*wser, to see how on average the new odor will show\n",
    "    new_output = relu_inplace(odormix - typical_w.dot(cbar))  # s = x - Wc\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_odor = np.roll(back_components[0], shift=-1)  # Should be a new vector\n",
    "full_basis = np.hstack([back_components.T, new_odor.reshape(-1, 1)])\n",
    "\n",
    "# Mix new odor with background\n",
    "current_background = back_components[1]\n",
    "# new_odor = 0.5*new_odor + 0.5*np.sum(back_components, axis=0) / n_components  # Combine with mean background\n",
    "new_odor = 0.5*new_odor + 0.5*current_background  # Combine with one component\n",
    "# Inhibit with average m synapses and w\n",
    "new_odor_after_inhibition_average = respond_new_odor_ibcm(new_odor, np.mean(mser[transient:], axis=0), \n",
    "                                                     np.mean(wser[transient:], axis=0), coupling_eta)\n",
    "# Inhibit with latest m and w\n",
    "new_odor_after_inhibition_latest = respond_new_odor_ibcm(new_odor, mser[-1], wser[-1], coupling_eta)\n",
    "\n",
    "# Response to new odor after inhibition by removal of average background\n",
    "# s = x - alpha/(alpha+beta)*mean_background\n",
    "new_odor_after_average_subtract = relu_inplace(new_odor - (inhib_rates[0]/(sum(inhib_rates))\n",
    "                                              * np.mean(bkvecser[transient:], axis=0)))\n",
    "\n",
    "# Show components along the three background vectors plus the new odor vector (full_basis)\n",
    "print(\"Decomposition on the basis of x_gamma\")\n",
    "print(\"Un-inhibited background\", decompose_nonorthogonal_basis(current_background, full_basis))\n",
    "print(\"Unhinibited new odor:\", decompose_nonorthogonal_basis(new_odor, full_basis))\n",
    "print(\"Average after IBCM inhibition:\", decompose_nonorthogonal_basis(\n",
    "                        new_odor_after_inhibition_average, full_basis))\n",
    "print(\"Inhibition by average subtraction:\", decompose_nonorthogonal_basis(\n",
    "                        new_odor_after_average_subtract, full_basis))\n",
    "\n",
    "# We indeed detect the new odor in the plane perpendicular to the inhibition. \n",
    "# Hence the more components of the new odor are not spanned by the old odors, the more\n",
    "# we can detect it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhibition of an alternating background\n",
    "In this case we know, from original papers, that the fixed points of the IBCM model are orthogonal to all but one of the alternating components. It is thus easy to predict analytically the inhibition operated by this model. \n",
    "\n",
    "## Analytical predictions\n",
    "\n",
    "### Fixed points of an IBCM neuron\n",
    "In terms of the inhibited (bar) variables, equations are decoupled and the solutions reduce to those of a single IBCM neuron. The $K$ stable fixed points when there are $K$ possible values of $\\vec{x}$ are defined by\n",
    "\n",
    "$$ c_{\\rho} = \\vec{m} \\cdot \\vec{x}_{\\rho} = \\frac{1}{p_{\\gamma}} \\delta_{\\rho \\gamma} $$\n",
    "    \n",
    "where $\\vec{x}_{\\gamma}$ is the component for which the neuron is selective, $p_{\\gamma}$ is the probability of that component, and $\\delta_{\\rho \\gamma}$ is the Kronecker-delta symbol, equal to one only when $\\vec{x} = \\vec{x}_{\\gamma}$. \n",
    "\n",
    "### Fixed point of inhibitory synaptic weights $\\vec{w}$\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Instantaneous value of the inhibited background\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import update_alternating_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_back_altern = [np.zeros(1), back_components[0]]  # Start with component 0\n",
    "back_params_altern = [np.arange(n_components)/n_components, back_components]\n",
    "init_synapses = 0.3*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "sim_res = integrate_inhib_ibcm_network(init_synapses, update_alternating_inputs, init_back_altern, ibcm_rates, \n",
    "            inhib_rates, back_params_altern, duration, deltat, seed=seed_from_gen(rgen_meta), noisetype=\"uniform\")\n",
    "\n",
    "t_ser_alt, bk_ser_alt, bkvec_ser_alt, m_ser_alt, cbar_ser_alt, _, w_ser_alt, s_ser_alt = sim_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 50000\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbar_ser_alt = m_ser_alt*(1.0 + coupling_eta) - coupling_eta*np.sum(m_ser_alt, axis=1, keepdims=True)\n",
    "c_gammas_alt = m_ser_alt.dot(back_components.T)\n",
    "cbars_gamma_alt = mbar_ser_alt.dot(back_components.T)\n",
    "\n",
    "# Constaint 1: sum of c_gammas for each neuron should be 1/p_gamma, so here, =n_components\n",
    "print(\"Comparison to analytical fixed points\")\n",
    "print(\"This matrix should have one non-zero element per row, equal to n_components={}:\\n\".format(n_components), \n",
    "      np.around(np.mean(cbars_gamma_alt[transient:], axis=0), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time evolution of the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_series(mbar_ser_alt, dim_idx=[0, 1, 2], transient=10000, skp=1000)\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=30, elev=30)\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "# fig.savefig(\"figures/three_odors/points_fixes_ibcm_3_odeurs_alternant.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background components after inhibition\n",
    "Total activity norm is constant, because alternating between background vectors of equal  norms. Need to compute the variance of each ORN/PN and show it is reduced in the inhibited layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(t_ser_alt, bkvec_ser_alt, s_ser_alt, skp=300)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bkvec_ser_alt[transient:])\n",
    "avg_snorm = np.mean(s_ser_alt[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bkvec_ser_alt[transient:])\n",
    "std_snorm = np.std(s_ser_alt[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity of a projection neuron reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of a projection neuron's activity reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-gaussian unimodal distribution\n",
    "If there is a discrete number of fixed points when the $\\nu_{\\alpha}$ have a distribution with non-zero third moment, there should be a transition from a continuum of fixed points to this discrete case as we increase a parameter $\\epsilon = \\langle (\\nu - \\langle\\nu\\rangle)^3 \\rangle$ above zero. \n",
    "\n",
    "To interpolate with a parameter $\\epsilon$ from a gaussian distribution to a distribution with non-zero central third moment, one trick is to simulate $x$ as an Ornstein-Uhlenbeck process with zero mean, then take\n",
    "$$ \\nu = s + x + \\epsilon x^2 $$\n",
    "or, in the multivariate case, \n",
    "$$ \\vec{\\nu} = \\vec{s} + \\vec{x} + \\epsilon \\mathrm{diag}(\\vec{x}) \\vec{x} $$\n",
    "\n",
    "If there are no correlations, we can treat each component $\\nu_{\\alpha}$ as a univariate case, and we then have a third moment of order $\\epsilon$, with only lower-order corrections to the second moment and order $\\epsilon$ corrections to the desired mean $s$:\n",
    "\n",
    "$$ \\langle \\nu \\rangle = s + \\epsilon \\sigma^2 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^2 \\rangle = \\sigma^2 + 2 \\epsilon^2 \\sigma^4 $$\n",
    "$$ \\langle (\\nu - \\langle \\nu \\rangle)^3 \\rangle = 6 \\epsilon \\sigma^4 + 8 \\epsilon^3 \\sigma^6 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.backgrounds import update_thirdmoment_kinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset some simulations parameters, others stay as before\n",
    "# Initial synaptic weights: small positive noise near origin\n",
    "rgen_meta = np.random.default_rng(seed=0xeefeb6e5f101c07cf9e80e95d5e8ecff)\n",
    "init_synapses = 0.3*rgen_meta.random(size=[n_neurons, n_dimensions])\n",
    "\n",
    "# Initial background vector and initial nu values\n",
    "averages_nu = np.ones(n_components) / np.sqrt(n_components)\n",
    "init_nu = np.zeros(n_components)\n",
    "init_bkvec = averages_nu.dot(back_components)\n",
    "# nus are first in the list of initial background params\n",
    "init_back_list = [init_nu, init_bkvec]\n",
    "\n",
    "## Compute the matrices in the Ornstein-Uhlenbeck update equation\n",
    "# Update matrix for the mean term: \n",
    "# Exponential decay with time scale tau_nu over time deltat\n",
    "tau_nu = 2.0  # Fluctuation time scale of the background nu_gammas (same for all)\n",
    "update_mat_A = np.identity(n_components)*np.exp(-deltat/tau_nu)\n",
    "\n",
    "# Steady-state covariance matrix\n",
    "sigma2 = 0.09\n",
    "correl_rho = 0.0\n",
    "epsilon_nu = 0.2\n",
    "steady_covmat = correl_rho * sigma2 * np.ones([n_components, n_components])  # Off-diagonals: rho\n",
    "steady_covmat[np.eye(n_components, dtype=bool)] = sigma2  # diagonal: ones\n",
    "\n",
    "# Cholesky decomposition of steady_covmat gives sqrt(tau/2) B\n",
    "# Update matrix for the noise term: \\sqrt(tau/2(1 - exp(-2*deltat/tau))) B\n",
    "psi_mat = np.linalg.cholesky(steady_covmat)\n",
    "update_mat_B = np.sqrt(1.0 - np.exp(-2.0*deltat/tau_nu)) * psi_mat\n",
    "\n",
    "back_params_3 = [update_mat_A, update_mat_B, back_components, averages_nu, epsilon_nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_init, update_bk, bk_init, ibcm_params, inhib_params, bk_params, tmax, dt, seed=None, noisetype=\"normal\"\n",
    "sim_results = integrate_inhib_ibcm_network(init_synapses, update_thirdmoment_kinputs, init_back_list, ibcm_rates, \n",
    "                inhib_rates, back_params_3, duration, deltat, seed=seed_from_gen(rgen_meta), noisetype=\"normal\")\n",
    "tser3, nuser3, bkvecser3, mser3, cbarser3, _, wser3, sser3 = sim_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cgammas_bar and mbars\n",
    "transient = 50000\n",
    "# Dot products \\bar{c}_{\\gamma} = \\bar{\\vec{m}} \\cdot \\vec{x}_{\\gamma}\n",
    "mbarser3 = mser3*(1.0 + coupling_eta) - coupling_eta*np.sum(mser3, axis=1, keepdims=True)\n",
    "c_gammas3 = mser3.dot(back_components.T)\n",
    "cbars_gamma3 = mbarser3.dot(back_components.T)\n",
    "\n",
    "# For quantitative comparison to analytical values, see below:\n",
    "# requires computing corrections due to third moment. \n",
    "sums_cbars_gamma_3 = np.sum(cbars_gamma3, axis=2)\n",
    "sums_cbars_gamma2_3 = np.sum(cbars_gamma3*cbars_gamma3, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time evolution of IBCM neurons"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If want to show this plot, need to calculate analytical prediction with corrections\n",
    "fig, axes = plot_cbars_gammas_sums(tser3, sums_cbars_gamma_3, sums_cbars_gamma2_3, skp=200, skp_lbl=1)\n",
    "axes[0].axhline(1.0 / averages_nu.mean(), ls=\"--\", color=\"k\", label=r\"$1 / \\langle \\nu \\rangle$\")\n",
    "axes[1].axhline(1.0 / sigma2, ls=\"-.\", color=\"k\", label=r'$1 / \\sigma^2$')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_series(mbarser3, dim_idx=[0, 1, 2], transient=10000, skp=1000)\n",
    "\n",
    "# Annotate with vectors representing the odor components\n",
    "orig = np.zeros([n_components, n_components])\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "scale = 3\n",
    "vecs = back_components.copy()\n",
    "for i in range(n_components):\n",
    "    vecs[i] = back_components[i] / np.sqrt(np.sum(back_components[i]**2)) * scale\n",
    "ax.quiver(*orig, *(vecs[:, :3].T), color=\"k\", lw=2.0)\n",
    "ax.view_init(azim=30, elev=30)\n",
    "ax.set(xlabel=r\"$\\overline{m}_1$\", ylabel=r\"$\\overline{m}_2$\", zlabel=r\"$\\overline{m}_3$\")\n",
    "# fig.savefig(\"figures/three_odors/points_fixes_ibcm_3_odeurs_non-gaussien.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background components after inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax, bknorm_ser3, snorm_ser3 = plot_background_norm_inhibition(tser3, bkvecser3, sser3)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bknorm_ser3[transient:])\n",
    "avg_snorm = np.mean(snorm_ser3[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bknorm_ser3[transient:])\n",
    "std_snorm = np.std(snorm_ser3[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of activity norm reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/three_odors/inhibition_non-gaussian_background_norm_3odors.pdf\", \n",
    "#            transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_mat, axes = plot_background_neurons_inhibition(tser3, bkvecser3, sser3, skp=100)\n",
    "\n",
    "# Compute noise reduction factor, annotate\n",
    "transient = 50000\n",
    "avg_bknorm = np.mean(bkvecser3[transient:])\n",
    "avg_snorm = np.mean(sser3[transient:])\n",
    "avg_reduction_factor = avg_snorm / avg_bknorm\n",
    "std_bknorm = np.std(bkvecser3[transient:])\n",
    "std_snorm = np.std(sser3[transient:])\n",
    "std_reduction_factor = std_snorm / std_bknorm\n",
    "\n",
    "print(\"Mean activity of a projection neuron reduced to \"\n",
    "      + \"{:.1f} % of input\".format(avg_reduction_factor * 100))\n",
    "print(\"Standard deviation of a projection neuron's activity reduced to \"\n",
    "      + \"{:.1f} % of input\".format(std_reduction_factor * 100))\n",
    "ax.annotate(\"St. dev. reduced to {:.1f} %\".format(std_reduction_factor * 100), \n",
    "           xy=(0.98, 0.98), xycoords=\"axes fraction\", ha=\"right\", va=\"top\")\n",
    "\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1.0, 0.8))\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"figures/three_odors/inhibition_non-gaussian_background_neurons_3odors.pdf\", \n",
    "#           bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical fixed point prediction compared to simulations\n",
    "Here, it is harder to make analytical progress. I could not solve the IBCM steady-state equation exactly yet, but I could show, using symmetries of the system of equations, that the set of variables $c_{\\alpha} = \\vec{m} \\cdot \\vec{x}_{\\alpha}$, at a given fixed point, can take either all the same value (leading to an unstable fixed point), or each take one of two values (which I am not able to compute explicitly). It is not possible to have three or more $c_{\\alpha}$ differing from each other (for instance, it is not possible to have $c_{\\alpha} \\neq c_{\\beta} \\neq c_{\\gamma}$), because the sum of any two $c_{\\alpha}, c_{\\beta}$ that are different from one another must be equal to the same one constant. \n",
    "\n",
    "This is confirmed numerically below (with some error): one component $c_{\\alpha}$ is large and positive, while other $c_{\\beta}$, $\\forall \\beta \\neq \\alpha$, are small and negative and equal to each other, with of course some variability due to correlations neglected in the analytical calculations, and to long excursions in the simulations due to having a flat direction in the landscape (the slow manifold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfcts.ibcm_analytics import fixedpoint_thirdmoment_perturbtheory, fixedpoint_thirdmoment_exact\n",
    "def fixedpoint_thirdmoment_xy(*args, **kwargs):\n",
    "    # The function returns cd and u2 too, here we just want x, y. \n",
    "    return fixedpoint_thirdmoment_perturbtheory(*args, **kwargs)[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analytical values\n",
    "# Compute actual mean, variance and third moment for nu = s + nu_gauss + epsilon*nu_gauss^2\n",
    "variance_nu3 = sigma2 + 2*(epsilon_nu*sigma2)**2\n",
    "mean_nu3 = averages_nu[0] + epsilon_nu*sigma2\n",
    "thirdmoment = 6*epsilon_nu*sigma2**2 + 8*(epsilon_nu*sigma2)**3\n",
    "\n",
    "print(\"Expected moments of the background mixture coefficients:\")\n",
    "print(\"<(nu - <nu>)^3> =\", thirdmoment)\n",
    "print(\"<(nu - <nu>)^2> =\", variance_nu3)\n",
    "print(\"<nu> =\", mean_nu3)\n",
    "\n",
    "# Check that empirical statistics of nu match analytical expectations\n",
    "print(\"Compare to empirical moments:\")\n",
    "fullnuser3 = averages_nu.reshape(1, -1) + nuser3 + epsilon_nu*nuser3**2\n",
    "empirical_thirdmoment = np.mean((fullnuser3[:, 0] - np.mean(fullnuser3[:, 0]))**3)\n",
    "print(\"<(nu - <nu>)^3> =\", empirical_thirdmoment)\n",
    "\n",
    "empirical_secondmoment = np.mean((fullnuser3[:, 0] - np.mean(fullnuser3[:, 0]))**2)\n",
    "print(\"<(nu - <nu>)^2> =\", empirical_secondmoment)\n",
    "\n",
    "print(\"<nu> =\", np.mean(fullnuser3[:, 0]))  # First moment\n",
    "\n",
    "\n",
    "# Compute the analytical predictions of the fixed points\n",
    "res = fixedpoint_thirdmoment_xy(mean_nu3, variance_nu3, thirdmoment, 1, 2, m3=1.0, order=1)\n",
    "cgamma1_analytical_order1, cgamma2_analytical_order1 = res\n",
    "print(\"c_\\gamma values, order 1:\", cgamma1_analytical_order1, cgamma2_analytical_order1)\n",
    "\n",
    "# Order zero\n",
    "res = fixedpoint_thirdmoment_xy(mean_nu3, variance_nu3, thirdmoment, 1, 2, m3=1.0, order=0)\n",
    "cgamma1_analytical_order0, cgamma2_analytical_order0 = res\n",
    "print(\"c_\\gamma values, zeroth order:\", cgamma1_analytical_order0, cgamma2_analytical_order0)\n",
    "\n",
    "# Exact solution\n",
    "res = fixedpoint_thirdmoment_exact([mean_nu3, variance_nu3, thirdmoment], 1, 2)\n",
    "cgamma1_exact, cgamma2_exact, _, _ = res\n",
    "print(\"c_\\gamma values, exact:\", cgamma1_exact, cgamma2_exact)\n",
    "\n",
    "### Empirical values\n",
    "# Average value of the m.x_a for each neuron, for each x_a\n",
    "# avg_m indexed [neuron, dimension], back_components indexed [component, dimension]\n",
    "# So need to take m.dot(x_a.T)\n",
    "transient = 60000\n",
    "avg_mbar = np.mean(mbarser3[transient:], axis=0)\n",
    "steadystate_c_gamma = avg_mbar.dot(back_components.T)\n",
    "print(steadystate_c_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nice graph of the time series of the c_gamma in each neuron vs the two possible \n",
    "# values for c_gamma as prediction from perturbation theory\n",
    "# These time series were computed above in cbar_gammas3\n",
    "# Also plot the average c_gammas of each neuron in addition to their time series? \n",
    "# These were computed above too in steadystate_c_gamma\n",
    "fig, ax, n_closest = plot_cbars_gamma_series(tser3, cbars_gamma3, skp=100, transient=60000)\n",
    "\n",
    "# Plot analytical prediction of the two possible values for any c_gamma:\n",
    "ax.axhline(cgamma1_analytical_order1, color=\"y\", ls=\"-\", \n",
    "           label=r\"Unique $\\bar{c}_\\gamma$, 1st-order\", lw=3.)\n",
    "ax.axhline(cgamma2_analytical_order1, color=\"y\", ls=\"--\", \n",
    "           label=r\"Other $\\bar{c}_\\gamma$s, 1st-order\", lw=3.)\n",
    "\n",
    "# Compare to zeroth order prediction\n",
    "#ax.axhline(cgamma1_analytical_order0, color=\"grey\", ls=\"-\", \n",
    "#           label=r\"Unique $\\bar{c}_\\gamma$, 0th-order\", lw=3.)\n",
    "#ax.axhline(cgamma2_analytical_order0, color=\"grey\", ls=\"--\", \n",
    "#           label=r\"Other $\\bar{c}_\\gamma$s, 0th-order\", lw=3.)\n",
    "# Compare to exact prediction\n",
    "ax.axhline(cgamma1_exact, color=\"grey\", ls=\"-\", \n",
    "           label=r\"Unique $\\bar{c}_\\gamma$, exact\", lw=3.)\n",
    "ax.axhline(cgamma2_exact, color=\"grey\", ls=\"--\", \n",
    "           label=r\"Other $\\bar{c}_\\gamma$s, exact\", lw=3.)\n",
    "\n",
    "ax.legend(fontsize=9, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "fig.set_size_inches(4.0, 4.)\n",
    "#fig.savefig(\"figures/three_odors/cgammas_thirdmoment_background_perturbation_theory_prediction.pdf\", \n",
    "#   transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical stability\n",
    "I can compute analytically the jacobian of the average dynamics (for i.i.d. background concentrations, weakly non-gaussian), but I am not able to compute its eigenvalues or to apply Routh-Hurwitz criterion to determine stability. So I will compute the jacobian based on analytical expressions, then diagonalize it numerically. \n",
    "\n",
    "There should be $n_B + 1$ non-zero eigenvalues, the rest ($n_O - n_B - 1$) should be zero because the jacobian is entirely made up of outer products of background odor vectors. \n",
    "\n",
    "I will compute the jacobian for various $k_1$ and $k_2$ (numbers of components with specific/non-specific dot product values). The goal is to show the only stable fixed points have $k_1 = 1$ and $k_2 = n_B - 1$. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from modelfcts.ibcm_analytics import jacobian_fixedpoint_thirdmoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_fixedpoint_thirdmoment(moments, ibcm_params, which_specif, back_comps, m3=1.0, order=1):\n",
    "    \"\"\" which_specif: boolean array equal to True for specific gammas. \"\"\"\n",
    "    ## 1. Evaluate x, y, cd, u^2 at the fixed point\n",
    "    # From the list of c_gammas which are specific, count k1 and k2\n",
    "    assert which_specif.size == back_comps.shape[0]\n",
    "    which_specif = which_specif.astype(bool)\n",
    "    k1 = np.sum(which_specif.astype(bool))\n",
    "    k2 = which_specif.size - k1\n",
    "\n",
    "    avgnu, variance, epsilon = moments\n",
    "    mu, tau_theta, eta = ibcm_params\n",
    "    c_sp, c_nsp, cd, u2 = fixedpoint_thirdmoment_perturbtheory(avgnu,\n",
    "                        variance, epsilon, k1, k2, m3=m3, order=order)\n",
    "    x_d = avgnu * np.sum(back_comps, axis=0)\n",
    "    cgammas_vec = np.where(which_specif, c_sp, c_nsp)\n",
    "    # 2. Evaluate the jacobian blocks\n",
    "    n_dims = x_d.shape[0]\n",
    "    n_comp = k1 + k2\n",
    "    jac = np.zeros([n_dims + 1, n_dims + 1])\n",
    "    # Scalar element\n",
    "    jac[-1, -1] = -1.0 / tau_theta\n",
    "    # Vector blocks\n",
    "    avg_cx = cd * x_d + variance * cgammas_vec.dot(back_comps)\n",
    "    # Last column\n",
    "    jac[:n_dims, -1] = 2.0 / tau_theta * avg_cx\n",
    "    # Last row\n",
    "    jac[-1, :n_dims] = -mu * avg_cx\n",
    "    # Matrix block\n",
    "    theta_ss = cd**2 + variance * u2\n",
    "    x_gammas_outer = back_comps[:, :, None] * back_comps[:, None, :]\n",
    "    xd_outer = np.outer(x_d, x_d)\n",
    "    xd_xgammas_outer = x_d[None, :, None] * back_comps[:, None, :]\n",
    "    avg_xx = xd_outer + variance*np.sum(x_gammas_outer, axis=0)\n",
    "    avg_cxx = (cd * avg_xx\n",
    "                + variance*np.sum(cgammas_vec[:, None, None]*xd_xgammas_outer, axis=0)\n",
    "                + variance*np.sum(cgammas_vec[:, None, None]*xd_xgammas_outer, axis=0).T\n",
    "                + epsilon*m3*np.sum(cgammas_vec[:, None, None]*x_gammas_outer, axis=0)\n",
    "            )\n",
    "    jac[:-1, :-1] = mu * (2*avg_cxx - theta_ss*avg_xx)\n",
    "\n",
    "    # Build the complete matrix\n",
    "    return jac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute jacobian for various $k_1$ and $k_2$. \n",
    "# Without loss of generality, assume it's the first \n",
    "# $k_1$ x_gammas which elicit a specific response.  \n",
    "moments_3 = [mean_nu3, variance_nu3, thirdmoment]\n",
    "for k1 in range(0, n_components):\n",
    "    # moments, ibcm_params, which_specif, back_comps, m3=1.0, order=1\n",
    "    which_specific = np.zeros(n_components, dtype=bool)\n",
    "    which_specific[:k1] = True\n",
    "    jacob = jacobian_fixedpoint_thirdmoment(moments_3, ibcm_rates, \n",
    "                    which_specific, back_components, m3=1.0, order=1)\n",
    "    print(\"For k1 = {}, jacobian matrix x 1000:\\n\".format(k1), np.round(jacob*1e3, 2))\n",
    "    eigenvalues = np.linalg.eigvals(jacob)\n",
    "    sort_order = np.argsort(np.abs(np.real(eigenvalues)))[::-1]\n",
    "    eigenvalues = eigenvalues[sort_order]\n",
    "    print(\"For k1 = {}, eigenvalues x 1000:\\n\".format(k1), np.round(eigenvalues*1e3, 6))\n",
    "    if np.all(np.real(eigenvalues[:n_components+1]) < 0):\n",
    "        print(\"This is a stable fixed point!\")\n",
    "    else:\n",
    "        print(\"This fixed point is unstable\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed points of the inhibition vectors w\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def fixedpoint_wvec_given_cgammas(cgammas, back_comps, back_stats, avg_rates):\n",
    "    \"\"\" For the special case where all $\\nu$ are iid with mean and variance\n",
    "    given in back_stats. \n",
    "    \n",
    "    Args:\n",
    "        cgammas (np.ndarray): dot products at the fixed point \n",
    "            of mbar with each background component. \n",
    "            Shape [..., component]\n",
    "        back_comps (np.ndarray): background vectors x_gammas, \n",
    "            indexed [component, dimension]\n",
    "        back_stats (list of 2 floats): average nu, sigma^2. \n",
    "        avg_rates (list of 2 floats): alpha, beta\n",
    "    \n",
    "    Returns:\n",
    "        wvec (np.ndarray): 1d steady-state vector, \n",
    "            shape [..., component]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "    avgnu, varinu = back_stats\n",
    "    alph, bet = avg_rates\n",
    "    cbard = avgnu * np.sum(cgammas, axis=-1, keepdims=True)\n",
    "    term_mean = avgnu * cbard * np.sum(back_comps, axis=0)\n",
    "    term_vari = varinu * np.dot(cgammas, back_comps)  # Sum over gamma (indexing background components)\n",
    "    return alph / (alph*cbard + bet) * (term_mean + term_vari)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fixed_cgammas = np.ones([n_components, n_components]) * cgamma2_analytical_order1\n",
    "all_fixed_cgammas[np.diag_indices(n_components)] = cgamma1_analytical_order1\n",
    "wvecs_analytical = fixedpoint_wvec_given_cgammas(all_fixed_cgammas, back_components, \n",
    "                                                 (mean_nu3, variance_nu3), inhib_rates)\n",
    "print(wvecs_analytical)\n",
    "\n",
    "all_fixed_cgammas_zeroth = np.ones([n_components, n_components]) * cgamma2_analytical_order0\n",
    "all_fixed_cgammas_zeroth[np.diag_indices(n_components)] = cgamma1_analytical_order0\n",
    "wvecs_analytical_zeroth = fixedpoint_wvec_given_cgammas(all_fixed_cgammas_zeroth, back_components, \n",
    "                                                 (mean_nu3, variance_nu3), inhib_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nice graph of the time series of the wvec in each neuron vs the possible \n",
    "# values for wvec components (exploit symmetry: fixed points are permutations of same 4 values)\n",
    "# These time series were computed above in wser3\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Colors: one palette per component\n",
    "palettes = [\"Blues\", \"Purples\", \"Greens\", \"Oranges\", \"Reds\"]\n",
    "if len(palettes) < n_dimensions:\n",
    "    raise ValueError(\"Need to define at least {} more palettes!\".format(n_dimensions-len(palettes)))\n",
    "all_colors = [sns.color_palette(p, n_colors=n_neurons) for p in palettes]\n",
    "\n",
    "tsli = slice(0, None, 200)\n",
    "for n in range(n_neurons):\n",
    "    for i in range(n_dimensions):\n",
    "        lbl = \"Dimension {}\".format(i) if n == n_neurons-1 else None\n",
    "        ax.plot(tser3[tsli], wser3[tsli, n, i], lw=1.25, color=all_colors[i][n], alpha=0.75, label=lbl)\n",
    "\n",
    "# Plot analytical prediction of the two possible values for any c_gamma:\n",
    "for i in range(n_dimensions):\n",
    "    lbl = r\"Analytical $\\vec{w}$, order 0\" if i == n_dimensions - 1 else \"\"\n",
    "    ax.axhline(wvecs_analytical_zeroth[0, i], color=\"grey\", ls=\"--\", label=lbl, lw=2.5)\n",
    "    lbl = r\"Analytical $\\vec{w}$, order 1\" if i == n_dimensions - 1 else \"\"\n",
    "    ax.axhline(wvecs_analytical[0, i], color=\"y\", ls=\"--\", label=lbl, lw=2.5)\n",
    "\n",
    "ax.set(xlabel=\"Time (steps)\", ylabel=r\"$\\vec{w}$ components\")\n",
    "ax.legend(fontsize=9)\n",
    "# fig.savefig(\"figures/three_odors/wvecs_thirdmoment_background_perturbation_theory_prediction.pdf\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
